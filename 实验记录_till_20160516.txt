#

library(ggplot2)
all<-read.table("samples_plasma_detected.txt",head=T,sep="\t")
library(reshape2)
a_melt <- melt(all)
ggplot(a_melt2) + geom_density(aes(x = value,y = ..density..,colour = variable,lty=variable))

#
all<-read.table("samples_plasma_detected8_17.txt",head=T,sep="\t")
a_melt <- melt(all)
library(reshape2)
library(sm)
pdf("samples_plasma_detected8_17.pdf")
sm.density.compare(a_melt$value,a_melt$variable,xlab="Log2(normalized indensity)")
legend("topright",fill=c(1:9),c("17_1","#8_2","#8_3","#8_4","#8_5","#8_6","#17_2","#17_3","#17_4"))
dev.off()


#
> setwd("/Users/lishasha/Desktop/home/project/2016/circRNA/figures/fig1")
a<-read.table("brain_9T.txt",head=T,row.names=1)
> dim(a)
[1] 175938      2
> View(a)
> pdf("fig1d_brain_9N.pdf")
plot(a$Human_Brain,a$P9T,lty=2,pch=20,cex=0.5,xlab="Log2(normalized density) of human_brain",ylab="Log2(normalized density) of cervical of #2")
dev.off()

#ttest

a<-read.table("p17_4.txt",head=T,sep="\t")
b<-read.table("p17_4_permutation.txt",head=T,sep="\t")
res<-matrix(,nrow=1,ncol=1)
for (i in 1:100){
m<-t.test(a$P17_4,b[,i])
res<-rbind(res,m[3])
}
write.table(res,"P17_4_pvalue.txt")   





#density_compare 
all<-read.table("p3_density_plot.txt",head=T,sep="\t")
library(reshape2)
a_melt <- melt(all)
sm.density.compare(a_melt$value,a_melt$variable,xlab="Log2(normalized indensity)")
legend("topright",fill="red",c("17_4"),bty="n",lty=1)  
legend("topright",inset=.05,fill="red",c("17_4"),bty="n",lty=1)  
#legend(locator(1),c("#3"),lty=2,col="magenta")   final

#legend(locator(1),fill=c("green","black"),title="circRNAs Classification",c("17_4","permutation")) 
#legend(locator(1),fill=c("green","black"),title="circRNAs Classification",c("17_4","permutation"))
#legend(locator(1),lty=cilfill,c("17_4","Permutated")) 
col=c("green","black")

#正式用
all<-read.table("p1_density_100.txt",head=T,sep="\t")
a_melt <- melt(all)
pdf("1_test_lty1.pdf")
sm.density.compare(a_melt$value,a_melt$variable,xlab="Log2(normalized indensity)")
legend("topright",c("#1"),lty=1,col="red")
dev.off()




#不用每次vim，这样就可以读取excel出来的，然后再也出来即可
a<-read.table("cancer_plasma.txt",head=T,row.names=1,sep="\t")

#抽取血浆中术后下降的
a<-read.table("p1.txt",head=T)
res<-matrix(,nrow=1459,ncol=1)
for (i in 1:1000){
res<-cbind(res,a[sample(1:85925,1459),])
}
head(res)
res2<-res[,2:1001]
write.table(res2,"p1_sampling_1000_1459.txt")

mean1<-apply(res2,2,mean)
median1<-apply(res2,2,median)
write.table(mean1,"mean1_p1.txt")
write.table(median1,"median1_p1.txt")

#抽样100次
a<-read.table("p3.txt",head=T)
res<-matrix(,nrow=1439,ncol=1)
for (i in 1:100){
res<-cbind(res,a[sample(1:86230,1439),])
}
head(res)
res2<-res[,2:101]
write.table(res2,"p3_sampling_100_1439.txt")

median1<-apply(res2,2,median)
write.table(median1,"median1_p3_100.txt")
#密度图
library(ggplot2)
all<-read.table("p1_test.txt",head=T,sep="\t")
all<-all[2:104]
library(reshape2)
a_melt <- melt(all)
ggplot(a_melt2) + geom_density(aes(x = value,y = ..density..,colour = variable,lty=V1))
＃ggplot(a_melt2, aes(x=value, fill=variable)) + geom_density(aes(alpha=.4,colour = variable))+xlab("Log2(normalized indensity)")+ylab("Density")
library(ggplot2)
all<-read.table("p1_permutation_13samples.txt",head=T,sep="\t")
all<-all[2:104]
library(reshape2)
a_melt <- melt(all)
f<-read.table("factor.txt")
a_melt2<-cbind(a_melt,f)
ggplot(a_melt2) + geom_density(aes(x = value,y = ..density..,colour = V1))
#ggplot(a_melt2, aes(x=value, fill=variable)) + geom_density(aes(alpha=.4,colour = V1))+xlab("Log2(normalized indensity)")+ylab("Density")
#ggplot(a_melt, aes(x=value,fill=variable)) + geom_density(aes(alpha=.3,colour = variable))+xlab("Log2(normalized indensity)")+ylab("Density")+theme(legend.position='none')
#ggplot(a_melt, aes(x=value)) + geom_density(aes(alpha=.3,colour = variable))+xlab("Log2(normalized indensity)")+ylab("Density")+theme(legend.position='none')
#ggplot(a_melt) + geom_density(aes(x = value,y = ..density..,colour = variable))+theme(legend.position='none') #最终使用
#alpha透明度
＃不加Legend
p+theme(legend.position='none')
#
a<-read.table("p17_4.txt",head=T)
res<-matrix(,nrow=533,ncol=1)
for (i in 1:1000){
res<-cbind(res,a[sample(1:86995,533),])
}
res2<-res[,2:1001]
write.table(res2,"p17_4_sampling_1000.txt")

median1<-apply(res2,2,median)
write.table(median1,"median1_p17_4_1000.txt")


#
library(ggplot2)
all<-read.table("p17_3_sampling_1000_density_plot.txt",head=T,sep="\t")
library(reshape2)
a_melt <- melt(all)
ggplot(a_melt) + geom_density(aes(x = value,y = ..density..,colour = variable))+theme(legend.position='none')


 图例（legend）的位置

图例（legend）的位置和对齐使用的主题设置legend.position来控制，其值可为right,left,top,bottom,none(不加图例，或是一个表示位置的数值。这个数值型位置由legend.justfication给定的相对边角位置表示（取0和1之间的值），它是一个长度为2的数值型向量：右上角为c(1,1),左下角为c(0,0)

例如：p+theme(legend.position=”left”)

＃举例
library(ggplot2)
  >df <- data.frame(cond = factor( rep(c("A","B"), each=200) ), rating = c(rnorm(200),rnorm(200, mean=.8)))
  >ggplot(df, aes(x=rating, fill=cond)) + geom_density(alpha=.3)+xlab("NEW RATING TITLE")+ylab("NEW DENSITY TITLE")

> f<-read.table("factor.txt")
> View(f)
> dim(f)
[1] 50219     1

dim(a_melt2)


> setwd("/Users/lishasha/Desktop/home/project/2016/circRNA/figures/fig1")
> a<-read.table("ref.txt",head=T,row.names=1)
> dim(a)
[1] 175938      2
> View(a)
> pdf("fig1cB_ref.pdf")
plot(a$Common_Reference_1,a$Common_Reference_2,lty=2,pch=20,cex=0.5,xlab="Log2(normalized density) of common_reference_1",ylab="Log2(normalized density) of common_reference_2")
abline(lm(a$Common_Reference_2~a$Common_Reference_1),col="blue")
dev.off()


> setwd("/Users/lishasha/Desktop/home/project/2016/circRNA/figures/fig1")
> a<-read.table("ref_brain.txt",head=T,row.names=1)
> dim(a)
[1] 175938      2
> View(a)
> pdf("fig1d_ref_brain.pdf")
plot(a$Human_Brain,a$Common_Reference,lty=2,pch=20,cex=0.5,xlab="Log2(normalized density) of human_brain",ylab="Log2(normalized density) of common_reference")
abline(lm(a$Common_Reference~a$Human_Brain),col="blue")
dev.off()

a<-read.table("brain.txt",head=T,row.names=1)
> pdf("fig1cA_brain.pdf")
> plot(a$Human_Brain_1,a$Human_Brain_2,lty=2,pch=20,cex=0.5,xlab="Log2(normalized density) of human_brain_1",ylab="Log2(normalized density) of human_brain_2")
> abline(lm(a$Human_Brain_2~a$Human_Brain_1),col="blue")
> dev.off()
lab杜梅捷  09:31:25
@王亚坤 计算平 qsub提交作业的时候，一直是qw的排队状态，请问是什么原因呢？
C-Tao  09:48:39
最好按照／Share／samples下的脚本为模版编写，否则有些细微的语法错误很难排查


#
x <- matrix(c(8730, 39989, 1032, 2700), ncol = 2)
chisq.test(x)


#自己运行
x <- matrix(c(2548, 19792, 198, 1090), ncol = 2)
> chisq.test(x)

	Pearson's Chi-squared test with Yates' continuity correction

data:  x
X-squared = 18.2754, df = 1, p-value = 1.912e-05
Calculate the chi-squared test statistic, \chi^2, which resembles a normalized sum of squared deviations between observed and theoretical frequencies (see below).
Accept or reject the null hypothesis that the observed frequency distribution is different from 
the theoretical distribution based on whether the test statistic exceeds the critical value of \chi^2. 
If the test statistic exceeds the critical value of \chi^2, the null hypothesis (H_0 = there is no difference between the distributions) 
can be rejected with the selected level of confidence and the alternative hypothesis (H_1 = there is a difference between the distributions)
 can be accepted with the selected level of confidence.



chisq.test
The events considered must be mutually exclusive and have total probability 1


## From Agresti(2007) p.39
M <- as.table(rbind(c(762, 327, 468), c(484, 239, 477)))
dimnames(M) <- list(gender = c("M","F"),
                    party = c("Democrat","Independent", "Republican"))
(Xsq <- chisq.test(M))  # Prints test summary
Xsq$observed   # observed counts (same as M)
Xsq$expected   # expected counts under the null
Xsq$residuals  # Pearson residuals
Xsq$stdres     # standardized residuals


## Effect of simulating p-values
x <- matrix(c(12, 5, 7, 7), ncol = 2)
chisq.test(x)$p.value           # 0.4233

对卡方检验和Fisher精确检验的的差别在于，本人认为数据期望频数的差别，如果期望频数有其中之一小于5，那么就应该使用Fisher精确检验，如果每个频数都大于5那么就应该使用卡方检验，



一：对于2*2的列联表：

（1）当T（此处为最小理论频数，下同）>=5, n>=40 时，直接用Pearson 卡方检验； 
（2）当1 = 40 时，需要用连续性校正公式做卡方检验。这是因为卡方分布为连续型分布，而2*2列联表资料是分类资料，所以样本量较小时要进行连续性校正。
（3）当T<1 , 或者 n < 40, 或做卡方检验后所得的P值接近检验水准a 时，用Fisher exact test 



R卡方检验
> M <- as.table(rbind(c(762, 327, 468), c(484, 239, 477)))
> M
    A   B   C
A 762 327 468
B 484 239 477
> dimnames(M) <- list(gender = c("M","F"),
+                     party = c("Democrat","Independent", "Republican"))
> M
      party
gender Democrat Independent Republican
     M      762         327        468
     F      484         239        477
> (Xsq <- chisq.test(M))

	Pearson's Chi-squared test

data:  M
X-squared = 30.0701, df = 2, p-value = 2.954e-07


＃
>a=data.frame()
> a=edit(a)
> a
var1 var2
1 1 9
2 10 10
> chisq.test(a)

Pearson's Chi-squared test with Yates' continuity correction

data: a
X-squared = 3.0323, df = 1, p-value = 0.08162

Warning message:
In chisq.test(a) : Chi-squared近似算法有可能不准
> fisher.test(a)

Fisher's Exact Test for Count Data

data: a
p-value = 0.04851
alternative hypothesis: true odds ratio is not equal to 1
95 percent confidence interval:
0.002314923 1.146071049
sample estimates:
odds ratio
0.1187667
#fisher's exact test[1][2][3] is a statistical significance test used in the analysis of contingency tables. Although in practice it is employed when sample sizes are small, it is valid for all sample sizes. It is named after its inventor, Ronald Fisher, and is one of a class of exact tests, so called because the significance of the deviation from a null hypothesis (e.g., P-value) can be calculated exactly, rather than relying on an approximation that becomes exact in the limit as the sample size grows to infinity, as with many statistical tests.
k阿芳针对样本量较大的，而fisher样本量较少的

library(ggplot2)
all<-read.table("density_evaluate_total.txt",head=T,sep="\t")
library(reshape2)
a_melt <- melt(all)
ggplot(a_melt) + geom_density(aes(x = value,y = ..density..,colour = variable)) + ggtitle('The distribution of density of tissue samples')+ xlab("Log2(normalized indensity)") + ylab("Density")



＃点大小不一的散点图,plot里面的cex就可以起到这个作用了
n <- 50
x <- rnorm(n)
y <- rnorm(n)
z <- rnorm(n)
my.renorm <- function (z) {
  z <- abs(z)
  z <- 10*z/max(z)
  z
}
z <- my.renorm(z)
op <- par(mar = c(3,2,4,2)+.1)
plot(x, y, cex = z,
     xlab = "", ylab = "", 
     main = "Bubble plot")
#哈哈，自己做就这样做可以了

> a<-c(2,12,3)
> b<-c(6,24,5)
> c<-c(56,4,10)
> m<-rbind(a,b,c)
> m
  [,1] [,2] [,3]
a    2   12    3
b    6   24    5
c   56    4   10
> plot(m[,1],m[,2],cex=m[,3],xlab="aaa",ylab="bbb",main="Bubble plot",col="blue")
     


#改变自己想变色的点，先做图，然后piont加颜色,先按照上面的方法进行作图，然后利用pionts对相应点改变颜色
plot(m[,1],m[,2],cex=m[,3],col=u,xlab="aaa",ylab="bbb",main="Bubble plot")
points(m[1,],m[2,],cex=m[3,],col='red')
points(m[1,3],cex=2,col="green") # 放单个点
> z<-6:10
> y<-6:10
> points(y,z,col="yellow") 可按照区间输入多个点，也可按照点输入单个点



library(ggplot2)
all<-read.table("tissue.txt",head=T,sep="\t")
library(reshape2)
a_melt <- melt(all)
ggplot(a_melt) + geom_density(aes(x = value,y = ..density..,colour = variable)) + ggtitle('The distribution of density of tissue samples')+ xlab("Log2(normalized density)") + ylab("Density") ＃成功添加标题和xy坐标轴名称

#不用每次vim，这样就可以读取excel出来的，然后再也出来即可
a<-read.table("cancer_plasma.txt",head=T,row.names=1,sep="\t")

#抽取血浆中术后下降的
a<-read.table("total.txt",head=T)
res<-matrix(,nrow=2757,ncol=1)
for (i in 1:1000){
res<-cbind(res,a[sample(1:85677,2757),])
}
head(res)
res2<-res[,2:1001]
write.table(res2,"res_sampling_1000_2757.txt")

mean1<-apply(res2,2,mean)
median1<-apply(res2,2,median)
write.table(mean1,"mean1_downafteroperation.txt")
write.table(median1,"median1_downafteroperation.txt")


#density compare

library(ggplot2)
all<-read.table("down_other_total_permutaion1000.txt",head=T,sep="\t")
all<-all[2:1004]
library(reshape2)
a_melt <- melt(all)
ggplot(a_melt) + geom_density(aes(x = value,y = ..density..,colour = variable))
#结束 1003条也许真的太大了

#6permutations
library(ggplot2)
all<-read.table("down_other_total_permutaion6.txt",head=T,sep="\t")
library(reshape2)
a_melt <- melt(all)
ggplot(a_melt) + geom_density(aes(x = value,y = ..density..,colour = variable))

#交集补集等问题
Performs set union, intersection, (asymmetric!) difference, equality and membership on two vectors.

Usage

union(x, y)
intersect(x, y)
setdiff(x, y)
setequal(x, y)

is.element(el, set)
Arguments

x, y, el, set	
vectors (of the same mode) containing a sequence of items (conceptually) with no duplicated values.

Details

Each of union, intersect, setdiff and setequal will discard any duplicated values in the arguments, and they apply as.vector to their arguments (and so in particular coerce factors to character vectors).

is.element(x, y) is identical to x %in% y.

Value

A vector of the same mode as x or y for setdiff and intersect, respectively, and of a common mode for union.

A logical scalar for setequal and a logical of the same length as x for is.element.

See Also

%in%

‘plotmath’ for the use of union and intersect in plot annotation.

Examples

(x <- c(sort(sample(1:20, 9)), NA))
(y <- c(sort(sample(3:23, 7)), NA))
union(x, y)
intersect(x, y)
setdiff(x, y)
setdiff(y, x)
setequal(x, y)

## True for all possible x & y :
setequal( union(x, y),
          c(setdiff(x, y), intersect(x, y), setdiff(y, x)))

is.element(x, y) # length 10
is.element(y, x) # length  8




#抽取两列
a<-read.table("total_with_density.txt",head=T)
res<-matrix(,nrow=23628,ncol=2)
for (i in 1:10){
res<-cbind(res,a[sample(1:85677,23628),])
}
head(res)
%res2<-res[,2:1001]
mean1<-apply(res2,2,mean)
median1<-apply(res2,2,median)
write.table(mean1,"mean1.txt")
write.table(median1,"median1.txt") 

#利用ggplot画多个密度图的比较https://learnr.wordpress.com/2009/03/16/ggplot2-plotting-two-or-more-overlapping-density-plots-on-the-same-graph/
更多参数用这个http://www.sthda.com/english/wiki/ggplot2-density-easy-density-plot-using-ggplot2-and-r-statistical-software
#install.packages("reshape2")
library(ggplot2)
df <- data.frame(x = rnorm(1000, 0, 1), y = rnorm(1000,0, 2), z = rnorm(1000, 2, 1.5))
library(reshape2)
df.m <- melt(df)
ggplot(df.m) + geom_freqpoly(aes(x = value,y = ..density.., colour = variable))
ggplot(df.m) + geom_density(aes(x = value,y = ..density..,colour = variable))

ggplot(df.m) + geom_density(aes(x = value,
     colour = variable)) + labs(x = NULL) +
     opts(legend.position = "none") + opts(title = "Densities from a kernel density estimator")
#真正运行
library(ggplot2)
all<-read.table("total_overlapped_non_overlapped_permutaion6.txt",head=T,sep="\t")
all<-all[2:1004]
library(reshape2)
a_melt <- melt(all)
ggplot(a_melt) + geom_density(aes(x = value,y = ..density..,colour = variable))
#结束 1003条也许真的太大了

#rnorm(n, mean = 0, sd = 1)
Default plot + few formatting adjustments:

> ggplot(df.m) + geom_freqpoly(aes(x = value,
     y = ..density.., colour = variable)) +
     labs(x = NULL) + opts(legend.position = "none") +
     opts(title = "Frequency Polygons (based on binned counts)")
     
Update: Hadley kindly points out that the above plots are frequency polygons. I have updated the post with a “real” density plot.

> ggplot(df.m) + geom_density(aes(x = value,
     colour = variable)) + labs(x = NULL) +
     opts(legend.position = "none") + opts(title = "Densities from a kernel density estimator")


这个函数的功能比较强大，它首先将数据进行分组（按行），然后对每一组数据进行函数统计，最后把结果组合成一个比较nice的表格返回。根据数据对象不同它有三种用法，分别应用于数据框（data.frame）、公式（formula）和时间序列（ts）：

aggregate(x, by, FUN, ..., simplify = TRUE)
aggregate(formula, data, FUN, ..., subset, na.action = na.omit)
aggregate(x, nfrequency = 1, FUN = sum, ndeltat = 1, ts.eps = getOption("ts.eps"), ...)
先用attach函数把mtcars的列变量名称加入到变量搜索范围内，然后使用aggregate函数按cyl（汽缸数）进行分类计算平均值：


> attach(mtcars)
> aggregate(mtcars, by=list(cyl), FUN=mean)
Group.1 mpg cyl disp hp drat wt qsec vs am gear carb
1 4 26.66364 4 105.1364 82.63636 4.070909 2.285727 19.13727 0.9090909 0.7272727 4.090909 1.545455
2 6 19.74286 6 183.3143 122.28571 3.585714 3.117143 17.97714 0.5714286 0.4285714 3.857143 3.428571
3 8 15.10000 8 353.1000 209.21429 3.229286 3.999214 16.77214 0.0000000 0.1428571 3.285714 3.500000

by参数也可以包含多个类型的因子，得到的就是每个不同因子组合的统计结果：


> aggregate(mtcars, by=list(cyl, gear), FUN=mean)

Group.1 Group.2 mpg cyl disp hp drat wt qsec vs am gear carb
1 4 3 21.500 4 120.1000 97.0000 3.700000 2.465000 20.0100 1.0 0.00 3 1.000000
2 6 3 19.750 6 241.5000 107.5000 2.920000 3.337500 19.8300 1.0 0.00 3 1.000000
3 8 3 15.050 8 357.6167 194.1667 3.120833 4.104083 17.1425 0.0 0.00 3 3.083333
4 4 4 26.925 4 102.6250 76.0000 4.110000 2.378125 19.6125 1.0 0.75 4 1.500000
5 6 4 19.750 6 163.8000 116.5000 3.910000 3.093750 17.6700 0.5 0.50 4 4.000000
6 4 5 28.200 4 107.7000 102.0000 4.100000 1.826500 16.8000 0.5 1.00 5 2.000000
7 6 5 19.700 6 145.0000 175.0000 3.620000 2.770000 15.5000 0.0 1.00 5 6.000000
8 8 5 15.400 8 326.0000 299.5000 3.880000 3.370000 14.5500 0.0 1.00 5 6.000000

公式（formula）是一种特殊的R数据对象，在aggregate函数中使用公式参数可以对数据框的部分指标进行统计：


> aggregate(cbind(mpg,hp) ~ cyl+gear, FUN=mean)
cyl gear mpg hp
1 4 3 21.500 97.0000
2 6 3 19.750 107.5000
3 8 3 15.050 194.1667
4 4 4 26.925 76.0000
5 6 4 19.750 116.5000
6 4 5 28.200 102.0000
7 6 5 19.700 175.0000
8 8 5 15.400 299.5000 

一.变量的相关性分析
拿到数据后，如果是上面有明确目标的统计指标，表示变量关系是清楚的，可以跳过这一步骤。

数据相关性分析主要找到哪些变量间具有相关性，即变量间存在着相互影响的联系。

我主要使用三种方式: 散点图、相关性矩阵(Correlation matrix)和簇分类:

1. 批次替换data frame中的数据

     i. 将所有为0的数据替换为100

        res2$valueX[res2$valueX %in% 0]<-100 

     ii.将NA替换为0

         res2$valueX[is.na(res2$valueX)]<-0
10.分类

dataCluster<-function(data,col,clusterNum) {
  require("fpc")
  require(cluster) 
  
  z2<-na.omit(data[,col])
  
  km <- kmeans(z2, clusterNum)
  
  clusplot(data, km$cluster, color=TRUE, shade=TRUE, labels=2, lines=0)
}
效果:http://blog.csdn.net/horkychen/article/details/18355613
12. 以列的名称进行操作

以字段名的取值会提高应用的灵活性，如下所示:

  as.matrix(res[c('data')]) 等价于res$data

这种用法可以解决以列号指定数据时无法应对数据变化的问题。比如:

  keys<-c('data_sum','data1','data2')
  
  for(key in keys){
    data[c(key)]<-asNumeric(as.matrix(data[c(key)])) #转为数值型
    data[c(key)][is.na(data[c(key)]),1]<-0  #将所有NA赋为0
  }
＃函数是这么写的：myfun<- function(k){

t<-round(runif(k,1,72))
t
}

请问t为什么不能取t[1]或t[i]这种里面的一个数据，查看了t的类型，是function，如何让它变为向量呢？
>mode(t)
[1] "function"
> t[2]
错误于t[2] : 类别为'closure'的对象不可以取子集
ata(utils)
data()所属R语言包：utils

                                        Data Sets
                                         数据集

                                         译者：生物统计家园网 机器人LoveR

描述----------Description----------

Loads specified data sets, or list the available data sets.
加载指定数据集，或列出可用的数据集。


用法----------Usage----------


data(..., list = character(), package = NULL, lib.loc = NULL,
     verbose = getOption("verbose"), envir = .GlobalEnv)



参数----------Arguments----------

参数：...
a sequence of names or literal character strings.
名称或文字字符串序列。


参数：list
a character vector.
字符向量。
面这个是使用sm包的sm.density.compare函数，针对不同的线程，绘制其数据量的分布，可以看到各个线程被平均分配了，所以下载的数据量变化不大。
R代码如下:
 library(sm)
 sm.density.compare(data$dataSize,data$threads,xlab='datasize vs. threads')

你调用myfun得到的结果需要赋值给一个变量， 而不是直接在外层环境里面调用函数中的参数
比如
t = myfun(10)

这样就可以了。

2. t()在r里面就是一个函数，赋值最好不用r已有的名字
1. 如果是数组（array）类型，melt 的用法就很简单，它依次对各维度的名称进行组合将 数据进行线性/向量化。如果数组有 n 维，那么得到的结果共有 n+1 列，前 n 列记录数组的 位置信息，最后一列才是观测值。


> datax <- array(1:8, dim=c(2,2,2))
> melt(datax)
    Var1 Var2 Var3 value
        1    1    1    1     1
        2    2    1    1     2
        3    1    2    1     3
        4    2    2    1     4
        5    1    1    2     5
        6    2    1    2     6
        7    1    2    2     7
        8    2    2    2     8 

melt就可以把数据变到同一列了并且对每一列进行标记
#for circRNAs distribution comaparison

par(lwd=2)

attach(mtcars)
library(sm)
head(mtcars)
cyl.f <- factor(cyl,levels=c(4,6,8))
sm.density.compare(mpg,cyl,xlab="Miles Per Gallon") 对mpg进行作图，以cyl为为分组
title(main="MPG Distribution by car cylinders")
cilfill<-c(2:(1+length(levels(cyl.f))))
legend(locator(1),fill=cilfill,title="circRNAs Classification",c("4 cylinder","6 cylinder","8 cylinder")) 


#比较密度图

install.packages("sm")
par(lwd=2)
attach(mtcars)
library(sm)
head(mtcars)
cyl.f <- factor(cyl,levels=c(4,6,8))
sm.density.compare(mpg,cyl,xlab="Miles Per Gallon")
title(main="MPG Distribution by car cylinders")
cilfill<-c(2:(1+length(levels(cyl.f))))
legend(locator(1),fill=cilfill,title="circRNAs Classification",c("4 cylinder","6 cylinder","8 cylinder"))
#
par(lwd = 2)
library(sm)

cyl.f <- factor(mtcars$cyl,levels = c(4,6,8),
                labels = c("4 cylinder","6 cylinder","8 cylinder"))

sm.density.compare(mtcars$mpg,mtcars$cyl,xlab = "Miles Per Gallon")
title(main = "MPG Distribution by Car Clyinders")
colfill <- c(2:(1+length(levels(cyl.f))))  #注意这里有一个levels函数，提取因子中的levels
legend(locator(1),levels(cyl.f),fill=colfill)  #这里的locator函数是用鼠标点击确定位置
＃ table居然可以统计多个变量
counts <- table(Arthritis$Improved, Arthritis$Treatment) 

＃r中lwd=10，表示线条宽度为默认宽度的10倍
attach(mtcars) #attach将数据框添加到R的搜索路径，R遇到一个变量名后，松林搜索路径中的数据框，以定位于这个变量。
r library(sm)
sm: Smoothing methods for nonparametric regression and density estimation
> ?sm.density.compare
sm.density.compare             package:sm              R Documentation

Comparison of univariate density estimates

Description:

     This function allows a set of univariate density estimates to be
     compared, both graphically and formally in a permutation test of
     equality.

Usage:

     sm.density.compare(x, group, h, model = "none",  ...)
     
Arguments:

       x: a vector of data.

   group: a vector of group labels.

       h: the smoothing parameter to be used in the construction of
          each density estimate.  Notice that the same smoothing
          parameter is used for each group. If this value is omitted,
          the mean of the normal optimal values for the different
          groups is used.

   model: the default value is ‘"none"’ which restricts comparison to
          plotting only. The alternative value ‘"equal"’ can produce
          a bootstrap hypothesis test of equality and the display of an
          appropriate reference band.

     ...: other optional parameters are passed to the ‘sm.options’
          function, through a mechanism which limits their effect only
          to this call of the function. Those specifically relevant for
          this function are the following: ‘method’, ‘df’,

locator(graphics)
locator()所属R语言包：graphics

                                        Graphical Input
                                         图形输入

                                         译者：生物统计家园网 机器人LoveR

描述----------Description----------

Reads the position of the graphics cursor when the (first) mouse button is pressed.
（一）鼠标按钮被按下时读取图形光标的位置。


用法----------Usage----------


locator(n = 512, type = "n", ...)



参数----------Arguments----------

参数：n
the maximum number of points to locate.  Valid values start at 1.
定位点的最大数量。有效值从1开始。


参数：type
One of "n", "p", "l" or "o". If "p" or "o" the points are plotted; if "l"  or "o" they are joined by lines.
一个"n"，"p"，"l"或"o"。如果"p"或"o"点绘制;如果"l"或"o"他们是由行加入。


参数：...
additional graphics parameters used if type != "n" for plotting the locations.
额外的图形使用的参数，如果type != "n"绘制的位置。
> ?legend
legend                package:graphics                 R Documentation

Add Legends to Plots

Description:

     This function can be used to add legends to plots.  Note that a
     call to the function ‘locator(1)’ can be used in place of the
     ‘x’ and ‘y’ arguments.

Usage:

     legend(x, y = NULL, legend, fill = NULL, col = par("col"),
            border = "black", lty, lwd, pch,
            angle = 45, density = NULL, bty = "o", bg = par("bg"),
            box.lwd = par("lwd"), box.lty = par("lty"), box.col = par("fg"),
            pt.bg = NA, cex = 1, pt.cex = cex, pt.lwd = lwd,
            xjust = 0, yjust = 1, x.intersp = 1, y.intersp = 1,
            adj = c(0, 0.5), text.width = NULL, text.col = par("col"),
            text.font = NULL, merge = do.lines && has.pch, trace = FALSE,
            plot = TRUE, ncol = 1, horiz = FALSE, title = NULL,
            inset = 0, xpd, title.col = text.col, title.adj = 0.5,
            seg.len = 2)
     
Arguments:

    x, y: the x and y co-ordinates to be used to position the legend.
          They can be specified by keyword or in any way which is
          accepted by ‘xy.coords’: See ‘Details’.

  legend: a character or expression vector of length >= 1 to appear in
          the legend.  Other objects will be coerced by
          ‘as.graphicsAnnot’.

    fill: if specified, this argument will cause boxes filled with the
          specified colors (or shaded in the specified colors) to
          appear beside the legend text.

     col: the color of points or lines appearing in the legend.

  border: the border color for the boxes (used only if ‘fill’ is
          specified).
    
#随机抽取表达值
a<-read.table("total.txt",head=T)
res<-matrix(,nrow=23628,ncol=1)
for (i in 1:1000){
res<-cbind(res,a[sample(1:85677,23628),])
}
head(res)
write.table(res,"res_sampling_1000_23628.txt")
res2<-res[,2:1001]
mean1<-apply(res2,2,mean)
median1<-apply(res2,2,median)
write.table(mean1,"mean1.txt")
write.table(median1,"median1.txt") 
 
 新手请教随机抽样的R实现
 比如我有以下数据：
X   Y
3   5
6   8
4   2
7   9
1   0
我想随机抽3对（x,y）出来，抽10次，请问怎么用R实现？谢谢
XY<-matrix(c(3,6,4,7,1,5,8,2,9,0),5,2)
假设你的XY 如上表所示排成一个矩阵XY
SampXY<-XY[sample(nrow(XY),3),] #


谢谢楼上，我终于解决了，代码可以是：
x<-c(3,6,4,7,1)
y<-c(5,8,2,9,0)
xy<-cbind(x,y)
xy_sample<-function(xy){  #xy是处理对象
mysample<-xy[sample(1:5,3),]  ＃这句是核心公式，sample可以随机生成1:5这个范围内的数，3是抽出3个，sample(1:5,3)是一个index，然后对数据进行xy[index,]按行抽取
return(mysample)
}
res<-matrix(,nrow=3,ncol=2)
for (i in 1:10){
res[]=xy_sample(xy) ＃res[]总被替换，所以出来的数是一样的，改成res<-cbind(res,xy_sample(xy))
}
res
write.table(res,"res_sampling_10_3.txt")
## 自己更改后的，不用定义函数直接使用
x<-c(3,6,4,7,1)
y<-c(5,8,2,9,0)
xy<-cbind(x,y)
res<-matrix(,nrow=3,ncol=2)
for (i in 1:10){
res<-cbind(res,xy[sample(1:5,3),])
}
res
write.table(res,"res_sampling_10_3.txt")

？sample
  x <- 1:12 
     # a random permutation
     sample(x)  #平时想打乱排序可以直接这么做
     # bootstrap resampling -- only if length(x) > 1 !
     sample(x, replace = TRUE)
     
     # 100 Bernoulli trials
     sample(c(0,1), 100, replace = TRUE)
     
     ## More careful bootstrapping --  Consider this when using sample()
     ## programmatically (i.e., in your function or simulation)!
     
     # sample()'s surprise -- example
     x <- 1:10
         sample(x[x >  8]) # length 2
         sample(x[x >  9]) # oops -- length 10!
         sample(x[x > 10]) # length 0
     
     resample <- function(x, ...) x[sample.int(length(x), ...)]
     resample(x[x >  8]) # length 2
     resample(x[x >  9]) # length 1
     resample(x[x > 10]) # length 0
     
     

如何编写R函数

已有 3803 次阅读 2011-10-26 19:28 |个人分类:统计分析|系统分类:论文交流|关键词:函数, R语言

如何编写R函数？

张金龙

jinlongzhang01@gmail.com

   R语言实际上是函数的集合，用户可以使用base，stats等包中的基本函数，也可以自己编写函数完成一定的功能。但是初学者往往认为编写R函数十分困难，或者难以理解。这里对如何编写R函数进行简要的介绍。

   函数是对一些程序语句的封装。换句话说，编写函数，可以减少人们对重复代码书写，从而让R脚本程序更为简洁，高效。同时也增加了可读性。一个函数往往完成一项特定的功能。例如，求标准差sd,求平均值，求生物多样性指数等。R数据分析，就是依靠调用各种函数来完成的。但是编写函数也不是轻而易举就能完成的，需要首先经过大量的编程训练。特别是对R中数据的类型，逻辑判别、下标、循环等内容有一定了解之后，才好开始编写函数。 对于初学者来说，最好的方法就是研究现有的R函数。因为R程序包都是开源的，所有代码可见。研究现有的R函数能够使编程水平迅速提高。

   R函数无需首先声明变量的类型，大部分情况下不需要进行初始化。一个完整的R函数，需要包括函数名称，函数声明，函数参数以及函数体几部分。

1. 函数名称，即要编写的函数名称，这一名称就作为将来调用R函数的依据。

2. 函数声明，包括  <- function, 即声明该对象的类型为函数。

3. 函数参数，这里是输入的数据，函数参数是一个虚拟出来的一个对象。函数参数所等于的数据，就是在函数体内部将要处理的值，或者对应的数据类型。 函数体内部的程序语句进行数据处理，就是对参数的值进行处理 ，这种处理只在调用函数的时候才会发生。函数的参数可以有多种类型。R help的界面对每个函数，及其参数的意义及所需的数据类型都进行了说明。

4. 函数体

常常包括三部分.

（1）. 异常处理

输入的数据不能满足函数计算的要求，或者类型不符， 这时候一定要设计相应的机制告诉用户，输入的数据在什么地方有错误。 错误又分为两种。

第一种， 如果输入的数据错误不是很严重，可以经过转换，变为符合处理要求的数据时， 此时只需要给用户一个提醒，告知数据类型不符，但是函数本身已经 进行了相应的转换。

第二种，数据完全不符合要求，这种情况下，就 要终止函数的运行，而告知因为什么，函数不能运行。这样，用户在 使用函数的情况先才不至于茫然。

（2）. 运算过程

包括具体的运算步骤。 运算过程和该函数要完成的功能有关。

R运算过程中，应该尽量减少循环的使用，特别是嵌套循环。R提供了 apply，replicate等一系列函数，来代替循环，应该尽量应用这些函数， 提高效率。 如果在R中实在太慢，那么核心部分只能依靠C或者Fortran 等语言编写，然后再用R调用这些编译好的模块，达到更高的效率。

运算过程中，需要大量用到if等条件作为判别的标准。if和while都是需要数据TRUE/FALSE这样的逻辑类型变量，这就意味着，if内部，往往是对条件的判别，例如 is.na, is.matrix, is.numeric等等，或者对大小的比较，如，if(x > 0)， if(x == 1)， if(length(x)== 3)等等。if后面，如果是1行，则花括号可以省略，否则就必须要将所有的语句都放在花括号中。这和循环是一致的。

例子1

## if与条件判断

fun.test <- function(a, b, method = "add"){

    if(method == "add") { ## 如果if或者for/while；

        res <- a + b       ## 等后面的语句只有一行，则无需使用花括号。

}

    if(method == "subtract"){

        res <- a - b

    }

    return(res)           ## 返回值

}

### 检验结果

fun.test(a = 10, b = 8, method = "add")

fun.test(a = 10, b = 8, method = "substract")

 

for循环有些时候是必须要用到的，for循环内部，往往需要用下标，访问数据内的一定元素，例如向量内的元素，这时候用方括号表示。一维的数据组合，或者数组，常常称为向量。二维的数据组合，往往称为矩阵，或者数据框。具体的访问方式主要是方括号内部有没有逗号的区别。for循环或者while循环有时候让人觉得比较困惑，可能需要专门的时间进行讲解。

例2

### for循环与算法

test.sum <- function(x)

{

    res <- 0               ###  设置初始值，在第一次循环的时候使用

    for(i in 1:length(x)){

        res <- res + x[i] ## 这部分是算法的核心，

##总是总右面开始计算，结果存到左边的对象

    }

    return(res)

}

 

### 检验函数

a <- c(1,2,1,6,1,8,9,8)

test.sum(a)

sum(a)

 

无论是什么样的函数，算法才是最关键的。往往需要巧妙得设计算法，让函数快捷高效。

（3）. 返回值。

返回值就是函数给出的结果。打个比方，编写一个函数，就像自己攒一个机器，例如现在攒好 一台豆浆机，该豆浆机要求输入大豆，输入的大豆就是参数， 返回的结果，就是豆浆。如果该豆浆机需要不停地输入大豆， 而不能产出豆浆，这样的机器就一定会被扔掉。函数也是一样的， 需要给出返回值。 R中默认的情况是将最后一句作为返回值。但是为了函数的可读性起见，应该尽量指名返回值。返回值用return()函数给出。 函数在内部处理过程中，一旦遇到return()，就会终止运行， 将return()内的数据作为函数处理的结果给出。

下面举例说明R函数的编写方法。

例3 计算标准差

sd2 <- function(x)

{

   # 异常处理，当输入的数据不是数值类型时报错   

   if(!is.numeric(x)){                          

      stop("the input data must be numeric!\n") 

   }                                            

   # 异常处理，当仅输入一个数据的时候，告知不能计算标准差

   if(length(x) == 1){                          

      stop("can not compute sd for one number,

           a numeric vector required.\n")       

   }

   ## 初始化一个临时向量，保存循环的结果，

   ## 求每个值与平均值的平方  

   x2 <- c()

   ## 求该向量的平均值 

   meanx <- mean(x)

   ## 循环 

   for(i in 1:length(x)){   

       xn <- x[i] - meanx  

       x2[i] <- xn^2

   }

      ## 求总平方和

   sum2 <- sum(x2)

   # 计算标准差

   sd <- sqrt(sum2/(length(x)-1))

   # 返回值

   return(sd)

}

 

## 程序的检验

## 正常的情况

sd2(c(2,6,4,9,12))

## 一个数值的情况

sd2(3)

## 输入数据不为数值类型时

sd2(c("1", "2"))

 

这样，一个完整的函数就编写完成了。当然，实际情况下，函数往往更为复杂，可能要上百行。但是好的编程人员往往将复杂的函数编写成小的函数。以便于程序的修改和维护，即使其中出现错误，也很好修改。

再有就是编写R函数时一定要注意缩进，编辑器用Notepad++, TinnR, Rstudio等，同时用等距字体（如Consolas, Courier new等）和语法高亮显示。这样便于快速寻找到其中的错误。



 
 R语言学习笔记--调用外部文件 (2012-11-15 23:16:27)
转载

标签： 杂谈 	

一、R读文件

1、读文本文件数据

先设置工作目录，把文本文件放于该目录下

(设置工作目录的方法：选择R软件工具栏”文件”―->”改变工作目录”。建议在开始使用R软件时就选择工作目录)

x=read.table(“abc.txt”)

如果文件不在工作目录下，写绝对路径并使用”\\”(一个\为转义字符)，如：c:\\tmp\\abc.txt

> x=read.table("abc.txt") > x V1 V2 1 175 67 2 183 75 3 165 56 4 145 45 5 178 67 6 187 90 7 156 43 8 176 58 9 173 60 10 170 56

5、读剪切板

文本或excel的数据均可通过剪贴板操作

y <- read.table("clipboard",header=F)

header=F 表示不读列头

> y <- read.table("clipboard",header=F) > y V1 V2 1 175 67 2 183 75 3 165 56 4 145 45 5 178 67 6 187 90 7 156 43 8 176 58 9 173 60 10 170 56 > z <- read.table("clipboard",header=T) > z 商品 价格 1 A 2 2 B 3 3 C 5 4 D 5

6、读Excel文件数据

方法1：先把excel另存为空格分隔的prn或”,”分隔的csv文本格式再读

w <- read.table("test.prn",header=T)

> w <- read.table("test.prn",header=T) > w 商品 价格 1 A 2 2 B 3 3 C 5 4 D 5 > q <- read.table("test.csv",header=T) > q 商品.价格 1 A,2 2 B,3 3 C,5 4 D,5 >

方法2：安装RODBC包，再通过ODBC读

library(RODBC)
z <- odbcConnectExcel("text.xls")
w <- sqlFetch(z,"Sheet1")

二、综合性例子

模拟产生统计专业同学的名单（学号区分），记录数学分析，线性代数，概率统计三
科成绩，然后进行一些统计分析。

1、模拟生成100名学生分数

> num=seq(10378001,10378100) > num [1] 10378001 10378002 10378003 10378004 10378005 10378006 10378007 10378008 [9] 10378009 10378010 10378011 10378012 10378013 10378014 10378015 10378016 [17] 10378017 10378018 10378019 10378020 10378021 10378022 10378023 10378024 [25] 10378025 10378026 10378027 10378028 10378029 10378030 10378031 10378032 [33] 10378033 10378034 10378035 10378036 10378037 10378038 10378039 10378040 [41] 10378041 10378042 10378043 10378044 10378045 10378046 10378047 10378048 [49] 10378049 10378050 10378051 10378052 10378053 10378054 10378055 10378056 [57] 10378057 10378058 10378059 10378060 10378061 10378062 10378063 10378064 [65] 10378065 10378066 10378067 10378068 10378069 10378070 10378071 10378072 [73] 10378073 10378074 10378075 10378076 10378077 10378078 10378079 10378080 [81] 10378081 10378082 10378083 10378084 10378085 10378086 10378087 10378088 [89] 10378089 10378090 10378091 10378092 10378093 10378094 10378095 10378096 [97] 10378097 10378098 10378099 10378100 >

使用seq()函数生成1-100个学号，代表100名学生。

> x1=round(runif(100,min=80,max=100)) > x1 [1] 99 90 81 83 89 99 80 89 98 97 88 81 96 96 85 98 89 92 [19] 82 82 84 98 95 91 91 94 87 80 95 91 83 91 93 88 96 85 [37] 90 80 97 94 100 97 91 89 85 96 93 96 89 94 96 90 81 91 [55] 99 82 83 93 89 91 81 95 81 90 81 85 100 89 81 89 83 81 [73] 87 92 97 82 80 86 89 88 87 89 86 83 82 92 89 88 97 82 [91] 84 95 81 88 88 98 82 92 90 99 >

runif()函数生成均匀分布函数，最大值100，最小值80，代表分数。
round()函数四舍五入取整。

> x2=round(rnorm(100,mean=80,sd=7)) > x2 [1] 71 82 76 85 67 84 84 83 75 74 66 91 87 76 76 83 81 78 76 91 87 92 80 89 [25] 75 76 78 77 79 84 80 89 90 69 78 89 85 76 90 77 78 79 82 87 87 68 78 79 [49] 79 83 76 79 63 87 84 76 85 89 69 74 85 81 83 76 84 76 81 70 79 84 93 90 [73] 83 88 86 87 93 82 75 71 75 69 79 86 77 75 88 83 81 84 93 78 75 89 79 79 [97] 84 82 88 70 >

rnorm()函数生成正态分布函数，总数100，均值为80，标准差为7。代表另一科成绩。

> x3=round(rnorm(100,mean=83,sd=18)) > x3 [1] 110 55 74 72 82 81 101 89 76 84 75 81 72 51 85 77 61 103 [19] 86 105 88 86 74 39 97 60 69 50 89 67 51 74 96 83 85 116 [37] 32 80 59 88 64 76 80 104 110 71 65 74 57 91 72 68 50 101 [55] 103 106 81 119 73 76 83 77 93 87 65 65 70 104 133 89 73 101 [73] 80 98 91 79 79 79 74 90 93 93 82 95 99 117 84 69 88 114 [91] 78 63 56 95 69 72 68 91 71 75 >

生成100个均值为83，标准差为18的学科成绩。

> x3[which(x3>100)]=100 > x3 [1] 100 55 74 72 82 81 100 89 76 84 75 81 72 51 85 77 61 100 [19] 86 100 88 86 74 39 97 60 69 50 89 67 51 74 96 83 85 100 [37] 32 80 59 88 64 76 80 100 100 71 65 74 57 91 72 68 50 100 [55] 100 100 81 100 73 76 83 77 93 87 65 65 70 100 100 89 73 100 [73] 80 98 91 79 79 79 74 90 93 93 82 95 99 100 84 69 88 100 [91] 78 63 56 95 69 72 68 91 71 75 >

分数大于100的设为100。

> x=data.frame(num,x1,x2,x3) > x num x1 x2 x3 1 10378001 99 71 100 2 10378002 90 82 55 3 10378003 81 76 74 4 10378004 83 85 72 5 10378005 89 67 82 6 10378006 99 84 81 7 10378007 80 84 100 8 10378008 89 83 89 9 10378009 98 75 76 10 10378010 97 74 84 11 10378011 88 66 75 12 10378012 81 91 81 13 10378013 96 87 72 14 10378014 96 76 51 15 10378015 85 76 85 16 10378016 98 83 77 17 10378017 89 81 61 18 10378018 92 78 100 19 10378019 82 76 86 20 10378020 82 91 100 21 10378021 84 87 88 22 10378022 98 92 86 23 10378023 95 80 74 24 10378024 91 89 39 25 10378025 91 75 97 26 10378026 94 76 60 27 10378027 87 78 69 28 10378028 80 77 50 29 10378029 95 79 89 30 10378030 91 84 67 31 10378031 83 80 51 32 10378032 91 89 74 33 10378033 93 90 96 34 10378034 88 69 83 35 10378035 96 78 85 36 10378036 85 89 100 37 10378037 90 85 32 38 10378038 80 76 80 39 10378039 97 90 59 40 10378040 94 77 88 41 10378041 100 78 64 42 10378042 97 79 76 43 10378043 91 82 80 44 10378044 89 87 100 45 10378045 85 87 100 46 10378046 96 68 71 47 10378047 93 78 65 48 10378048 96 79 74 49 10378049 89 79 57 50 10378050 94 83 91 51 10378051 96 76 72 52 10378052 90 79 68 53 10378053 81 63 50 54 10378054 91 87 100 55 10378055 99 84 100 56 10378056 82 76 100 57 10378057 83 85 81 58 10378058 93 89 100 59 10378059 89 69 73 60 10378060 91 74 76 61 10378061 81 85 83 62 10378062 95 81 77 63 10378063 81 83 93 64 10378064 90 76 87 65 10378065 81 84 65 66 10378066 85 76 65 67 10378067 100 81 70 68 10378068 89 70 100 69 10378069 81 79 100 70 10378070 89 84 89 71 10378071 83 93 73 72 10378072 81 90 100 73 10378073 87 83 80 74 10378074 92 88 98 75 10378075 97 86 91 76 10378076 82 87 79 77 10378077 80 93 79 78 10378078 86 82 79 79 10378079 89 75 74 80 10378080 88 71 90 81 10378081 87 75 93 82 10378082 89 69 93 83 10378083 86 79 82 84 10378084 83 86 95 85 10378085 82 77 99 86 10378086 92 75 100 87 10378087 89 88 84 88 10378088 88 83 69 89 10378089 97 81 88 90 10378090 82 84 100 91 10378091 84 93 78 92 10378092 95 78 63 93 10378093 81 75 56 94 10378094 88 89 95 95 10378095 88 79 69 96 10378096 98 79 72 97 10378097 82 84 68 98 10378098 92 82 91 99 10378099 90 88 71 100 10378100 99 70 75 > write.table(x,"D:\\mark.txt",col.names=F,row.names=F,sep=" ") >

合成数据框并保存到硬盘。

data.frame()函数用于生成数据框。
write.table()将数据框写到文件中。col.names=F:不写列名；row.names=F:不写行名；sep=” “:用空格做分隔符。

2、计算各科平均分

使用函数：mean(),colMeans(),apply()

> mean(x) num x1 x2 x3 10378050.50 89.31 80.69 79.79 警告信息： mean() is deprecated. Use colMeans() or sapply(*, mean) instead. > colMeans(x) num x1 x2 x3 10378050.50 89.31 80.69 79.79 > colMeans(x)[c("x1","x2","x3")] x1 x2 x3 89.31 80.69 79.79 > apply(x,2,mean) num x1 x2 x3 10378050.50 89.31 80.69 79.79 >

colMeans():计算每列的平均值。

单用colMeans()将学号也做了平均值计算，colMeans(x)[c("x1","x2","x3")]只计算科目成绩。

3、计算各科最高最低分

> apply(x,2,max) num x1 x2 x3 10378100 100 93 100 > apply(x,2,min) num x1 x2 x3 10378001 80 63 32 >

apply(x,2,mean)也用来计算平均值，第二个参数2代表计算每列，如果按行计算，则第二个参数为1。

apply(x,2,max)计算每列的最大值；apply(x,2,min)计算每列的最小值。

4、计算每人总分

> apply(x[c("x1","x2","x3")],1,sum) [1] 270 227 231 240 238 264 264 261 249 255 229 253 255 223 246 258 231 270 [19] 244 273 259 276 249 219 263 230 234 207 263 242 214 254 279 240 259 274 [37] 207 236 246 259 242 252 253 276 272 235 236 249 225 268 244 237 194 278 [55] 283 258 249 282 231 241 249 253 257 253 230 226 251 259 260 262 249 271 [73] 250 278 274 248 252 247 238 249 255 251 247 264 258 267 261 240 266 266 [91] 255 236 212 272 236 249 234 265 249 244 >

三、分布函数

正态分布函数：rnorm()
泊松分布函数：rpois()
指数分布函数：rexp()
Gamma分布函数：rgamma()
均匀分布函数：runif()
二项分布函数：rbinom()
几何分布函数：rgeom()


a<-read.table("p1.txt",head=T,sep="\t")
pdf("p1_scatter2.pdf")
plot(a$Plasma,a$Tumor,lty=2,pch=20,cex=0.5,xlab="log2(normalized density) of plasma",ylab="log2(normalized density) of tumor")
abline(lm(a$Tumor~a$Plasma),col="blue")
dev.off()

# [程序问答] 如何提取cor.test(X,Y,method=""）最后得出的cor [推广有奖] 
cor.test(x,y)$estimate
cor.test(a$Tumor,a$Plasma)[3]就可以
还可以获得相关系数的可信度区间和P-value

perl circos -conf <conf.file>
孟凡琳  19:05:08
就是这样的命令

library(limma)
hsb2 <- read.table("patient1_before.txt",header=T)
attach(hsb2)
hw <- (write <-hsb2$Detected_in_cancer)
hm <- (math <-hsb2$Detected_in_plasma)
hr <- (read <-hsb2$Up_regulated_in_cancer)
hn <- (nil <-hsb2$Down_regulated_in_plasma)
c3 <- cbind(hw, hm, hr,hn)
a <- vennCounts(c3)
vennDiagram(a, include = "both", names = c("High Writing", "High Math", "High Reading","High nil"), cex = 1, counts.col = "red")



library(grid)
library(VennDiagram)
mydata1<-read.table("Detected_in_cancer.txt",head=T)
mydata2<-read.table("Detected_in_plasma.txt",head=T)
mydata3<-read.table("Up_regulated_in_cancer.txt",head=T)
mydata4<-read.table("Down_regulated_in_plasma.txt",head=T)
venn.diagram(list("Detected_in_cancer"=mydata1$Detected_in_cancer,"Detected_in_plasma"=mydata2$Detected_in_plasma,"Up_regulated_in_cancer"=mydata3$Up_regulated_in_cancer,"Down_regulated_in_plasma"=mydata4$Down_regulated_in_plasma),fill=c("blue","yellow","paleturquoise2","thistle1"),"/Users/lishasha/Desktop/home/project/2015_new_work/circRNA/plasm_cancer_compare/plasma_circRNAs_were_from_cancer/patient1_before.png",main="overlap in 4 samples",main.col = "black",main.cex =1.5,cex.labels=0.6, font.labels=4)


LINUX, *NIX, MAC OS X

To install the missing module, use the perl CPAN shell. At the terminal,

> perl -MCPAN -e shell
...
cpan[1]> install Missing::Module
...
... lengthy output here, should end with a message like
...
... /usr/bin/make install  -- OK
...
cpan[2]> exit
>
If you attempt to install a module you already have an up-to-date version for CPAN will duly inform you.

cpan[1]> install Already::Installed::Module
Already::Installed::Module is up to date (1.00)
Except for GD, installing the modules should cause you no problems.

On Mac OS X you will need to install the XCode tools (which, among others, provide 'make') in order to compile modules like GD. These tools are available on the Mac OS X distribution DVD as an extra installation package.

WINDOWS

Your Perl distribution comes with a package manager. Use it to install modules.

GD
Installing GD can be tricky, since it depends on your system's C libgd library, among others. Getting all the dependency ducks in a row ranges across the full scale of pain, from "wow that was fast" to "omg omg we're all going to die".


＃查看perl安装的模块
有些时候需要查看系统下，安装了哪些perl模块。可以通过下面的方式来查看：
   Bundle::NetSNMP
   CPAN::Meta
   CPAN::Meta::YAML
   Carp
   Clone
   Config::General
   Cwd
   Data::Dumper
   Digest::MD5
   Exporter::Tiny
   ExtUtils::MakeMaker
   File::Temp
   Font::TTF
   Getopt::Long
   List::MoreUtils
   List::Util
   Math::Round
   Math::VecStat
   Module::Build
   Parse::CPAN::Meta
   Perl
   Test::Harness
   local::lib
  
1.  perllocal
# perldoc perllocal
1
	
# perldoc perllocal
不行
2.  instmodsh
查看全部模块
该指令是由ExtUtils::Installed模块提供的一个可执行脚本, 用来查看当前安装的模块信息。
Linux中查看是否安装perl模块

这里介绍两种linux中查看perl模块是否安装的方法，一种是对于单体的模块，一种是对于群体的。

单体验证：

    [root@root ~]# perl -MShell -e "print\"module installed\n\"" 查看指定模块
    module installed

这里使用-M后边紧跟着Shell这个perl模块，如果输出module installed结果。那么此模块是存在在系统中的。

    [root@root ~]# perl -MMail::Sender -e "print\"module installed\n\""
    Can't locate Mail/Sender.pm in @INC (@INC contains: /usr/local/lib/perl5 /usr/local/share/perl5 /usr/lib/perl5/vendor_perl /usr/share/perl5/vendor_perl /usr/lib/perl5 /usr/share/perl5 .).
    BEGIN failed--compilation aborted.

如果出现类似于这种Can't locate。。。的提示，那么证明你系统中没有安装此模块。

 

群体验证：

这里所谓的群体验证只有一种方式，那么就是使用一个脚本来输出系统中所有已安装的perl脚本：

    #!/usr/bin/perl
    use strict;
    use ExtUtils::Installed;

    my $inst = ExtUtils::Installed->new();

    my @modules = $inst->modules();

    foreach  (@modules) {
            my  $ver = $inst->version($_) || "???";
            printf("%-22s -Version- %-22s\n", $_, $ver);
    }
    exit;

运行得到的结果为：

    DBD::Oracle           -Version- 1.16                  
    DBI                       -Version- 1.611                 
    ExtUtils::Install        -Version- 1.54                  
    Perl                       -Version- 5.8.8  

我们也可以使用grep函数来输出单个模块是否安装，但是需要开发自己的脑筋哟~
例如查看到了
perl -MFile::Basename -e "print\"module installed\n\""
module installed

＃＃＃circos安装
闲来无事，翻看CELL杂志，发现很多基因组的图都使用circos来作图，于是就去circos.ca网上看了一眼，发现果然是个基因组研究绘图强大工具，应该为生物信息员掌握。于是花了点时间，来试验它的每一个设置。上网搜索发现，它的中文资料少得可怜，于是将心得体会做一总结，形成这一系列教程。希望能对急于掌握circos又不擅长阅读英语的人有所帮助。－－糗世界之糗糗
下载与安装

下载地址：http://circos.ca/software/download/circos/

circos是基于perl的脚本程序。它的安装难度在于安装好perl以及它所需要的模块。对于windows用户，可以试着安装Strawberry Perl或者ActiveState Perl。这两者都是不错的选择。对于Unix/linux/MacOS用户，很可能你已经安装了perl，否则，你可以到http://www.perl.org/get.html去下载安装。

我们需要测试一下perl的环境， UNIX/Linux/MacOS用户
> which perl /usr/bin/perl > perl -v This is perl, v5.10.0 built for...

Windows用户
> perl -v This is perl, v5.10.0 built for...

接着，我们将下载下来的circos程序解压，假设它的目录是circos-x.xx
> cd circos-x.xx > bin/circos -man

你可能得到一个帮助页面，那么你安装circos已经成功。也可能得到是出错信息。

常见错误1：
-bash:/bin/env: No such file or directory

对于MacOS用户或者遇到这一提示的用户的问题是，env的安装目录并非典型的/bin/env，而是/usr/bin/env。我们需要将env文件的位置设置正确。
> which env /usr/bin/env

现在有两个解决办法，第一个是将所有bin/*或者tools/*/bin文件中第一行写有
#!/bin/env perl

改成
#!/usr/bin/env perl

第二个办法（可能更方便）是在bin下建立一个env文件的链接。
> cd /bin > sudo ln -s/usr/bin/env env
#-bash: sudo: Permission denied
lishashadeiMac:bin lishasha$ sudo cd /bin > ln -s/usr/bin/env env
-bash: ln: Permission denied
第二种方法无权限
获得权限

　　为了防止误操作破坏系统，再用户状态下时没有权限操作系统重要文件的，所以先要取得root权限：“sudo -s”
sudo -s
cd /bin > ln -s/usr/bin/env env
cd /bin > sudo ln -s/usr/bin/env env
常见错误2：
Can't locate Config/General.pm in @INC ...

无论是什么pm文件，如果出现这类错误，那就说明相应的模块没有安装。Circos需要以下模块
Config::General (v2.50 or later) GD GD::Polyline List::MoreUtils Math::Bezier Math::Round Math::VecStat Params::Validate Readonly Regexp::Common Set::IntSpan (v1.16 or later)

如果是Window用户，Strawberry Perl和ActiveState Perl都有安装包管理程序，你可以很轻松的从CPAN安装上面的这些模块。如果是UNIX/Linux/MacOS用户，需要使用CPAN shell安装。
对于UNIX/Linux/MacOS用户，可以使用circos包中bin/test.modules来测试需要安装那些包
> cd bin >./test.modules
#并没有得到什么信息，所以运行夏明的镜像那个命令即可
对提示有failed模块，我们进入shell安装。
> sudo perl -MCPAN -e shell #这里需要root权限，否则你有可能得到permission denied error> install Config::General#缺少什么模块就install什么模块，这里的Config::General只是个示例
local::lib is installed. You must now add the following environment variables
to your shell configuration files (or registry, if you are on Windows) and
then restart your command line shell and CPAN before installing modules:

PATH="/Users/lishasha/perl5/bin${PATH+:}${PATH}"; export PATH;
PERL5LIB="/Users/lishasha/perl5/lib/perl5${PERL5LIB+:}${PERL5LIB}"; export PERL5LIB;
PERL_LOCAL_LIB_ROOT="/Users/lishasha/perl5${PERL_LOCAL_LIB_ROOT+:}${PERL_LOCAL_LIB_ROOT}"; export PERL_LOCAL_LIB_ROOT;
PERL_MB_OPT="--install_base \"/Users/lishasha/perl5\""; export PERL_MB_OPT;
PERL_MM_OPT="INSTALL_BASE=/Users/lishasha/perl5"; export PERL_MM_OPT;
Would you like me to append that to /Users/lishasha/.bashrc now? [yes] 
yes 以后就安装成功了
然后
install Config::General就自己安装Config::General了，还是比较方便的 成功
install Carp 成功
install Clone 成功
install Cwd 成功
install Data::Dumper 成功
install Digest::MD5 成功
install File::Basename 成功 本来就有安装，不用管了，查看过了
install File::Spec::Functions 成功
install File::Temp 成功
install FindBin 成功
install Font::TTF::Font 成功
install GD 失败
install GD::Image 失败
install Getopt::Long 成功
install IO::File 成功
install List::MoreUtils 成功
install List::Util 成功
install Math::Round 成功
install Math::Trig 成功
install Math::VecStat 成功
install Memoize 成功
install POSIX 失败
install Params::Validate 成功
install Pod::Usage 成功
install Readonly 成功
install Regexp::Common 成功
install Statistics::Basic 成功
install Storable 成功
install Text::Balanced 失败
install Text::Format 成功
install Time::HiRes 成功


 遇到困难
＃安装install File::Basename 失败
#!/usr/bin/perl -w
use strict;

use File::Basename;

my $dir="/usr/bin/test.txt";

my $a=basename($dir);
print "$a\n";
＃安装install GD 失败
 Could not find gdlib-config in the search path. Please install libgd 2.0.28 or higher.
Warning: No success on command[/usr/bin/perl Build.PL]
  LDS/GD-2.56.tar.gz
  /usr/bin/perl Build.PL -- NOT OK
Running Build test
  Make had some problems, won't test
Running Build install
  Make had some problems, won't install
Could not read metadata file. Falling back to other methods to determine prerequisites
Failed during this command:
 LDS/GD-2.56.tar.gz                           : writemakefile NO '/usr/bin/perl Build.PL' returned status 512
install gdlib-config 不行
install libgd 2.0.28 不行，稍后解决
＃安装install GD::Image 失败
**UNRECOVERABLE ERROR**
Could not find gdlib-config in the search path. Please install libgd 2.0.28 or higher.
If you want to try to compile anyway, please rerun this script with the option --ignore_missing_gd.
Warning: No success on command[/usr/bin/perl Makefile.PL]
  LDS/GD-2.53.tar.gz
  /usr/bin/perl Makefile.PL -- NOT OK
Running make test
  Make had some problems, won't test
Running make install
  Make had some problems, won't install
Failed during this command:
 LDS/GD-2.53.tar.gz                           : writemakefile NO '/usr/bin/perl Makefile.PL' returned status 512
＃安装install POSIX 失败
Running install for module 'POSIX'
The most recent version "1.53_01" of the module "POSIX"
is part of the perl-5.22.1 distribution. To install that, you need to run
  force install POSIX   --or--
  install S/SH/SHAY/perl-5.22.1.tar.bz2
Running make test
  Can't test without successful make
Running make install
  Make had returned bad status, install seems impossible
Failed during this command:
 SHAY/perl-5.22.1.tar.bz2                     : make NO isa perl
强制安装
force install POSIX
还是会报这个错
[Type carriage return to continue] carriage

Much effort has been expended to ensure that this shell script will run on any
Unix system.  If despite that it blows up on yours, your best bet is to edit
Configure and run it again.  If you can't run Configure for some reason,
you'll have to generate a config.sh file by hand.  Whatever problems you
have, let me (perlbug@perl.org) know how I blew it.

This installation script affects things in two ways:

1) it may do direct variable substitutions on some of the files included
   in this kit.
2) it builds a config.h file for inclusion in C programs.  You may edit
   any of these files as the need arises after running this script.

If you make a mistake on a question, there is no easy way to back up to it
currently.  The easiest thing to do is to edit config.sh and rerun all the SH
files.  Configure will offer to let you do this before it runs the SH files.
[Type carriage return to continue] carriage
Operating system version?
OS X 10.10.1
Checking if your C library has broken 64-bit functions...
clang: error: no such file or directory: 'optimizer'
(I can't seem to compile the test program.)
Assuming that your C library's 64-bit functions are ok.

*** You have chosen to use 64-bit integers,
*** but none can be found.
*** Please rerun Configure without -Duse64bitint and/or -Dusemorebits.
*** Cannot continue, aborting.

Warning: No success on command[/Users/lishasha/.cpan/build/perl-5.22.1-4Me97Q/Configure]
  SHAY/perl-5.22.1.tar.bz2
  /Users/lishasha/.cpan/build/perl-5.22.1-4Me97Q/Configure -- NOT OK
Running make test
  Can't test without successful make
Running make install
  Make had returned bad status, install seems impossible
Failed during this command:
 SHAY/perl-5.22.1.tar.bz2                     : writemakefile NO '/Users/lishasha/.cpan/build/perl-5.22.1-4Me97Q/Configure' returned status 256
＃强制安装还是不行得想别的办法

＃安装install Text::Balanced失败 和前面一样的错误
cpan[35]> install Sys::Hostname
Running install for module 'Sys::Hostname'
The most recent version "1.20" of the module "Sys::Hostname"
is part of the perl-5.22.1 distribution. To install that, you need to run
  force install Sys::Hostname   --or--
  install S/SH/SHAY/perl-5.22.1.tar.bz2
Running make test
  Can't test without successful make
Running make install
  Make had returned bad status, install seems impossible
Failed during this command:
 SHAY/perl-5.22.1.tar.bz2                     : make NO isa perl




#.tgz解压
tgz跟tar.gz是一样的 
tar   zxvf   XX.tar.gz  即可


#对不同批次的microarray数据进行归一化

library(preprocessCore)

data<-read.table("combine_3_raw2.txt",header=T,row.names=1)
head(data)
dim(data)
data1<-normalize.quantiles(as.matrix(data))
log_data<-log2(data1)
par(mfrow=c(1,2))
boxplot(log2(data))
boxplot(log2(data.quantile))

boxplot(finaldata,col=c("mediumturquoise"),ylab="normalized Indensity Values",las=1,font.lab=2)
pdf("cutoff20_exon.pdf")
#对目标对象进行排序然后uniq，合并统计个数，从大到小排序
sort all_cadidate_in_all_patients.txt |uniq -c |sort -g -r -o all_cadidate_in_all_patients_uniq.fa

＃cluster 倒出一个thumbnail就可以拿到清晰的名字，貌似zoomed也可以，还可以save list将想要的群提出来包括其对应的表达值重新聚类

＃manhattan_onefactor_plot，软件中文名单因素曼哈顿图，曼哈顿图是种散点图，通常用于大量数据点和数值的描述。例如用于全基因组关联性分析（GWAS），观测显著差异SNP位点的分布。 ，通常用于描述大量数据点和数值的小工具。例如用于全基因组关联分析（GWAS）。百迈客公司提供的测序服务中曼哈顿图多用于观测显著差异SNP位点的分布
#prism里面有一个graph portfolio里面有许多好的图片
 library(igraph)
> library(RTN)
输入表达谱的gsea分析
GSEA富集还是正反向分开比较
java -Xmx512m xtools.gsea.Gsea -res /Users/lishasha/Desktop/home/project/2016/edgeR/PATHWAY/L23_Deseq_pos.gct -cls /Users/lishasha/Desktop/home/project/2016/edgeR/PATHWAY/L23_Deseq.cls#Treat_versus_Nontreat -gmx gseaftp.broadinstitute.org://pub/gsea/gene_sets/c2.all.v5.1.symbols.gmt -collapse false -mode Max_probe -norm meandiv -nperm 1000 -permute phenotype -rnd_type no_balance -scoring_scheme weighted -rpt_label my_analysis_L23_Deseq_pos -metric Signal2Noise -sort real -order descending -include_only_symbols true -make_sets true -median false -num 30 -plot_top_x 20 -rnd_seed timestamp -save_rnd_lists false -set_max 500 -set_min 15 -zip_report false -out /Users/lishasha/gsea_home/output/三月17 -gui false

Further information is available in the vignettes by typing vignette("RTN"). Documented topics are also available in HTML by typing help.start() and selecting the RTN package from the menu.
RTN code，真正用的方法不是这样，这个是内置脚本
### R code from vignette source 'RTN.Rnw'

###################################################
### code chunk number 1: Ropts
###################################################
options(width=70)


###################################################
### code chunk number 2: Load a sample dataset
###################################################
library(RTN)
data(dt4rtn)


###################################################
### code chunk number 3: Create a new TNI object
###################################################
#Input 1: 'gexp', a named gene expression matrix (samples on cols)
#Input 2: 'transcriptionFactors', a named vector with TF ids (3 TFs for quick demonstration!)
#Input 3: 'gexpIDs', an optional data frame with gene annotation (it can be used to remove duplicated genes)
rtni <- new("TNI", gexp=dt4rtn$gexp, 
            transcriptionFactors=dt4rtn$tfs[c("PTTG1","E2F2","FOXM1")]
            )
rtni<-tni.preprocess(rtni,gexpIDs=dt4rtn$gexpIDs)


###################################################
### code chunk number 4: Permutation
###################################################
rtni<-tni.permutation(rtni)


###################################################
### code chunk number 5: Bootstrap
###################################################
rtni<-tni.bootstrap(rtni) 


###################################################
### code chunk number 6: Run DPI filter
###################################################
rtni<-tni.dpi.filter(rtni) 


###################################################
### code chunk number 7: Check summary
###################################################
tni.get(rtni,what="summary")
refnet<-tni.get(rtni,what="refnet")
tnet<-tni.get(rtni,what="tnet")


###################################################
### code chunk number 8: Get graph
###################################################
g<-tni.graph(rtni)


###################################################
### code chunk number 9: Create a new TNA object (preprocess TNI-to-TNA)
###################################################
#Input 1: 'object', a TNI object with a pre-processed transcripional network
#Input 2: 'phenotype', a named numeric vector of phenotypes
#Input 3: 'hits', a character vector of gene ids considered as hits
#Input 4: 'phenoIDs', an optional data frame with anottation used to aggregate genes in the phenotype
rtna<-tni2tna.preprocess(object=rtni, 
                         phenotype=dt4rtn$pheno, 
                         hits=dt4rtn$hits, 
                         phenoIDs=dt4rtn$phenoIDs
                         )


###################################################
### code chunk number 10: Run MRA analysis pipeline
###################################################
rtna<-tna.mra(rtna)


###################################################
### code chunk number 11: Run overlap analysis pipeline
###################################################
rtna<-tna.overlap(rtna)


###################################################
### code chunk number 12: Run GSEA analysis pipeline
###################################################
rtna<-tna.gsea1(rtna)


###################################################
### code chunk number 13: Get results
###################################################
tna.get(rtna,what="summary")
tna.get(rtna,what="mra")
tna.get(rtna,what="overlap")
tna.get(rtna,what="gsea1")


###################################################
### code chunk number 14: Plot GSEA
###################################################
tna.plot.gsea1(rtna, file="tna_test", width=6, height=4, 
              heightPanels=c(1,0.7,3), 
              ylimPanels=c(0,3.5,0,0.8)) 


###################################################
### code chunk number 15: Session information
###################################################
print(sessionInfo(), locale=FALSE)

#??RedeR
RedeR is an R-based package combined with a stand-alone Java application for interactive visualization and manipulation of modular structures, 
nested networks and multiple levels of hierarchical associations. The software takes advantage of R to run robust statistics, while the R-to-Java interface bridges the gap between network analysis and visualization.


#http://www.r-bloggers.com/deseq-vs-edger-comparison/

baseMean mean normalised counts, averaged over all samples from both conditions
baseMeanA mean normalised counts from condition A
baseMeanB mean normalised counts from condition B 

#                    baseMean log2FoldChange      lfcSE       stat      pvalue        padj
##                   <numeric>      <numeric>  <numeric>  <numeric>   <numeric>   <numeric>
## ENSG00000000003 708.6021697    -0.37423028 0.09872592 -3.7905980 0.000150285 0.001288634
## ENSG00000000419 520.2979006     0.20214241 0.10929202  1.8495625 0.064376631 0.197285253
## ENSG00000000457 237.1630368     0.03624420 0.13682871  0.2648874 0.791096181 0.914554085
## ENSG00000000460  57.9326331    -0.08520813 0.24645454 -0.3457357 0.729541350 0.883905862
## ENSG00000000938   0.3180984    -0.11522629 0.14589383 -0.7897955 0.429647219          NA
## ...                     ...            ...        ...        ...         ...         ...
## ENSG00000273485   1.2864477     0.03490688  0.2986168  0.1168952   0.9069431          NA
## ENSG00000273486  15.4525365    -0.09662406  0.3385222 -0.2854290   0.7753155   0.9062370
## ENSG00000273487   8.1632350     0.56255493  0.3731295  1.5076666   0.1316399   0.3297036
## ENSG00000273488   8.5844790     0.10794134  0.3680474  0.2932811   0.7693073   0.9034871
## ENSG00000273489   0.2758994     0.11249632  0.1420250  0.7920882   0.4283092          NA

The first column, baseMean, is a just the average of the normalized count values, dividing by size factors, taken over all samples in the DESeqDataSet. The remaining four columns refer to a specific contrast, namely the comparison of the trt level over the untrt level for the factor variable dex. We will find out below how to obtain other contrasts.

The column log2FoldChange is the effect size estimate. It tells us how much the gene’s expression seems to have changed due to treatment with dexamethasone in comparison to untreated samples. This value is reported on a logarithmic scale to base 2: for example, a log2 fold change of 1.5 means that the gene’s expression is increased by a multiplicative factor of 21.560≈602.82.

Of course, this estimate has an uncertainty associated with it, which is available in the column lfcSE, the standard error estimate for the log2 fold change estimate. We can also express the uncertainty of a particular effect size estimate as the result of a statistical test. The purpose of a test for differential expression is to test whether the data provides sufficient evidence to conclude that this value is really different from zero. DESeq2 performs for each gene a hypothesis test to see whether evidence is sufficient to decide against the null hypothesis that there is zero effect of the treatment on the gene and that the observed difference between treatment and control was merely caused by experimental variability (i.e., the type of variability that you can expect between different samples in the same treatment group). As usual in statistics, the result of this test is reported as a p value, and it is found in the column pvalue. Remember that a p value indicates the probability that a fold change as strong as the observed one, or even stronger, would be seen under the situation described by the null hypothesis.

We can also summarize the results with the following line of code, which reports some additional information, that will be covered in later sections.


## Agresti (1990, p. 61f; 2002, p. 91) Fisher's Tea Drinker
## A British woman claimed to be able to distinguish whether milk or
##  tea was added to the cup first.  To test, she was given 8 cups of
##  tea, in four of which milk was added first.  The null hypothesis
##  is that there is no association between the true order of pouring
##  and the woman's guess, the alternative that there is a positive
##  association (that the odds ratio is greater than 1).
TeaTasting <-
matrix(c(3, 1, 1, 3),
       nrow = 2,
       dimnames = list(Guess = c("Milk", "Tea"),
                       Truth = c("Milk", "Tea")))
fisher.test(TeaTasting, alternative = "greater")
## => p = 0.2429, association could not be established

## Fisher (1962, 1970), Criminal convictions of like-sex twins
Convictions <-
matrix(c(2, 10, 15, 3),
       nrow = 2,
       dimnames =
       list(c("Dizygotic", "Monozygotic"),
            c("Convicted", "Not convicted")))
Convictions
fisher.test(Convictions, alternative = "less")
fisher.test(Convictions, conf.int = FALSE)
fisher.test(Convictions, conf.level = 0.95)$conf.int
fisher.test(Convictions, conf.level = 0.99)$conf.int

## A r x c table  Agresti (2002, p. 57) Job Satisfaction
Job <- matrix(c(1,2,1,0, 3,3,6,1, 10,10,14,9, 6,7,12,11), 4, 4,
dimnames = list(income = c("< 15k", "15-25k", "25-40k", "> 40k"),
                satisfaction = c("VeryD", "LittleD", "ModerateS", "VeryS")))
fisher.test(Job)
fisher.test(Job, simulate.p.value = TRUE, B = 1e5)


#edgeR

exactTest {edgeR}	R Documentation
Exact Tests for Differences between Two Groups of Negative-Binomial Counts

Description

Compute genewise exact tests for differences in the means between two groups of negative-binomially distributed counts.


#edgeR自己计算cpm,rpkm

cpm {edgeR}	R Documentation
Counts per Million or Reads per Kilobase per Million

Description

Computes counts per million (CPM) or reads per kilobase per million (RPKM) values.

Usage

## S3 method for class 'DGEList'
cpm(x, normalized.lib.sizes=TRUE, log=FALSE, prior.count=0.25, ...)
## Default S3 method:
cpm(x, lib.size=NULL, log=FALSE, prior.count=0.25, ...)
## S3 method for class 'DGEList'
rpkm(x, gene.length=NULL, normalized.lib.sizes=TRUE, log=FALSE, prior.count=0.25, ...)
## Default S3 method:
rpkm(x, gene.length, lib.size=NULL, log=FALSE, prior.count=0.25, ...)
Arguments

x	
matrix of counts or a DGEList object

normalized.lib.sizes	
logical, use normalized library sizes?

lib.size	
library size, defaults to colSums(x).

log	
logical, if TRUE then log2 values are returned.

prior.count	
average count to be added to each observation to avoid taking log of zero. Used only if log=TRUE.

gene.length	
vector of length nrow(x) giving gene length in bases, or the name of the column x$genes containing the gene lengths.

...	
other arguments are not currently used

Details

CPM or RPKM values are useful descriptive measures for the expression level of a gene or transcript. By default, the normalized library sizes are used in the computation for DGEList objects but simple column sums for matrices.

If log-values are computed, then a small count, given by prior.count but scaled to be proportional to the library size, is added to x to avoid taking the log of zero.

The rpkm method for DGEList objects will try to find the gene lengths in a column of x$genes called Length or length. Failing that, it will look for any column name containing "length" in any capitalization.

Value

numeric matrix of CPM or RPKM values.

Note

aveLogCPM(x), rowMeans(cpm(x,log=TRUE)) and log2(rowMeans(cpm(x)) all give slightly different results.

Author(s)

Davis McCarthy, Gordon Smyth

See Also

aveLogCPM

Examples

y <- matrix(rnbinom(20,size=1,mu=10),5,4)
cpm(y)

d <- DGEList(counts=y, lib.size=1001:1004)
cpm(d)
cpm(d,log=TRUE)

d$genes$Length <- c(1000,2000,500,1500,3000)
rpkm(d)


#急着查看的网站
http://www.bea.ki.se/staff/reimers/Web.Pages/Normalization.Intro.htm
http://www.bea.ki.se/staff/reimers/Web.Pages/Affymetrix.Normalization.htm
https://en.wikipedia.org/wiki/Quantile_normalization
Ideally we would like a method that is based on some understanding of the hybridization process, and uses simple statistical procedures, to bring all chips to a common reference. Scaling is simple, but seems to be inaccurate. Methods based on multiple house-keeping genes, such as the MAS method for the 133 chip, and the Li and Wong  method, appear promising, however they would work better if the reference set of genes were similar across all chips. These methods use a single chip reference, so peculiarities in that chip are forced onto all the others.
 Quantile normalization uses a single standard for all chips, however it assumes that no serious change in distribution occurs. This appears to be a rather strong assumption about gene distributions; however, in practice genes move up and down roughly equally; it would need several hundred genes to be changed greatly and in one direction, 
 to drive quantile normalization in error by more than 20%. 
 This may well be true in studies of senescence, or interference with basal transcriptional apparatus,
  or selective comparisons of RNA's attached to ribosomes, and perhaps in extremely malignant tumors



Quantile normalization and subsequent data processing were performed
with using the GeneSpring GX v11.5.1 software (Agilent Technologies).

any scaling transform applied on a log scale, will shift the distribution curve to the right or left, but not change its shape

/Share/home/wangdong/local/app/bowtie-1.1.1/bowtie-build -f /Share/home/wangdong/lss/project/2016_project/all_probes_in_our_lab_capital_ours_uniq.fa /Share/home/wangdong/lss/project/2016_project/all_probes_in_our_lab_capital_ours_uniq


＃deseq edger的不同
Here, if genes were found differentially expressed by edgeR only, they’re colored red; if found by both, 
colored green. What’s striking here is that for a handful of genes, DESeq is (1) reporting massive fold changes, and (2) not calling them statistically significant. What’s going on here?

It turns out that these genes have extremely low counts (usually one or two counts in only one or two samples). 
The DESeq vignette goes through the logic of independent filtering, showing that the likelihood of a gene being significantly differentially expressed is related to how strongly it’s expressed, and advocates for discarding extremely lowly expressed genes, because differential expression is likely not statistically detectable.


#excel 计算相关性
=CORREL(G:G;I:I)

#register(MulticoreParam(4)) 申请4线程


#R里的grep相关
grep {base}	R Documentation
Pattern Matching and Replacement

Description

grep, grepl, regexpr and gregexpr search for matches to argument pattern within each element of a character vector: they differ in the format of and amount of detail in the results.

sub and gsub perform replacement of the first and all matches respectively.

Usage

grep(pattern, x, ignore.case = FALSE, perl = FALSE, value = FALSE,
     fixed = FALSE, useBytes = FALSE, invert = FALSE)

grepl(pattern, x, ignore.case = FALSE, perl = FALSE,
      fixed = FALSE, useBytes = FALSE)

sub(pattern, replacement, x, ignore.case = FALSE, perl = FALSE,
    fixed = FALSE, useBytes = FALSE)

gsub(pattern, replacement, x, ignore.case = FALSE, perl = FALSE,
     fixed = FALSE, useBytes = FALSE)

regexpr(pattern, text, ignore.case = FALSE, perl = FALSE,
        fixed = FALSE, useBytes = FALSE)

gregexpr(pattern, text, ignore.case = FALSE, perl = FALSE,
         fixed = FALSE, useBytes = FALSE)

regexec(pattern, text, ignore.case = FALSE,
        fixed = FALSE, useBytes = FALSE)
Arguments

pattern	
character string containing a regular expression (or character string for fixed = TRUE) to be matched in the given character vector. Coerced by as.character to a character string if possible. If a character vector of length 2 or more is supplied, the first element is used with a warning. Missing values are allowed except for regexpr and gregexpr.

x, text	
a character vector where matches are sought, or an object which can be coerced by as.character to a character vector. Long vectors are supported.

ignore.case	
if FALSE, the pattern matching is case sensitive and if TRUE, case is ignored during matching.

perl	
logical. Should perl-compatible regexps be used?

value	
if FALSE, a vector containing the (integer) indices of the matches determined by grep is returned, and if TRUE, a vector containing the matching elements themselves is returned.

fixed	
logical. If TRUE, pattern is a string to be matched as is. Overrides all conflicting arguments.

useBytes	
logical. If TRUE the matching is done byte-by-byte rather than character-by-character. See ‘Details’.

invert	
logical. If TRUE return indices or values for elements that do not match.

replacement	
a replacement for matched pattern in sub and gsub. Coerced to character if possible. For fixed = FALSE this can include backreferences "\1" to "\9" to parenthesized subexpressions of pattern. For perl = TRUE only, it can also contain "\U" or "\L" to convert the rest of the replacement to upper or lower case and "\E" to end case conversion. If a character vector of length 2 or more is supplied, the first element is used with a warning. If NA, all elements in the result corresponding to matches will be set to NA.

Details

Arguments which should be character strings or character vectors are coerced to character if possible.

Each of these functions (apart from regexec, which currently does not support Perl-style regular expressions) operates in one of three modes:

fixed = TRUE: use exact matching.

perl = TRUE: use Perl-style regular expressions.

fixed = FALSE, perl = FALSE: use POSIX 1003.2 extended regular expressions.

See the help pages on regular expression for details of the different types of regular expressions.

The two *sub functions differ only in that sub replaces only the first occurrence of a pattern whereas gsub replaces all occurrences. If replacement contains backreferences which are not defined in pattern the result is undefined (but most often the backreference is taken to be "").

For regexpr, gregexpr and regexec it is an error for pattern to be NA, otherwise NA is permitted and gives an NA match.

The main effect of useBytes is to avoid errors/warnings about invalid inputs and spurious matches in multibyte locales, but for regexpr it changes the interpretation of the output. It inhibits the conversion of inputs with marked encodings, and is forced if any input is found which is marked as "bytes" see Encoding).

Caseless matching does not make much sense for bytes in a multibyte locale, and you should expect it only to work for ASCII characters if useBytes = TRUE.

regexpr and gregexpr with perl = TRUE allow Python-style named captures, but not for long vector inputs.

Invalid inputs in the current locale are warned about up to 5 times.

Value

grep(value = FALSE) returns a vector of the indices of the elements of x that yielded a match (or not, for invert = TRUE. This will be an integer vector unless the input is a long vector, when it will be a double vector.

grep(value = TRUE) returns a character vector containing the selected elements of x (after coercion, preserving names but no other attributes).

grepl returns a logical vector (match or not for each element of x).

For sub and gsub return a character vector of the same length and with the same attributes as x (after possible coercion to character). Elements of character vectors x which are not substituted will be returned unchanged (including any declared encoding). If useBytes = FALSE a non-ASCII substituted result will often be in UTF-8 with a marked encoding (e.g., if there is a UTF-8 input, and in a multibyte locale unless fixed = TRUE). Such strings can be re-encoded by enc2native.

regexpr returns an integer vector of the same length as text giving the starting position of the first match or -1 if there is none, with attribute "match.length", an integer vector giving the length of the matched text (or -1 for no match). The match positions and lengths are in characters unless useBytes = TRUE is used, when they are in bytes. If named capture is used there are further attributes "capture.start", "capture.length" and "capture.names".

gregexpr returns a list of the same length as text each element of which is of the same form as the return value for regexpr, except that the starting positions of every (disjoint) match are given.

regexec returns a list of the same length as text each element of which is either -1 if there is no match, or a sequence of integers with the starting positions of the match and all substrings corresponding to parenthesized subexpressions of pattern, with attribute "match.length" a vector giving the lengths of the matches (or -1 for no match).

Warning

POSIX 1003.2 mode of gsub and gregexpr does not work correctly with repeated word-boundaries (e.g., pattern = "\b"). Use perl = TRUE for such matches (but that may not work as expected with non-ASCII inputs, as the meaning of ‘word’ is system-dependent).

Performance considerations

If you are doing a lot of regular expression matching, including on very long strings, you will want to consider the options used. Generally PCRE will be faster than the default regular expression engine, and fixed = TRUE faster still (especially when each pattern is matched only a few times).

If you are working in a single-byte locale and have marked UTF-8 strings that are representable in that locale, convert them first as just one UTF-8 string will force all the matching to be done in Unicode, which attracts a penalty of around 3x for the default POSIX 1003.2 mode.

If you can make use of useBytes = TRUE, the strings will not be checked before matching, and the actual matching will be faster. Often byte-based matching suffices in a UTF-8 locale since byte patterns of one character never match part of another.

Source

The C code for POSIX-style regular expression matching has changed over the years. As from R 2.10.0 the TRE library of Ville Laurikari (http://laurikari.net/tre/) is used. The POSIX standard does give some room for interpretation, especially in the handling of invalid regular expressions and the collation of character ranges, so the results will have changed slightly over the years.

For Perl-style matching PCRE (http://www.pcre.org) is used.

References

Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988) The New S Language. Wadsworth & Brooks/Cole (grep)

See Also

regular expression (aka regexp) for the details of the pattern specification.

regmatches for extracting matched substrings based on the results of regexpr, gregexpr and regexec.

glob2rx to turn wildcard matches into regular expressions.

agrep for approximate matching.

charmatch, pmatch for partial matching, match for matching to whole strings.

tolower, toupper and chartr for character translations.

apropos uses regexps and has more examples.

grepRaw for matching raw vectors.

Examples

grep("[a-z]", letters)

txt <- c("arm","foot","lefroo", "bafoobar")
if(length(i <- grep("foo", txt)))
   cat("'foo' appears at least once in\n\t", txt, "\n")
i # 2 and 4
txt[i]

## Double all 'a' or 'b's;  "\" must be escaped, i.e., 'doubled'
gsub("([ab])", "\\1_\\1_", "abc and ABC")

txt <- c("The", "licenses", "for", "most", "software", "are",
  "designed", "to", "take", "away", "your", "freedom",
  "to", "share", "and", "change", "it.",
   "", "By", "contrast,", "the", "GNU", "General", "Public", "License",
   "is", "intended", "to", "guarantee", "your", "freedom", "to",
   "share", "and", "change", "free", "software", "--",
   "to", "make", "sure", "the", "software", "is",
   "free", "for", "all", "its", "users")
( i <- grep("[gu]", txt) ) # indices
stopifnot( txt[i] == grep("[gu]", txt, value = TRUE) )

## Note that in locales such as en_US this includes B as the
## collation order is aAbBcCdEe ...
(ot <- sub("[b-e]",".", txt))
txt[ot != gsub("[b-e]",".", txt)]#- gsub does "global" substitution

txt[gsub("g","#", txt) !=
    gsub("g","#", txt, ignore.case = TRUE)] # the "G" words

regexpr("en", txt)

gregexpr("e", txt)

## Using grepl() for filtering
## Find functions with argument names matching "warn":
findArgs <- function(env, pattern) {
  nms <- ls(envir = as.environment(env))
  nms <- nms[is.na(match(nms, c("F","T")))] # <-- work around "checking hack"
  aa <- sapply(nms, function(.) { o <- get(.)
               if(is.function(o)) names(formals(o)) })
  iw <- sapply(aa, function(a) any(grepl(pattern, a, ignore.case=TRUE)))
  aa[iw]
}
findArgs("package:base", "warn")

## trim trailing white space
str <- "Now is the time      "
sub(" +$", "", str)  ## spaces only
sub("[[:space:]]+$", "", str) ## white space, POSIX-style
sub("\\s+$", "", str, perl = TRUE) ## Perl-style white space

## capitalizing
txt <- "a test of capitalizing"
gsub("(\\w)(\\w*)", "\\U\\1\\L\\2", txt, perl=TRUE)
gsub("\\b(\\w)",    "\\U\\1",       txt, perl=TRUE)

txt2 <- "useRs may fly into JFK or laGuardia"
gsub("(\\w)(\\w*)(\\w)", "\\U\\1\\E\\2\\U\\3", txt2, perl=TRUE)
 sub("(\\w)(\\w*)(\\w)", "\\U\\1\\E\\2\\U\\3", txt2, perl=TRUE)

## named capture
notables <- c("  Ben Franklin and Jefferson Davis",
              "\tMillard Fillmore")
# name groups 'first' and 'last'
name.rex <- "(?<first>[[:upper:]][[:lower:]]+) (?<last>[[:upper:]][[:lower:]]+)"
(parsed <- regexpr(name.rex, notables, perl = TRUE))
gregexpr(name.rex, notables, perl = TRUE)[[2]]
parse.one <- function(res, result) {
  m <- do.call(rbind, lapply(seq_along(res), function(i) {
    if(result[i] == -1) return("")
    st <- attr(result, "capture.start")[i, ]
    substring(res[i], st, st + attr(result, "capture.length")[i, ] - 1)
  }))
  colnames(m) <- attr(result, "capture.names")
  m
}
parse.one(notables, parsed)

## Decompose a URL into its components.
## Example by LT (http://www.cs.uiowa.edu/~luke/R/regexp.html).
x <- "http://stat.umn.edu:80/xyz"
m <- regexec("^(([^:]+)://)?([^:/]+)(:([0-9]+))?(/.*)", x)
m
regmatches(x, m)
## Element 3 is the protocol, 4 is the host, 6 is the port, and 7
## is the path.  We can use this to make a function for extracting the
## parts of a URL:
URL_parts <- function(x) {
    m <- regexec("^(([^:]+)://)?([^:/]+)(:([0-9]+))?(/.*)", x)
    parts <- do.call(rbind,
                     lapply(regmatches(x, m), `[`, c(3L, 4L, 6L, 7L)))
    colnames(parts) <- c("protocol","host","port","path")
    parts
}
URL_parts(x)
[Package base version 3.1.2 Index]

> library(affy)
data.rma.lo<-normalize(data.rma,method="loess") 是affy里面自己的包
library(limma)
normalizeWithinArrays(object, layout, method="printtiploess", weights=object$weights, span=0.3, iterations=4, controlspots=NULL, df=5, robust="M", bc.method="subtract", offset=0)
method："none"，"median"，"loess"，"printtiploess"，"composite"，"control"和"robustspline"。初始值设定为“printtiploess”，但是对于Agilent芯片或者小样本芯片（每个“点样块”少于150个探针）。可用选用的loess或者robustspline。“loess归一化方法假设了有相当大的一部分探针没有发生差异化表达，而不是上调或者下调的基因数差不多或者差异化程度围绕着0波动。”（参考文献1）。需要注意，组内归一化只是对单张芯片的M值进行了规整（不影响A值），但是对与组间各个通道没有进行比较。
weight：是图像处理软件对探针权重的标记，如果使用weight，那么在归一化过程中weight为0的探针不会影响其他探针，这并不意味着这些探针会被剔除，它们同样会被归一化，也会出现在归一化的结果中。如果不想使用，那么weight设为NULL。
iterations：设定loess的循环数，循环数越多，结果越强健（robust）。

#AFFY里面几个包mas5,expresso,rma等，以及相应方法
http://www.jianshu.com/p/4fe4269eb86f

“loess归一化方法假设了有相当大的一部分探针没有发生差异化表达，而不是上调或者下调的基因数差不多或者差异化程度围绕着0波动。”
 这个网址感觉还靠谱http://www.zilhua.com/48.html
 3.2 组内和组间归一化

MA <- normalizeWithinArrays(RG,weights=NULL)
MA.pAq <- normalizeBetweenArrays(MA,method="Aquantile")

normalizeWithinArrays(object, layout, method="printtiploess", weights=object$weights, span=0.3, iterations=4, controlspots=NULL, df=5, robust="M", bc.method="subtract", offset=0)
method："none"，"median"，"loess"，"printtiploess"，"composite"，"control"和"robustspline"。初始值设定为“printtiploess”，但是对于Agilent芯片或者小样本芯片（每个“点样块”少于150个探针）。可用选用的loess或者robustspline。“loess归一化方法假设了有相当大的一部分探针没有发生差异化表达，而不是上调或者下调的基因数差不多或者差异化程度围绕着0波动。”（参考文献1）。需要注意，组内归一化只是对单张芯片的M值进行了规整（不影响A值），但是对与组间各个通道没有进行比较。
weight：是图像处理软件对探针权重的标记，如果使用weight，那么在归一化过程中weight为0的探针不会影响其他探针，这并不意味着这些探针会被剔除，它们同样会被归一化，也会出现在归一化的结果中。如果不想使用，那么weight设为NULL。
iterations：设定loess的循环数，循环数越多，结果越强健（robust）。
3.3 组间归一化
normalizeBetweenArrays(object, method=NULL, targets=NULL, cyclic.method="fast", ...)
method：“none"，"scale"，"quantile"，"Aquantile"，"Gquantile"，“Rquantile”，“Tquantile”，“cyclicloess”。“scale”对log-ratio数值进行归一化；“quantile”对密度（intensity）进行归一化；“Aquantile”对A数值进行归一化，不调正M数值；"Gquantile"和“Rquantile”则分别对Green和Red通道进行归一化，适用于Green或者Red为“参考标准（common reference）”的样品；“Tquantile”表示样品分开归一化或者Green和Red一先一后进行归一化。
而且limma包可以直接画火山图
最后，还可以使用 volcanoplot(fit, coef=1, highlight=0, names=fit$genes$ID, xlab=”Log Fold Change”, ylab=”Log Odds”, pch=16, cex=0.35, …)画“火山图”（highlight标记前N个差异基因）；
使用vennDiagram(object, include=”both”, names, mar=rep(1,4), cex=1.5, lwd=1, circle.col, counts.col, show.include, …)画“韦恩图”；


limma包的使用
By zilhua | 2013 年 9 月 5 日
暂无评论
双色芯片数据处理

【转】limmar package是一个功能比较全的包，既含有cDNA芯片的RAW data输入、前处理（归一化）功能，同时也有差异化基因分析的“线性”算法（limma: Linear Models for Microarray Data），特别是对于“多因素实验（multifactor designed experiment）”。limmar包的可扩展性非常强，单通道（one channel）或者双通道（tow channel）数据都可以分析差异基因，甚至也包括了定量PCR和RNA-seq（第一次见分析microarray的包也能处理RNA-seq）。
limmar package是一个集大成的包，对载入数据、数据前处理（背景矫正、组内归一化和组间归一化都有很多种选择方法）⒉钜旎因分析都有很多的选择。而且，所设计的线性回归和经验贝叶斯方法找差异基因非常值得学习。

1. 读入样本信息

> library(limma)
> targets <- readTargets(“targets.txt”)
> f <- function(x) as.numeric(x$Flags > -99)
2.读入探针密度数据

> RG <- read.maimages(targets, source=”genepix”, wt.fun=f)

与marray包一致，Bioconductor不能读入原始的TIFF图像文件，只能输出扫描仪输入的、转换成数字信号的文本文件。使用函数read.maimages(files=NULL, source=”generic”, path=NULL, ext=NULL, names=NULL, columns=NULL, other.columns=NULL, annotation=NULL, green.only=FALSE, wt.fun=NULL, verbose=TRUE, sep=”\t”, quote=NULL, …)
参数说明：files需要通过函数dir(pattern = “Mypattern”)配合正则表达式筛选（规范命名很重要），同时该函数可以读入符合格式的压缩过的文件，比如*.txt.gz的文件，这极大的减小的数据储存大小；source的取值分为两类，一类是“高富帅”，比如“agilent”、“spot”等等（下表），它们是特定扫描仪器的特定输出格式；如果不幸是“潘俊保即格式是自己规定的，可以选定source=”generic”，这时需要规定columns；任何cDNA文件都要有R/G/Rb/Gb四列（Mean或者Median）；annotation可以规定哪些是注释列；wt.fun用于对点样点进行质量评估，取值为0表示这些点将在后续的分析中被剔除，取值位1表示需要保留，对点样点的评估依赖于图像扫描软件的程序设定，比如SPOT和GenePix软件，查看QualityWeights（现成函数或者自己写函数）。
读入单通道数据：读入单通道数据，可以设定green.only = TRUE即可，然后对应读入columns = list(G = “Col1″, Gb = “Col2″)。
读入的数据，如果是单通道，则成为EListRaw class；如果是双通道，则是RGList class。
数据操作：
cbind()：合并数据；
“[”：分割数据；
RGList class有的names是 "R"，"G"，"Rb"，"Gb"，"weights"，"printer"，"genes"，"targets"，"notes"： R/G/Rb/Gb分别红和绿的前景和背景噪音；weight是扫描软件的质量评估；printer是点样规则（printer layout）；genes是基因注释；target是样本注释；notes是一般注释。可以通过myRGList$names进行相应的取值和赋值。
3.前处理
3.1 背景去除：

RG <- backgroundCorrect(RG,method="subtract",offset=0,printer=RG$printer,verbose=TRUE)

backgroundCorrect(RG, method="auto", offset=0, printer=RG$printer, normexp.method="saddle", verbose=TRUE)
RG：可以是EListRaw，也可以是RGList class；
method：取“auto”，“none”，“subtract”，"half"，"minimum"，"movingmin"，"edwards"或者"normexp"；
normexp.method：在method取“normexp”时，可以取"saddle"，"mle"，"rma"或者"rma75"。
作者建议使用“mle”或者“saddle”，两个运行速度也差不多。如果数据中含有“形态背景估计（morphological background estimates）”，比如SPOT和GenePix图像处理软件，那么method = "subtract"也就较好的表现。
3.2 组内和组间归一化

MA <- normalizeWithinArrays(RG,weights=NULL)
MA.pAq <- normalizeBetweenArrays(MA,method="Aquantile")

normalizeWithinArrays(object, layout, method="printtiploess", weights=object$weights, span=0.3, iterations=4, controlspots=NULL, df=5, robust="M", bc.method="subtract", offset=0)
method："none"，"median"，"loess"，"printtiploess"，"composite"，"control"和"robustspline"。初始值设定为“printtiploess”，但是对于Agilent芯片或者小样本芯片（每个“点样块”少于150个探针）。可用选用的loess或者robustspline。“loess归一化方法假设了有相当大的一部分探针没有发生差异化表达，而不是上调或者下调的基因数差不多或者差异化程度围绕着0波动。”（参考文献1）。需要注意，组内归一化只是对单张芯片的M值进行了规整（不影响A值），但是对与组间各个通道没有进行比较。
weight：是图像处理软件对探针权重的标记，如果使用weight，那么在归一化过程中weight为0的探针不会影响其他探针，这并不意味着这些探针会被剔除，它们同样会被归一化，也会出现在归一化的结果中。如果不想使用，那么weight设为NULL。
iterations：设定loess的循环数，循环数越多，结果越强健（robust）。
3.3 组间归一化
normalizeBetweenArrays(object, method=NULL, targets=NULL, cyclic.method="fast", ...)
method：“none"，"scale"，"quantile"，"Aquantile"，"Gquantile"，“Rquantile”，“Tquantile”，“cyclicloess”。“scale”对log-ratio数值进行归一化；“quantile”对密度（intensity）进行归一化；“Aquantile”对A数值进行归一化，不调正M数值；"Gquantile"和“Rquantile”则分别对Green和Red通道进行归一化，适用于Green或者Red为“参考标准（common reference）”的样品；“Tquantile”表示样品分开归一化或者Green和Red一先一后进行归一化。

使用plotDensities()绘图

plotDensities(MA)
plotDensities(MA.pAq)
4.不同实验的数据处理

不同的实验，处理方式是不一样的。

> fit <- lmFit(MA)
> fit <- eBayes(fit)
> topTable(fit)

t <- fit$t
write.csv(t,"_2_all_t_value.txt",row.names=FALSE)

其他参考博客：http://blog.sina.com.cn/s/blog_61f013b8010138ub.html
5. 做图
5.1 背景和前景
imageplot(z, layout, low = NULL, high = NULL, ncolors = 123, zerocenter = NULL, zlim = NULL, mar=c(2,1,1,1), legend=TRUE, ...)
z：可以定义R/Rb/G/Gb，也可以是导数；
low/high：规定颜色，比如low = "white" , high = "red"
下图是一张芯片的绿色前景图、红色背景图和log-ratio图（M值图）：
=======================================================
> imageplot(log2(beta7$G[, 1]), beta7$printer, low = “white”, high = “green”)
> imageplot(log2(beta7$Rb[, 1]), beta7$printer, low = “white”, high = “red”)
> imageplot(beta7q$M[, 1], beta7$printer)
=======================================================
limma包的使用技巧limma包的使用技巧limma包的使用技巧
也可以使用imageplot3by2(RG, z=”Gb”, prefix=paste(“image”,z,sep=”-”), path=NULL, zlim=NULL, common.lim=TRUE, …)，将图以3*2的格式六个一组保存成图片。
5.2 M-A图
使用plotMA(MA, array=1, xlab=”A”, ylab=”M”, main=colnames(MA)[array], xlim=NULL, ylim=NULL, status, values, pch, col, cex, legend=TRUE, zero.weights=FALSE, …)画单个MA图。但这个函数有些问题，有时画不出。所以，完全可以自己来画，比如：
==========================================================
# plot the MAplot befor normalization
> M <- log2((beta7$R[, 1]-beta7$Rb[, 1])/(beta7$G[, 1]-beta7$Gb[, 1]))
> A <- (log2((beta7$R[, 1]-beta7$Rb[, 1]))+log2((beta7$G[, 1]-beta7$Gb[, 1])))/2
> plot(A, M, cex = 0.5)
# we can also plot the MAplot above using marray package
> beta7ma <- as(beta7, “marrayRaw”)
> plot(maA(beta7ma)[,1], maM(beta7ma)[, 1])
# plot the MAplot after normalization
> plot(beta7q$A[, 1], beta7q$M[, 1], cex = 0.5)
===========================================================
limma包的使用技巧limma包的使用技巧
使用plotPrintTipLoess(object,layout,array=1,span=0.4,…)画print-tip的MA图，下图分别是归一化前和后的print-tip的MA图。
limma包的使用技巧limma包的使用技巧
使用boxplot()画盒箱图，使用plotDensity()画密度曲线图。下图为原始数据、组内归一化（method = “normexp”, normexp.method=”mle”）和组建归一化（method = “scale”）的盒箱图。

limma包的使用技巧
limma包的使用技巧
limma包的使用技巧
最后，还可以使用 volcanoplot(fit, coef=1, highlight=0, names=fit$genes$ID, xlab=”Log Fold Change”, ylab=”Log Odds”, pch=16, cex=0.35, …)画“火山图”（highlight标记前N个差异基因）；
使用vennDiagram(object, include=”both”, names, mar=rep(1,4), cex=1.5, lwd=1, circle.col, counts.col, show.include, …)画“韦恩图”；
使用plotMDS()绘制多纬标度图（multidimensional scaling plot, MDS）。


library(limma)
*在R中获得某个包的帮助命令格式为：
> ?read.maimages  或者  > help("read.maimages")
在mac命令行中输入？edger等就可以出相应包的帮助信息了。


a<-matrix(data,ncol=2,nrow=5,dimnames=list(c("r1","r2","r3","r4","r5"),c("c1","c2")))
> a
   c1 c2
r1  1  6
r2  2  7
r3  3  8
r4  4  9
r5  5 10

> a[[8]]
[1] 8
我不理解 这个 a[[8]] 表示什么意思啊
　a本身是一个矩阵，而定义dimnames=list()则表示其每一个元素都被命名且命名方式是列表（list），因此在调用a中的元素的时候可以调用a[]或者a[[]]都可以。a[]是调用a本身的第几个元素，a[[]]是命名中的第几个名字下的元素。

pcm1<-matrix(c(0,50,0,50,
				   100,0,0,0,
				   0,100,0,0,
				   0,0,100,0,
				   0,0,0,100,
				   50,50,0,0,
				   0,0,50,50), nrow=4)
	pcm2<-matrix(c(50,50,0,0,
				   0,100,0,0,
				   0,50,50,0,
				   0,0,0,100,
				   50,50,0,0,
				   0,0,50,50), nrow=4)
	rownames(pcm1)<-c("A","C","G","T")
	rownames(pcm2)<-c("A","C","G","T")
	pfms<-list(p1=new("pfm",mat=pcm2pfm(pcm1),name="m1"),
			   p2=new("pfm",mat=pcm2pfm(pcm2),name="m2"))
	pfms<-DNAmotifAlignment(pfms)
	plotMotifLogoStack(pfms)
	

http://www.molmine.com/magma/analysis/sam.htm
http://statweb.stanford.edu/~tibs/SAM/
http://wenku.baidu.com/link?url=ETMu_pqSsQ_MqCHAsQBvITqLig7Y_2m57oKpQJDeT5AOOeQZC4waG8LEgqiqQP_eZ-Grp76MFSLmislxPWo2K4bq8_3Z4PDOozbTuHwLxNq
http://blog.sina.com.cn/s/blog_53a8bd0001008m96.html
http://www.zilhua.com/48.html

## try http:// if https:// URLs are not supported
source("https://bioconductor.org/biocLite.R")
biocLite("limma")

Documentation

To view documentation for the version of this package installed in your system, start R and enter:

browseVignettes("limma")
#
SAM方法以t检验为基础，并根据芯片数据噪音大小与表达峰
度相关的特点进行修正。其特点是在筛选得到较多特征基因的同时，错误发
现率(FDR)还保持在较低的水平。
用SAM方法将60个肿瘤细胞系的基因表达数据与其药物活
性数据相关联，得到了SAM相关系数表示的基因与药物之间的关系。
单地说，NCI的体外化疗药物筛选系统由60种不同的人体肿瘤细胞系组
成，测定每个受试化合物在一定浓度范围抑止各肿瘤细胞的生长的能力或细胞
毒性程度。
50，取．109。。G150表示该细胞对某种药物的敏感性或抗
耐性，同时也表示该药物的活性。经过数据清洗，选择其中110药物的数据，
于是得到彳’110
GI50也可以合并基因表达做相关性分析，sam 算相对差异系数   dij。 
(4) Pearson correlation coefficient (得到 R 值) 目的在比较技术性重复下的相似性，R 值越高表示两芯片结果越近似。当 R 值超过 0.975，我们才将此次的实验结果视为可信，才继续后面的分析流程,也就是r2是0.95


#GSEA经过摸索
normal:meandiv:其实是对nes的计算，即对ES是否进行es/mean(ess agaist all permutations of the datasets),不让es,nes就都等于es了
选择classific, weight,p=1,p=1.5,p=2的关系，其实就是
When
p  0, ES(S) reduces to the standard KolmogorovCSmirnov statistic;
when p  1, we are weighting the genes in S by their correlation
with C normalized by the sum of the correlations over all of the
genes in S

对于我们的ranked list,ase of GSEAPreranked, you should make sure that this weighted scoring scheme applies to your choice of ranking statistic. When in doubt, we recommend using a more conservative scoring approach by setting Enrichment statistic to classic
但是好像我们也是希望
we are weighting the genes in S by their correlation
with C normalized by the sum of the correlations over all of the
genes in S
所以是否选用weight有点用

其实过滤剩下treat也好呀，将dmso与treat放在一起，过滤的时候按照行进行过滤，出来就不能合成矩阵了，但是gsea是可以做的，分开做

java -Xmx512m xtools.gsea.GseaPreranked -gmx /Users/lishasha/Desktop/home/project/2016/xielan_diabetes/Diabetes.gmt -collapse false -mode Max_probe -norm meandiv -nperm 1000 -rnk /Users/lishasha/Desktop/home/project/2015_new_work/20150904_7000compound/capital_bio/probe_test/diff_expression_gene/gsea/paste/column221.rnk_with_0_removed.rnk -scoring_scheme weighted -rpt_label my_analysis -include_only_symbols true -make_sets true -plot_top_x 20 -rnd_seed timestamp -set_max 500 -set_min 15 -zip_report false -out /Users/lishasha/gsea_home/output/三月02 -gui false

#多种检验r的实现例子

 R语言：常用统计检验 (2010-03-08 15:12:39)
转载

标签： r 统计 检验 杂谈 	分类： R语言学习
R语言：常用统计检验方法

写在前面

R已经成为当前国际学术界最流行的统计和绘图软件之一，该语言较为简单易学，统计分析功能强大，且具有很强的绘图功能，能够绘制学术出版要求的多种图表.R语言在生物信息学，进化生物学、生态学与环境、经济学、语言学等领域有着极为广泛的应用。

R软件是跨平台的，可以在Linux， MacOs， Windows等多种系统上运行。针对每个研究方向，有大量的科研人员编写了相关的程序包，可以导入到基本的程序平台上运行。现有的程序包已经超过了1800个，并且还在增加中。

不仅如此，R是完全免费的，而且全部代码是公开的。

读者可以到 http://cran.cnr.berkeley.edu/bin/windows/base/R-2.9.0-win32.exe

下载windows版的R软件，安装程序仅为30M。

学习并掌握R语言，对于需要用到统计学的研究人员和学生都是非常必要的。

这里选取了R语言中若干操作实例，所有的命令行均可以在R中运行，并得到结果。

正态总体均值的假设检验
t检验
单个总体
例一
某种元件的寿命X（小时），服从正态分布，N（mu,sigma^2），其中mu,sigma^2均未知，16只元件的寿命如下：问是否有理由认为元件的平均寿命大于255小时。
命令：
X<-c(159, 280, 101, 212, 224, 379, 179, 264,
222, 362, 168, 250, 149, 260, 485, 170)
t.test(X, alternative = "greater", mu = 225)
两个总体
例二
X为旧炼钢炉出炉率，Y为新炼钢炉出炉率，问新的操作能否提高出炉率
命令：
X<-c(78.1,72.4,76.2,74.3,77.4,78.4,76.0,75.5,76.7,77.3)
Y<-c(79.1,81.0,77.3,79.1,80.0,79.1,79.1,77.3,80.2,82.1)
t.test(X, Y, var.equal=TRUE, alternative = "less")
成对数据t检验
例三
对每个高炉进行配对t检验
命令：
X<-c(78.1,72.4,76.2,74.3,77.4,78.4,76.0,75.5,76.7,77.3)
Y<-c(79.1,81.0,77.3,79.1,80.0,79.1,79.1,77.3,80.2,82.1)
t.test(X-Y, alternative = "less")

正态总体方差的假设检验
例四
从小学5年级男生中抽取20名，测量其身高（厘米）如下：
问，在0.05显著性水平下，
平均值是否等于149
sigma^2 是否等于 75
命令：
X<-scan()
136 144 143 157 137 159 135 158 147 165
158 142 159 150 156 152 140 149 148 155
var.test(X,Y)
例五
对炼钢炉的数据进行分析
命令：
X<-c(78.1,72.4,76.2,74.3,77.4,78.4,76.0,75.5,76.7,77.3)
Y<-c(79.1,81.0,77.3,79.1,80.0,79.1,79.1,77.3,80.2,82.1)
var.test(X,Y)
二项分布的总体检验
例六 有一批蔬菜种子的平均发芽率为P=0.85,现在随机抽取500粒，用种衣剂进行浸种处理，结果有445粒发芽，问种衣剂有无效果。
命令：
binom.test(445,500,p=0.85)
例七 按照以往经验，新生儿染色体异常率一般为1%，某医院观察了当地400名新生儿，有一例染色体异常，问该地区新生儿染色体是否低于一般水平？
命令：
binom.test(1,400,p=0.01,alternative="less")
非参数检验
#数据是否正态分布的Neyman-Pearson 拟合优度检验-chisq
例八
5种品牌啤酒爱好者的人数如下
A 210
B 312
C 170
D 85
E 223
问不同品牌啤酒爱好者人数之间有没有差异？
命令：
X<-c(210, 312, 170, 85, 223)
chisq.test(X)
例九
检验学生成绩是否符合正态分布
命令：
X<-scan()
25 45 50 54 55 61 64 68 72 75 75
78 79 81 83 84 84 84 85 86 86 86
87 89 89 89 90 91 91 92 100
A<-table(cut(X, br=c(0,69,79,89,100)))
p<-pnorm(c(70,80,90,100), mean(X), sd(X))
p<-c(p[1], p[2]-p[1], p[3]-p[2], 1-p[3])
chisq.test(A,p=p)
# cut 将变量区域划分为若干区间
# table 计算因子合并后的个数
# 均值之间有无显著区别
大麦的杂交后代芒性状的比例 无芒：长芒： 短芒=9：3：4,而实际观测值为335：125：160 ,检验观测值是否符合理论假设？
命令：
chisq.test(c(335, 125, 160), p=c(9,3,4)/16)

例十
# 现有42个数据，分别表示某一时间段内电话总机借到呼叫的次数，
# 接到呼叫的次数 0   1   2   3   4   5   6
# 出现的频率     7   10  12  8   3   2   0
# 问：某个时间段内接到的呼叫次数是否符合Possion分布？
命令：
x<-0:6
y<-c(7,10,12,8,3,2,0)
mean<-mean(rep(x,y))
q<-ppois(x,mean)
n<-length(y)
p[1]<-q[1]
p[n]<-1-q[n-1]
for(i in 2:(n-1))
p<-q-q
chisq.test(y, p=p)
Z<-c(7, 10, 12, 8)
n<-length(Z); p<-p[1:n-1]; p[n]<-1-q[n-1]
chisq.test(Z, p=p)

内容来自
薛毅 陈立萍 《统计建模与Ｒ软件》 清华大学出版社 2006
本文引用地址：http://www.sciencenet.cn/blog/user_content.aspx?id=240107


R语言：常用统计检验-续
理论分布依赖于若干未知参数时
Kolmogorov-Smirnov 检验
ks.test()
例一 对一台设备进行寿命检验，记录十次无故障操作时间，并按从小到大的次序排列如下，
用ks检验方法检验此设备无故障工作时间是否符合rambda=1/1500的指数分布
命令：
X<-c(420, 500, 920, 1380, 1510, 1650, 1760, 2100, 2300, 2350)
ks.test(X, "pexp", 1/1500)
例二 假设从分布函数F(x)和G(x)的总体中分别随机抽取25个和20个观察值样本，检验F(x)和G(x)是否相同。
命令
X<-scan()
0.61 0.29 0.06 0.59 -1.73 -0.74 0.51 -0.56 0.39
1.64 0.05 -0.06 0.64 -0.82 0.37 1.77 1.09 -1.28
2.36 1.31 1.05 -0.32 -0.40 1.06 -2.47
Y<-scan()
2.20 1.66 1.38 0.20 0.36 0.00 0.96 1.56 0.44
1.50 -0.30 0.66 2.31 3.29 -0.27 -0.37 0.38 0.70
0.52 -0.71
ks.test(X, Y)
ks多样本检验的局限性，只用在理论分布为一维连续分布，且分布完全已知的情形。ks检验可用的情况下，功效一般优于Pearson chisq检验
列联表（contingerncy table）的独立性检验
Pearson chisquare 进行独立性检验
例三 为了研究吸烟是否与肺癌有关，对63位患者及43名非肺癌患者调查了其中的吸烟人数，得到2*2列联表
数据     肺癌     健康      合计
吸烟     60        32          92
不吸烟   3        11          14
合计     63        43         106
命令
x<-c(60, 3, 32, 11)
dim(x)<-c(2,2)
chisq.test(x,correct = FALSE) # 不带连续校正的情况
chisq.test(x) # 带连续校正的情况
例四
在 一次社会调查中，以问卷方式调查了901人的年收入，及其对工作的满意程度，其中年收入A分为四档：小于6000元，6000-15000元，15000 元至25000元，超过25000元。对工作的满意程度B 分为 很不满意，较不满意，基本满意和很满意四档，结果如下
                    很不满意  较不满意   基本满意   很满意        合计
< 6000                      20        24         80           82            206
6000 ~15000          22        38         104        125           289
15000 ~25000        13        28         81          113           235
> 25000                    7          18         54           92            171
合计                           62        108       319        412           901
命令如下
x<-scan()
20 24 80 82 22 38 104 125
13 28 81 113 7 18 54 92
dim(x)<-c(4,4)
chisq.test(x)
Fisher 精确的独立检验
试用条件 样本数小于4
例五
某医师研究乙肝免疫球蛋白防止子宫内胎儿感染HBV的效果，将33例HBsAg阳性孕妇随机分为预防注射组和对照组，结果由下表所示，两组新生儿HBV总体感染率有无差别
组别           阳性    阴性    合计   感染率
预防注射组     4       18      22     18.8
对照组         5       6       11     45.5
命令如下
x<-c(4,5,18,6); dim(x)<-c(2,2)
fisher.test(x)
对前面提到的肺癌进行检验
x<-c(60, 3, 32, 11); dim(x)<-c(2,2)
fisher.test(x)
McNemar检验
McNemar检验不是独立性检验，但是是关于列连表的检验
例六
甲乙两种方法检测细菌的结果
         乙方法          
                           合计
甲方法   +        -            
+           49      25         74
-            21      107        128
合计     70      132        202
命令
X<-c(49, 21, 25, 107); dim(X)<-c(2,2)
mcnemar.test(X,correct=FALSE)
符号检验
1 假设一个样本是否来自某个总体
例七
联合国人员在世界上66个大城市的生活花费指数（以纽约1996年12月为100），按照从小到大的次序排列如下，其中北京的指数为99。假设这个样本是从世界大城市中随机抽样得到的。用符号检验分析，北京是在中位数之上，还是中位数之下。
X<-scan()
66 75 78 80 81 81 82 83 83 83 83
84 85 85 86 86 86 86 87 87 88 88
88 88 88 89 89 89 89 90 90 91 91
91 91 92 93 93 96 96 96 97 99 100
101 102 103 103 104 104 104 105 106 109 109
110 110 110 111 113 115 116 117 118 155 192
binom.test(sum(X>99), length(X), al="l")
2 用成对样本检验两总体间是否有差异
例八
两种不同饲料，对猪增重情况如下，分析两种饲料养猪有无差异
命令
x<-scan()
25 30 28 23 27 35 30 28 32 29 30 30 31 16
y<-scan()
19 32 21 19 25 31 31 26 30 25 28 31 25 25
binom.test(sum(x<y), length(x))
例九
某饮料店为调查了顾客对饮料的爱好情况，某日随机调查了13为顾客，喜欢奶茶超过咖啡用-表示，喜欢咖啡超过奶茶用+表示，两者都喜欢用0表示，结果如下，分析顾客是更喜欢咖啡开始奶茶。
顾客编号 1 2 3 4 5 6 7 8 9 10 11 12 13
喜欢咖啡 1   1 1 1 0 1   1  1  1    1
喜欢奶茶    1           1          1
binom.test(3,12,p=1/2, al="l", conf.level = 0.90)
秩统计量
Spearman秩相关检验
例十
一项有六人参加表演的竞赛，有两人进行评定，评定结果用如表所示，试用Spearman秩相关检验方法检验这两个评定员对于等级评定有无相关性
选手编号 1 2 3 4 5 6
甲的打分 4 2 2 4 5 6
乙的打分 5 3 4 3 2 5
x<-c(4,2,2,4,5,6); y<-c(5,3,4,3,2,5)
cor.test(x, y, method = "spearman")
Kendall相关检验
例十一
某幼儿园对9对双胞胎的智力进行测验，并按照百分制打分，试用Kendall相关检验方法检验双胞胎的智力是否相关。
1   2  3  4  5  6  7  8   9
86  77 68 91 70 71 85 87 63
88  76 64 96 65 80 81 72 60
X<-c(86, 77, 68, 91, 70, 71, 85, 87, 63)
Y<-c(88, 76, 64, 96, 65, 80, 81, 72, 60)
cor.test(X, Y, method = "kendall")
Wilcoxon秩检验―― 考虑了样本观察值月总体中位数的差。
1 对于来自同一个总体样本的检验
例十二
某 电池厂称其生产的某种电池，中位数为140安培小时，现随机从其新生产的电池中抽取20个，检验其寿命，137.0 140.0 138.3 139.0 144.3 139.1 141.7 137.3 133.5 138.2 141.1 139.2 136.5 136.5 135.6 138.0 140.9 140.6 136.3 134.1
用Wilcoxon符号检验分析该厂生产的电池是否符合标准
X<-scan()
137.0 140.0 138.3 139.0 144.3 139.1 141.7 137.3 133.5 138.2
141.1 139.2 136.5 136.5 135.6 138.0 140.9 140.6 136.3 134.1
wilcox.test(X, mu=140, alternative="less",
exact=FALSE, correct=FALSE, conf.int=TRUE)
该方法也可用于成对样本的检验
例十三
为检验某种新肥料，将现有麦地分为十块，再将每一块分为两部分，一半施普通肥料，一半儿施新肥料，用Wilcoxon符号检验法检验新复合肥能否显著提高小麦产量。
1   2   3   4   5   6   7   8   9   10
459 367 303 392 310 342 421 446 430 412
414 306 321 443 281 301 353 391 405 390
  
x<-c(459, 367, 303, 392, 310, 342, 421, 446, 430, 412)
y<-c(414, 306, 321, 443, 281, 301, 353, 391, 405, 390)
wilcox.test(x, y, alternative = "greater", paired = TRUE)
wilcox.test(x-y, alternative = "greater")
binom.test(sum(x>y), length(x), alternative = "greater")
非成对样本的秩次和检验
Wilcoxon-Mann-Whitney 统计量 U
例十四
测量了10名不同作业组的工人血铅含量，分析两组之间是否有差别。
非铅作业组 24 26 29 34 43 58 63 72 87 101
含铅作业组 82 87 97 121 164 208 213
x<-c(24, 26, 29, 34, 43, 58, 63, 72, 87, 101)
y<-c(82, 87, 97, 121, 164, 208, 213)
wilcox.test(x,y,alternative="less",exact=FALSE,correct=FALSE)
wilcox.test(x, y, alternative="less", exact=FALSE)
例十五
学生数学能力排序
新方法 3 5 7 9 10
原方法 1 2 4 6 8
新方法 4 6 7 9 10
原方法 1 2 3 5 8
x<-c(3, 5, 7, 9, 10); y<-c(1, 2, 4, 6, 8)
wilcox.test(x, y, alternative="greater")

例十六
检验一种药物对于慢性支气管炎有没有效果，抽取了216个病例，治疗效果。分析该药物对两种慢性支气管炎的治疗效果是否相同。
      控制 显效 进步 无效
单纯型 62   41   14   11
喘息型 20   37   16   15
x<-rep(1:4, c(62, 41, 14,11)); y<-rep(1:4, c(20, 37, 16, 15))
wilcox.test(x, y, exact=FALSE) 



# 86 hits compounds 2016 with dmso
#normalization
stable<-read.table("stable.txt",head=T,row.names=1)
total<-read.table("filtered_samples.txt",head=T,row.names=1)
me<-apply(stable,2,median)
head(me)
for (i in 1:337){
    total[,i]=round(100*total[,i]/me[i],digits=2)
}
write.table(total,"normalized_by_median100.txt")

# get fc
treat<-read.table("normalized_by_median100.txt",head=T,row.names=1)
control<-read.table("dmso.txt",head=T,row.names=1)
treat<-(log2(treat/as.matrix(control[,1]))+log2(treat/as.matrix(control[,2]))+log2(treat/as.matrix(control[,3]))+log2(treat/as.matrix(control[,4]))+log2(treat/as.matrix(control[,5]))+log2(treat/as.matrix(control[,6]))+log2(treat/as.matrix(control[,7]))+log2(treat/as.matrix(control[,8]))+log2(treat/as.matrix(control[,9])))/9
write.table(treat,"fc.txt")

# filtered fc

fc<-read.table("fc.txt",head=T,row.names=1)
dmso<-read.table("dmso.txt",head=T,row.names=1,sep="\t")
treat<-read.table("normalized_by_median100.txt",head=T,row.names=1)
stable<-read.table("stable.txt",head=T,row.names=1,sep="\t")
dmso1<-apply(dmso,1,mean)
me<-apply(stable,2,median)
dmso_cutoff<-as.numeric(dmso1)
for (i in 1:337){
    cutoff<-as.numeric(treat[,i])
    indexi<-which(cutoff<=100*10/me[i] & dmso_cutoff<=9.809342674)
    Timei <- rep(0, length(cutoff))
    fc[indexi,i]<-as.numeric(Timei[indexi])
}
write.table(fc,"plate3_fc_filtered_by_median5_both10.txt")

for (i in 1:337){
    cutoff<-as.numeric(fc[,i])
    indexi<-which(cutoff <1 & cutoff >-1)
    Timei <- rep(0, length(cutoff))
    fc[indexi,i]<-as.numeric(Timei[indexi])
}
write.table(fc,"plate3_fc_by_minor1to1_both10.txt")

for (i in 1:337){
    cutoff<-as.numeric(fc[,i])
    indexi<-which(cutoff!=0)
    Timei <- rep(10, length(cutoff))
    fc[indexi,i]<-as.numeric(Timei[indexi])
}
write.table(fc,"plate3_fc_modified_final_v2_both10.txt")

a<-matrix(rep(1:337,2), nrow = 2)
for (i in 1:337){
    a[,i]<-table(fc[,i])
}

colnames(a) <- colnames(fc)
write.table(a,"plate3_clasification_both10.txt")

# for scoring
fc<-read.table("7dmso_fc_by_minor1to1_both10_test.txt",head=T,row.names=1)

for (i in 1:337){
    cutoff<-as.numeric(fc[,i])
    indexi<-which(cutoff < -1)
    Timei <- rep(-1, length(cutoff))
    fc[indexi,i]<-as.numeric(Timei[indexi])
}

for (i in 1:337){
    cutoff<-as.numeric(fc[,i])
    indexi<-which(cutoff >1)
    Timei <- rep(1, length(cutoff))
    fc[indexi,i]<-as.numeric(Timei[indexi])
}

write.table(fc,"7dmso_fc_for_scoring.txt")


#
library(pheatmap)
pdf(file="86hits_dmso.pdf")
data<-read.table("dmso.txt",head=T,sep='\t',row.names=1)
data1<-log2(data)
data<-data.matrix(data1)
pheatmap(data,cluster_row=TRUE,clustering_method="average",cluster_col=TRUE,cellwidth=8,fontsize_row=2,fontsize_col=8)
dev.off()

stable<-read.table("stable.txt",head=T,row.names=1)
total<-read.table("filtered_samples.txt",head=T,row.names=1)
me<-apply(stable,2,median)
head(me)
for (i in 1:337){
    total[,i]=round(100*total[,i]/me[i],digits=2) #
}
write.table(total,"normalized_by_median100.txt")
#fc
treat<-read.table("normalized_by_median100.txt",head=T,row.names=1)
control<-read.table("dmso.txt",head=T,row.names=1)
treat<-(log2(treat/as.matrix(control[,1]))+log2(treat/as.matrix(control[,2]))+log2(treat/as.matrix(control[,3]))+log2(treat/as.matrix(control[,4]))+log2(treat/as.matrix(control[,5]))+log2(treat/as.matrix(control[,6]))+log2(treat/as.matrix(control[,7]))+log2(treat/as.matrix(control[,8]))+log2(treat/as.matrix(control[,9])))/9
write.table(treat,"fc.txt")

#
a<-read.table("pcc_absolute.txt",head=T,row.names=1)
ta<-replace(a,abs(a)<0.9,0)
ta1<-abs(ta)
ta2<-ta1[,1:4017]
dim(ta2)
p<-rowSums(ta2)
write.table(p,"rowsums_abs_absolute_cor90.txt")

#捞出大于.95的基因
ta<-read.table("ta_total.txt",head=T,row.names=1,sep="\t")
ta<-replace(ta,abs(ta)<0.96,0)
keep = rowSums(ta)!=0
dim(keep)
write.table(keep,"abovecor95.txt",sep="\t")
dim(ta)
write.table(ta,"abovecor95_not_filterec.txt",sep="\t")
head(ta)
max(ta[,1])
p<-rowSums(ta)
head(p)
ta1<-abs(ta))
ta1<-abs(ta)
p<-rowSums(ta1)
write.table(p,"rowsums_abs_ta.txt")
history()



数据库
bp. Hi-C data are downloaded
from the Gene Expression Omnibus (GEO) 
The ChIP-seq data of all histone modifications are download from the
ENCODE project (25) and the Roadmap Epigenomics

Gene Ontology (GO) enrichment analysis is performed
using FuncAssociate (31).
Functional similarity between two genes is computed
using the Resnik functional similarity measure that


#根据选择的decircrna作火山图
a<-read.table("fig3A_volcano_selected.txt",head=T,row.names=1)
dim(a)
pdf("fig3A_volcano_selected.pdf")
plot(a$FC,a$PVALUE,col=c("green","black","red")[a$Regulation],lty=2,pch=20,cex=0.2,xlab="Log2(linear-tumor/linear-normal)",ylab="Log2(circular-tumor/circular-normal)")
dev.off()
#
a<-read.table("fig3A_volcano_selected.txt",head=T,row.names=1)
dim(a)
pdf("fig3A_volcano_selected.pdf")
plot(a$FC,a$PVALUE,col=c("green","black","red")[a$Regulation],lty=2,pch=20,cex=0.5,xlab="Log2(linear-tumor/linear-normal)",ylab="Log2(circular-tumor/circular-normal)",abline(h=1.30103,v=c(-2,-1,1,2),col="red",lty=2))
dev.off()
#
a<-read.table("fig3A_volcano_classify.txt",head=TRUE,sep="\t",row.names=1)
P<- c(a$PVALUE)
FC <- c(a$FC)
df <- data.frame(P, FC)
pdf("fig3A_volcano_classify.pdf")
thred = c()
for (i in 1:dim(df)[1]) {
    if ((df$FC[i] < 6 & df$FC[i] > 2) & df$P[i] > 1.30103) {
        thred = c(thred,"red4")
        
    } else if ((df$FC[i] < 2 & df$FC[i] > 1) & df$P[i] > 1.30103) {
        thred = c(thred,"red")
        
    } else if ((df$FC[i] < -1 & df$FC[i] > -2) & df$P[i] > 1.30103) {
        thred = c(thred,"green")  
          
    } else if ((df$FC[i] < -2 & df$FC[i] > -9) & df$P[i] > 1.30103) {
        thred = c(thred,"green4") 
    } else {
        thred = c(thred, "black")
    }
}
df$threshold = as.factor(thred)
plot(FC,P,lty=2,pch=20,cex=0.5,col=c("black","green","green4","red","red4")[df$threshold],xlim=c(-9, 6),ylim=c(-1, 8),xlab="Log2(linear-tumor/linear-normal)",ylab="Log2(circular-tumor/circular-normal)",abline(h=1.30103,v=c(-2,-1,1,2),col="red",lty=2))
dev.off()
q()


#RCLRde scatterplot
layout(matrix(c(1,2,3,4,5,6,7,8,9),nrow=3,byrow=T))

a<-read.table("fig3A_1.txt",head=T,row.names=1)
plot(a$linearRNA,a$circRNA,col=c("green","black","red")[a$Regulation],lty=2,pch=20,cex=0.2,xlab="Log2(linear-1T/linear-1N)",ylab="Log2(circular-1T/circular-1N)")
a<-read.table("fig3A_2.txt",head=T,row.names=1)
plot(a$linearRNA,a$circRNA,col=c("green","black","red")[a$Regulation],lty=2,pch=20,cex=0.2,xlab="Log2(linear-2T/linear-2N)",ylab="Log2(circular-2T/circular-2N)")
a<-read.table("fig3A_3.txt",head=T,row.names=1)
plot(a$linearRNA,a$circRNA,col=c("green","black","red")[a$Regulation],lty=2,pch=20,cex=0.2,xlab="Log2(linear-3T/linear-3N)",ylab="Log2(circular-3T/circular-3N)")
a<-read.table("fig3A_4.txt",head=T,row.names=1)
plot(a$linearRNA,a$circRNA,col=c("green","black","red")[a$Regulation],lty=2,pch=20,cex=0.2,xlab="Log2(linear-4T/linear-4N)",ylab="Log2(circular-4T/circular-4N)")
a<-read.table("fig3A_5.txt",head=T,row.names=1)
plot(a$linearRNA,a$circRNA,col=c("green","black","red")[a$Regulation],lty=2,pch=20,cex=0.2,xlab="Log2(linear-5T/linear-5N)",ylab="Log2(circular-5T/circular-5N)")
a<-read.table("fig3A_8.txt",head=T,row.names=1)
plot(a$linearRNA,a$circRNA,col=c("green","black","red")[a$Regulation],lty=2,pch=20,cex=0.2,xlab="Log2(linear-8T/linear-8N)",ylab="Log2(circular-8T/circular-8N)")
a<-read.table("fig3A_9.txt",head=T,row.names=1)
plot(a$linearRNA,a$circRNA,col=c("green","black","red")[a$Regulation],lty=2,pch=20,cex=0.2,xlab="Log2(linear-9T/linear-9N)",ylab="Log2(circular-9T/circular-9N)")
a<-read.table("fig3A_10.txt",head=T,row.names=1)
plot(a$linearRNA,a$circRNA,col=c("green","black","red")[a$Regulation],lty=2,pch=20,cex=0.2,xlab="Log2(linear-10T/linear-10N)",ylab="Log2(circular-10T/circular-10N)")
a<-read.table("fig3A_16.txt",head=T,row.names=1)
plot(a$linearRNA,a$circRNA,col=c("green","black","red")[a$Regulation],lty=2,pch=20,cex=0.2,xlab="Log2(linear-16T/linear-16N)",ylab="Log2(circular-16T/circular-16N)")
a<-read.table("fig3A_17.txt",head=T,row.names=1)
plot(a$linearRNA,a$circRNA,col=c("green","black","red")[a$Regulation],lty=2,pch=20,cex=0.2,xlab="Log2(linear-17T/linear-17N)",ylab="Log2(circular-17T/circular-17N)")



a<-read.table("fig3A_average.txt",head=T,row.names=1)
dim(a)
pdf("fig3A_10.pdf")
plot(a$FC,a$CLR,col=c("green","black","red")[a$Regulation],lty=2,pch=20,cex=0.2,xlab="Log2(linear-10T/linear-10N)",ylab="Log2(circular-10T/circular-10N)")
dev.off()
1: In min(x) : min里所有的参数都不存在; 回Inf
2: In max(x) : max里所有的参数都不存在；回覆-Inf
3: In min(x) : min里所有的参数都不存在; 回覆Inf
4: In max(x) : max里所械牟问都不存在；回17-Inf
#下标出界的原因是plot(a$FC,a$CLR,与数据标记不符合

. layout()：mat用矩阵设置窗口的划分，矩阵的0元素表示该位置不画图，非0元素必须包括从1开始的连续的整数值，比如：1……N，按非0元素的大小设置图形的顺序。widths用来设置窗口不同列的宽度，heights设置不同行的高度。par()的mfcol,和mfrow参数灿欣嗨layout的功能。layout()函数的一般形式为layout(mat)，mat为一矩阵，mat元素的数量决定了一个output device被等分成几份相同元素为一块。

layout(matrix(c(1,2,3,0,2,3,0,0,3),nr=3)) matrix有9个元素，具有这样的形式：


＃volcanoplot最终版本20160117 circrna 分figure2A
a<-read.csv("volcanoplot.csv",head=TRUE,sep=";")
P<- c(a$PValue)
FC <- c(a$FC)
df <- data.frame(P, FC)
pdf("figure2A_volcano.pdf")
thred = c()
for (i in 1:dim(df)[1]) {
    if ((df$FC[i] < 4 & df$FC[i] > 2) & df$P[i] > 1.30103) {
        thred = c(thred,"red4")
        
    } else if ((df$FC[i] < 2 & df$FC[i] > 1) & df$P[i] > 1.30103) {
        thred = c(thred,"red")
        
    } else if ((df$FC[i] < -1 & df$FC[i] > -2) & df$P[i] > 1.30103) {
        thred = c(thred,"green")  
          
    } else if ((df$FC[i] < -2 & df$FC[i] > -4) & df$P[i] > 1.30103) {
        thred = c(thred,"green4") 
    } else {
        thred = c(thred, "black")
    }
}
df$threshold = as.factor(thred)
levels(df$threshold)
[1] "black"  "green"  "green4" "red"    "red4"  #按照这个顺序给col=c("black","green","green4","red","red4")就是自己想要的颜色顺序
plot(FC,P,lty=2,pch=20,cex=0.5,col=c("black","green","green4","red","red4")[df$threshold],xlim=c(-3, 4),ylim=c(0, 8),xlab="Log2(fold change)",ylab="-log10(p-value)",abline(h=1.30103,v=c(-2,-1,1,2),col="red",lty=2))
dev.off()
q()


#volcano plot的几个版本
library(ggplot2)
a<- read.table("mRNA2.csv",head=TRUE,sep=";")
P.Value <- c(a$PValue)
FC <- c(a$FC)
P.Value
FC <- c(a$FC)
FC
data <- data.frame(P.Value, FC)
data
data$threshold = as.factor(abs(data$FC) > 1 & data$P.Value < 0.05)
data
g = ggplot(data=df, aes(x=FC, y=-log10(P.Value), colour=threshold)) +
 geom_point(alpha=0.4, size=1.75) +
  opts(legend.position = "none") +
  xlim(c(-5, 5)) + ylim(c(0, 5)) +
  xlab("log2 fold change") + ylab("-log10 p-value")
g = ggplot(data=data, aes(x=FC, y=-log10(P.Value), colour=threshold)) +
 geom_point(alpha=0.4, size=1.75) +
  opts(legend.position = "none") +
  xlim(c(-5, 5)) + ylim(c(0, 5)) +
  xlab("log2 fold change") + ylab("-log10 p-value")
?ggplot2
??ggplot2
g = ggplot2(data=data, aes(x=FC, y=-log10(P.Value), colour=threshold)) +
 geom_point(alpha=0.4, size=1.75) +
  opts(legend.position = "none") +
  xlim(c(-5, 5)) + ylim(c(0, 5)) +
  xlab("log2 fold change") + ylab("-log10 p-value")
g = ggplot(data=data, aes(x=FC, y=-log10(P.Value), colour=threshold)) +
 geom_point(alpha=0.4, size=1.75) +
  theme(legend.position = "none") +
  xlim(c(-5, 5)) + ylim(c(0, 5)) +
  xlab("log2 fold change") + ylab("-log10 p-value")
g
dev.off()
q()
library(ggplot2)
a<- read.csv("mRNA2.csv",head=TRUE,sep=";")
a
a<- read.csv("mRNA_right.csv",head=TRUE,sep=";")
a
head(a)
P.Value <- c(a$PValue)
FC <- c(a$FC)
P.Value
FC
df <- data.frame(P.Value, FC)
df
df$threshold = as.factor(abs(df$FC) > 1 & df$P.Value < 0.05)
g = ggplot(data=df, aes(x=FC, y=-log10(P.Value), colour=threshold)) +
 geom_point(alpha=0.4, size=1.75) +
  theme(legend.position = "none") +  
  xlim(c(-5, 5)) + ylim(c(0, 5)) +
  xlab("log2 fold change") + ylab("-log10 p-value")
g
dev.off()
q()
library(ggplot2)
tiff("new.tiff")
a<- read.csv("mRNA2.csv",head=TRUE,sep=";")
P.Value <- c(a$PValue)
FC <- c(a$FC)
df <- data.frame(P.Value, FC)
df$threshold = as.factor(abs(df$FC) > 1 & df$P.Value < 0.05)
df
df$threshold = as.factor(abs(df$FC) > 2 & df$P.Value < 1.30103)
df
g = ggplot(data=df, aes(x=FC, y=-log10(P.Value), colour=threshold)) +
 geom_point(alpha=0.4, size=1.75) +
  theme(legend.position = "none") +  
  xlim(c(-5, 5)) + ylim(c(0, 5)) +
  xlab("log2 fold change") + ylab("-log10 p-value")
g
dev.off()
q()
history
history()
library(ggplot2)
tiff("new2.tiff")
a<- read.csv("mRNA2.csv",head=TRUE,sep=";")
P.Value <- c(a$PValue)
FC <- c(a$FC)
df <- data.frame(P.Value, FC)
df$threshold = as.factor(abs(df$FC) > 1 & df$P.Value > 1.30103)
g = ggplot(data=df, aes(x=FC, y=-log10(P.Value), colour=threshold)) +
 geom_point(alpha=0.4, size=1.75) +
  theme(legend.position = "none") +  
  xlim(c(-5, 5)) + ylim(c(0, 5)) +
  xlab("log2 fold change") + ylab("-log10 p-value")
g
dev.off()
y
q()
a<- read.csv("mRNA_right.csv",head=TRUE,sep=";")
P.Value <- c(a$PValue)
FC <- c(a$FC)
df <- data.frame(P.Value, FC)
pdf("Results5.pdf")
thred = c()
for (i in 1:dim(df)[1]) {
    if ((df$FC[i] < 4 & df$FC[i] > 2) & df$P.Value[i] > 1.30103) {
        thred = c(thred,"red4")
        
    } else if ((df$FC[i] < 2 & df$FC[i] > 1) & df$P.Value[i] > 1.30103) {
        thred = c(thred,"red")
        
    } else if ((df$FC[i] < -1 & df$FC[i] > -2) & df$P.Value[i] > 1.30103) {
        thred = c(thred,"green")  
          
    } else if ((df$FC[i] < -2 & df$FC[i] > -4) & df$P.Value[i] > 1.30103) {
        thred = c(thred,"green4") 
           
    } else if ((df$FC[i] < 1 & df$FC[i] > -1) & df$P.Value[i] > 1.30103) {
        thred = c(thred,"yellow")  
          
    } else if ((df$FC[i] < 1 & df$FC[i] > -1) & df$P.Value[i] < 1.30103) {
        thred = c(thred,"pink")
        
    } else {
        thred = c(thred, "black")
    }
}
df$threshold = as.factor(thred)
plot(FC,P.Value,colour=threshold,xlim(c(-3, 4)),ylim(c(0, 8)),xlab("log2 fold change"),ylab("-log10 p-value"));abline(h=1.30103,v=c(-2,-1,1,2,3),col="red",lty=2)
dev.off()
q()
library(ggplot2)
a<- read.csv("mRNA_right.csv",head=TRUE,sep=";")
P.Value <- c(a$PValue)
FC <- c(a$FC)
df <- data.frame(P.Value, FC)
thred = c()
for (i in 1:dim(df)[1]) {
    if ((df$FC[i] < 3 & df$FC[i] > 2) & df$P.Value[i] > 1.30103) {
        thred = c(thred,"red4")
        
    } else if ((df$FC[i] < 2 & df$FC[i] > 1) & df$P.Value[i] > 1.30103) {
        thred = c(thred,"red")
        
    } else if ((df$FC[i] < -1 & df$FC[i] > -2) & df$P.Value[i] > 1.30103) {
        thred = c(thred,"green")  
          
    } else if ((df$FC[i] < -2 & df$FC[i] > -3) & df$P.Value[i] > 1.30103) {
        thred = c(thred,"green4") 
           
    } else if ((df$FC[i] < 1 & df$FC[i] > -1) & df$P.Value[i] > 1.30103) {
        thred = c(thred,"yellow")  
          
    } else if ((df$FC[i] < 1 & df$FC[i] > -1) & df$P.Value[i] < 1.30103) {
        thred = c(thred,"pink")
        
    } else {
        thred = c(thred, "black")
    }
}
#df$threshold = as.factor(((df$FC<4 & df$FC >2) & df$P.Value > 1.30103) & ((df$FC<2 & df$FC > 1) & df$P.Value > 1.30103) & ((df$FC<-1 & df$FC > -2) & df$P.Value > 1.30103) & ((df$FC<-2 & df$FC > -4) & df$P.Value > 1.30103) & ((df$FC<1 & df$FC > -1) & df$P.Value > 1.30103) & ((df$FC<1 & df$FC > -1) & df$P.Value < 1.30103))
df$threshold = as.factor(thred)
pdf("volcanoplot_no_abline.pdf")
g = ggplot(data=df, aes(x=FC, y=P.Value, colour=threshold)) + geom_point(alpha=0.4, size=1.75) + theme(legend.position = "none") +  xlim(c(-3, 4)) + ylim(c(0, 8)) + xlab("log2 fold change") + ylab("-log10 p-value")
g
dev.off()
library(ggplot2)
a<- read.csv("mRNA_right.csv",head=TRUE,sep=";")
P.Value <- c(a$PValue)
FC <- c(a$FC)
df <- data.frame(P.Value, FC)
thred = c()
for (i in 1:dim(df)[1]) {
    if ((df$FC[i] < 3 & df$FC[i] > 2) & df$P.Value[i] > 1.30103) {
        thred = c(thred,"red4")
        
    } else if ((df$FC[i] > 3) & df$P.Value[i] > 1.30103) {
        thred = c(thred,"blue")
        
    } else if ((df$FC[i] < 2 & df$FC[i] > 1) & df$P.Value[i] > 1.30103) {
        thred = c(thred,"red")
        
    } else if ((df$FC[i] < -1 & df$FC[i] > -2) & df$P.Value[i] > 1.30103) {
        thred = c(thred,"green")  
          
    } else if ((df$FC[i] < -2 & df$FC[i] > -3) & df$P.Value[i] > 1.30103) {
        thred = c(thred,"green4") 
           
    } else if ((df$FC[i] < 1 & df$FC[i] > -1) & df$P.Value[i] > 1.30103) {
        thred = c(thred,"yellow")  
          
    } else if ((df$FC[i] < 1 & df$FC[i] > -1) & df$P.Value[i] < 1.30103) {
        thred = c(thred,"pink")
        
    } else {
        thred = c(thred, "black")
    }
}
#df$threshold = as.factor(((df$FC<4 & df$FC >2) & df$P.Value > 1.30103) & ((df$FC<2 & df$FC > 1) & df$P.Value > 1.30103) & ((df$FC<-1 & df$FC > -2) & df$P.Value > 1.30103) & ((df$FC<-2 & df$FC > -4) & df$P.Value > 1.30103) & ((df$FC<1 & df$FC > -1) & df$P.Value > 1.30103) & ((df$FC<1 & df$FC > -1) & df$P.Value < 1.30103))
df$threshold = as.factor(thred)
pdf("volcanoplot_no_abline_changed.pdf")
g = ggplot(data=df, aes(x=FC, y=P.Value, colour=threshold)) + geom_point(alpha=0.4, size=1.75) + theme(legend.position = "none") +  xlim(c(-3, 4)) + ylim(c(0, 8)) + xlab("log2 fold change") + ylab("-log10 p-value")
g
dev.off()
a<- read.csv("mRNA_right.csv",head=TRUE,sep=";")
P.Value <- c(a$PValue)
FC <- c(a$FC)
df <- data.frame(P.Value, FC)
pdf("Results6_plot_edit.pdf")
thred = c()
for (i in 1:dim(df)[1]) {
    if ((df$FC[i] < 4 & df$FC[i] > 2) & df$P.Value[i] > 1.30103) {
        thred = c(thred,"red4")
        
    } else if ((df$FC[i] < 2 & df$FC[i] > 1) & df$P.Value[i] > 1.30103) {
        thred = c(thred,"red")
        
    } else if ((df$FC[i] < -1 & df$FC[i] > -2) & df$P.Value[i] > 1.30103) {
        thred = c(thred,"green")  
          
    } else if ((df$FC[i] < -2 & df$FC[i] > -4) & df$P.Value[i] > 1.30103) {
        thred = c(thred,"green4") 
           
    } else if ((df$FC[i] < 1 & df$FC[i] > -1) & df$P.Value[i] > 1.30103) {
        thred = c(thred,"yellow")  
          
    } else if ((df$FC[i] < 1 & df$FC[i] > -1) & df$P.Value[i] < 1.30103) {
        thred = c(thred,"pink")
        
    } else {
        thred = c(thred, "black")
    }
}
df$threshold = as.factor(thred)
plot(FC,P.Value,lty=2,pch=20,cex=0.5,col=threshold,xlim(c(-3, 4)),ylim(c(0, 8)),xlab("log2 fold change"),ylab("-log10 p-value"),abline(h=1.30103,v=c(-2,-1,1,2,3),col="red",lty=2))
dev.off()
q()



#然后就是直接利用igraph包的做图功能绘制相应的网络图。考虑前面提到的用户同名情况，直接用id来做后续的分析。

library(igraph)
people = data.frame(id = tmp1[, 4], name = tmp1[, 3])
gg = graph.data.frame(d = tmp2, directed = F, vertices = people)
is.simple(gg)
gg = simplify(gg)
## 去掉重复的连接
is.simple(gg)
dg = degree(gg)
gg = subgraph(gg, which(dg > 0) - 1)
## 去掉孤立点

## png("net_simple.png", width = 500, height = 500)
par(mar = c(0, 0, 0, 0))
set.seed(14)
plot(gg, layout = layout.fruchterman.reingold, vertex.size = 5, vertex.label = NA,
    edge.color = grey(0.5), edge.arrow.mode = "-")
## dev.off()
三、子群分割

信息的分类和过滤是社会网络服务的一项特征，例如人人网对好友关系有一套自己的分类方式，用户可以自行对好友进行分组，从而对信息的收发做分组的管理。但是作为用户却未必能够养成并保持这种分组的习惯（例如作者自己就从来没有对好友做过分组）。与此同时我们揣测，作为真实关系的线上反映，人人网的好友网络是能够自动呈现出一定的人群分割的，而在社会网络分析中，对网络成分的分析也确实是一项重点。通过分析网络的结构，提取出其中的子群，能够让我们更好地理解这个网络的组成方式，从而更好地管理和利用信息流。

寻找子群的算法有很多，igraph包提供了若干函数以实现对网络子群的搜索，本文采用了其中的walktrap.community()函数，更多细节及其他算法可以查看帮助文档。

为了在网络图中展示这些子群，我们采用不同的颜色来标记他们。

com = walktrap.community(gg, steps = 5)
subgroup = split(com$labels, com$membership)
## subgroup
V(gg)$sg = com$membership + 1
V(gg)$color = rainbow(max(V(gg)$sg))[V(gg)$sg]
## png("net_walktrap.png", width = 500, height = 500)
par(mar = c(0, 0, 0, 0))
set.seed(14)
plot(gg, layout = layout.fruchterman.reingold, vertex.size = 5,
    vertex.color = V(gg)$color, vertex.label = NA, edge.color = grey(0.5),
    edge.arrow.mode = "-")
## dev.off()
#用平均作图
a<-read.table("figure2A_average.txt",head=T,row.names=1)
dim(a)
pdf("figure2A_average.pdf")
plot(a$N_average,a$T_average,col=c("green","black","red")[a$Regulation],lty=2,pch=20,cex=0.3,xlab="Log2(normalized density of normal)",ylab="Log2(normalized density of tumor)")
dev.off()


#指定颜色
a<-read.table("tmp.txt",head=T,row.names=1,sep="\t")
dim(a)
a
plot(a$a,a$b,col=c("red","blue","green")[a$color])
history()

#circRNA fig1a scatterplot,指定颜色,画成2*5
layout(matrix(c(1,2,3,4,5,6,7,8,9,10),nrow=2,byrow=T))
a<-read.table("figure2A_1.txt",head=T,row.names=1)
dim(a)
pdf("figure2A_1.pdf")
plot(a$P1N,a$P1T,col=c("green","black","red")[a$Regulation],lty=2,pch=20,cex=0.3,xlab="Log2(normalized density of 1N)",ylab="Log2(normalized density of 1T)")
dev.off()

a<-read.table("figure2A_2.txt",head=T,row.names=1)
dim(a)
pdf("figure2A_2.pdf")
plot(a$P2N,a$P2T,col=c("green","black","red")[a$Regulation],lty=2,pch=20,cex=0.3,xlab="Log2(normalized density of 2N)",ylab="Log2(normalized density of 2T)")
dev.off()

a<-read.table("figure2A_3.txt",head=T,row.names=1)
dim(a)
pdf("figure2A_3.pdf")
plot(a$P3N,a$P3T,col=c("green","black","red")[a$Regulation],lty=2,pch=20,cex=0.3,xlab="Log2(normalized density of 3N)",ylab="Log2(normalized density of 3T)")
dev.off()

a<-read.table("figure2A_4.txt",head=T,row.names=1)
dim(a)
pdf("figure2A_4.pdf")
plot(a$P4N,a$P4T,col=c("green","black","red")[a$Regulation],lty=2,pch=20,cex=0.3,xlab="Log2(normalized density of 4N)",ylab="Log2(normalized density of 4T)")
dev.off()

a<-read.table("figure2A_5.txt",head=T,row.names=1)
dim(a)
pdf("figure2A_5.pdf")
plot(a$P5N,a$P5T,col=c("green","black","red")[a$Regulation],lty=2,pch=20,cex=0.3,xlab="Log2(normalized density of 5N)",ylab="Log2(normalized density of 5T)")
dev.off()

a<-read.table("figure2A_8.txt",head=T,row.names=1)
dim(a)
pdf("figure2A_8.pdf")
plot(a$P8N,a$P8T,col=c("green","black","red")[a$Regulation],lty=2,pch=20,cex=0.3,xlab="Log2(normalized density of 8N)",ylab="Log2(normalized density of 8T)")
dev.off()

a<-read.table("figure2A_9.txt",head=T,row.names=1)
dim(a)
pdf("figure2A_9.pdf")
plot(a$P9N,a$P9T,col=c("green","black","red")[a$Regulation],lty=2,pch=20,cex=0.3,xlab="Log2(normalized density of 9N)",ylab="Log2(normalized density of 9T)")
dev.off()

a<-read.table("figure2A_10.txt",head=T,row.names=1)
dim(a)
pdf("figure2A_10.pdf")
plot(a$P10N,a$P10T,col=c("green","black","red")[a$Regulation],lty=2,pch=20,cex=0.3,xlab="Log2(normalized density of 10N)",ylab="Log2(normalized density of 10T)")
dev.off()

a<-read.table("figure2A_16.txt",head=T,row.names=1)
dim(a)
pdf("figure2A_16.pdf")
plot(a$P16N,a$P16T,col=c("green","black","red")[a$Regulation],lty=2,pch=20,cex=0.3,xlab="Log2(normalized density of 16N)",ylab="Log2(normalized density of 16T)")
dev.off()

a<-read.table("figure2A_17.txt",head=T,row.names=1)
dim(a)
pdf("figure2A_17.pdf")
plot(a$P17N,a$P17T,col=c("green","black","red")[a$Regulation],lty=2,pch=20,cex=0.3,xlab="Log2(normalized density of 17N)",ylab="Log2(normalized density of 17T)")
dev.off()

> plot(a$P1N,a$P1T)
错误于plot.window(...) : 'xlim'值不能是无薜17
此外: 警告信息：
1: In min(x) : min里所有的参数都不存在; 回覆Inf
2: In max(x) : max里所有的参数都不存在；回覆-Inf
3: In min(x) : min里所有的参数都不存在; 回覆Inf
4: In max(x) : max里所有的参数都不存在；回覆-Inf
总是出现以上下标出界错误，可能是对数据进行了排序，所以对名字排序后让数字随即既可以不错了。

data <- iris
plot(data$Sepal.Length, data$Sepal.Width, col=data$Species)

maybe library(ggplot2); qplot(Sepal.Length, Sepal.Width, data=iris, colour=Species) would be helpful
plot(data$Sepal.Length, data$Sepal.Width, col=data$Species)
legend(7,4.3,unique(data$Species),col=1:length(data$Species),pch=1)
legend('topright', legend = levels(iris$Species), col = 1:3, cex = 0.8, pch = 1)


should do it for you. But I prefer ggplot2 and would suggest that for better graphics in R.
Call "levels" instead of "unique" to get the possible values from the factor
legend('topright', legend = levels(iris$Species), col = 1:3, cex = 0.8, pch = 1)

#指定颜色
require(ggplot2)
data(diamonds)
qplot(carat, price, data = diamonds, colour = color)
# example taken from Hadley's ggplot2 book
#
 22
down vote
favorite
10
	

In a dataset, I want to take two attributes and create supervised scatter plot. Does anyone know how to give different color to each class ?

I am trying to use col == c("red","blue","yellow") in the plot command but not sure if it is right as if I include one more color, that color also comes in the scatter plot even though I have only 3 classes.

Here is a solution using traditional graphics (and Dirk's data):

> DF <- data.frame(x=1:10, y=rnorm(10)+5, z=sample(letters[1:3], 10, replace=TRUE)) 
> DF
    x        y z
1   1 6.628380 c
2   2 6.403279 b
3   3 6.708716 a
4   4 7.011677 c
5   5 6.363794 a
6   6 5.912945 b
7   7 2.996335 a
8   8 5.242786 c
9   9 4.455582 c
10 10 4.362427 a
> attach(DF); plot(x, y, col=c("red","blue","green")[z]); detach(DF)

This relies on the fact that DF$z is a factor, so when subsetting by it, its values will be treated as integers. So the elements of the color vector will vary with z as follows:

> c("red","blue","green")[DF$z]
 [1] "green" "blue"  "red"   "green" "red"   "blue"  "red"   "green" "green" "red"    

You can add a legend using the legend function:

legend(x="topright", legend = levels(DF$z), col=c("red","blue","green"), pch=1)

shareimprove this answer
	
edited Apr 14 '15 at 20:24
	
answered Sep 19 '11 at 15:37
Aniko
9,4182937
	
1 	 
	
How do you add a legend using the legend function? C tommy.carstensen Apr 14 '15 at 11:18
2 	 
	
@tommy.carstensen I have added a legend example C Aniko Apr 14 '15 at 20:25
   	 
	
And if you have many groups and don't want to specify each color, try using the grDevices colorRampPalette function.


#对表达量进行标记
fc<-read.table("host_gene_exp.txt",head=T,row.names=1,sep="\t")
for (i in 1:10){
    cutoff<-as.numeric(fc[,i])
    indexi<-which(cutoff <1 & cutoff >-1)
    Timei <- rep(0, length(cutoff))
    fc[indexi,i]<-as.numeric(Timei[indexi])
}
write.table(fc,"lane7_plate1_fc_by_minor1to1.txt")

for (i in 1:10){
    cutoff<-as.numeric(fc[,i])
    indexi<-which(cutoff>=1)
    Timei <- rep(10, length(cutoff))
    fc[indexi,i]<-as.numeric(Timei[indexi])
}
write.table(fc,"total_above1.txt")

for (i in 1:10){
    cutoff<-as.numeric(fc[,i])
    indexi<-which(cutoff<=-1)
    Timei <- rep(-10, length(cutoff))
    fc[indexi,i]<-as.numeric(Timei[indexi])
}
write.table(fc,"mrna_express_tags.txt")


#莫名其妙多出来一行，所以
fc<-read.table("figure2B.txt",head=T,row.names=1,sep="\t")
a<-matrix(rep(1:20,4), nrow = 4)
for (i in 1:20){
    a[,i]<-table(fc[,i])
}

colnames(a) <- colnames(fc)
write.table(a,"count_the_detected_numbers_counted.txt")

#很重要的输出，不然会excel打不开sep="\t"
write.table(a,"10000compounds_new_filtered_DEGs_new_order_siRNA_for_heatmap.txt",sep="\t")


#单独过滤绝对值的pcc，实在打不开
a<-read.table("pcc_absolute.txt",head=T,row.names=1,sep="\t")
dim(a)
mrna<-read.table("/Share/home/wangdong/lss/project/cir/new_DEGs/DEGs_pick/mRNA_ratio.txt",head=T,row.names=1,sep="\t")
dim(mrna)
dim(a)
total_t<-a
number<-rep(0,31860)
for (i in 1:31860) {
   m<-table(total_t[i,])
   mz<-m['0']
   number[i]<-mz
}
total_cir3<-cbind(total_t,number)
new_cir3<-subset(total_cir3,number<3918)
dim(new_cir3)
write.table(new_cir3,"pcc_absolute_100.txt",sep="\t")




#cirRNA_distribution CLR, absolute, desity/ditribution
mydata <-read.table("absolute_associate.txt",head=T)
pdf("absolute_associate_density.pdf")
d<-density(mydata$million) 
plot(d)
polygon(d,col="red",border="blue")
rug(mydata$million,col="brown")
dev.off()
pdf("absolute_associate.pdf")
hist(mydata$million,freq=FALSE,breaks=200,col="red",xlab="Number of associated circRNAs",main="Distribution of associated circRNAs for each coding gene")
rug(jitter(mydata$million),col="brown")
lines(density(mydata$million),col="blue",lwd=2)
dev.off()


DAVID看这个就可以了
Functional Annotation Chart Report

#absolute_DE
library(pheatmap)

data<-read.table("z_soce_3.txt",head=T,row.names=1,sep='\t')
data<-data.matrix(data)
pdf("z_soce_3.pdf",height=5,width=4)
pheatmap(data,cluster_row=TRUE,cluster_col=TRUE,show_rownames=F,color = colorRampPalette(c("green", "black", "red"))(50),cellwidth=10,cellheight=0.05)
dev.off()

#CLR_DE
library(pheatmap)

data<-read.table("DEGs_final2_modified_3.txt",head=T,row.names=1,sep='\t')
data<-data.matrix(data)
pdf("DEGs_final2_modified_3.pdf",height=4,width=3)
pheatmap(data,cluster_row=TRUE,cluster_col=TRUE,show_rownames=F,color = colorRampPalette(c("blue", "white", "red"))(50),cellwidth=10,cellheight=0.05)
dev.off()


library(pheatmap)

data<-read.table("DEGs_final.txt",head=T,row.names=1,sep='\t')
data<-data.matrix(data)
pdf("DE_circRNA.pdf")
pheatmap(data,cluster_row=TRUE,cluster_col=TRUE,show_rownames=F,show_colnames=TRUE,color = colorRampPalette(c("blue", "white", "red"))(50),cellwidth=4,cellheight=0.4)
dev.off()

fc<-read.table("z_soce.txt",head=T,row.names=1)
for (i in 1:20){
    cutoff<-as.numeric(fc[,i])
    indexi<-which(cutoff>=3)
    Timei <- rep(3, length(cutoff))
    fc[indexi,i]<-as.numeric(Timei[indexi])
}
write.table(fc,"total_above1.txt")

for (i in 1:20){
    cutoff<-as.numeric(fc[,i])
    indexi<-which(cutoff<=-3)
    Timei <- rep(-3, length(cutoff))
    fc[indexi,i]<-as.numeric(Timei[indexi])
}
write.table(fc,"z_soce_3.txt")


#scatterplot
a<-read.table("scatter_average2.txt",head=T,sep="\t")
pdf("scatter_average_cutoff2.pdf")
plot(a$linear,a$circRNA,col=as.factor(a$factor),lty=2,pch=20,cex=0.5,xlab="Log2(linear-RNA-T/ linear-RNA-N)",ylab="Log2(circRNA-T/ circRNA-N)")

circRNA作图
mydata <-read.table("for_density_plot.txt",head=T)  #记得取log10图才会好看
pdf("density.pdf")
d<-density(mydata$million) 
plot(d)
polygon(d,col="red",border="blue")
rug(mydata$million,col="brown")
dev.off()


mydata <-read.table("for_density_plot.txt",head=T)
hist(a$million,breaks=200,col="red")


密度的
pdf("hist_density2.pdf")
hist(a$million,freq=FALSE,breaks=200,col="red",xlab="Number of associated circRNAs",main="Distribution of associated circRNAs for each coding gene")
rug(jitter(a$million))
lines(density(a$million),col="blue",lwd=2)
dev.off()
频率的
pdf("freq2.pdf")
x<-a$million
h<-hist(x,,breaks=200,col="red",xlab="Number of associated circRNAs",ylab="Number of genes",main="Distribution of associated circRNAs for each coding gene")
xfit<-seq(min(x),max(x),length=40)
yfit<-dnorm(xfit,mean=mean(x),sd=sd(x))
yfit<-yfit*diff(h$mids[1:2])*length(x)
lines(xfit,yfit,col="blue",lwd=2)
box()
dev.off()


ggplot 做相关性
GQ<-read.table(file="invivo_proliferation.txt",head=T,row.names=1,sep='\t')
GQ_log<-log2(GQ[1:10211,1:11])
library(reshape2)
cormat<-round(cor(GQ_log),2)
melted_cormat <- melt(cormat)
library(ggplot2)

get_upper_tri <- function(cormat){
    cormat[lower.tri(cormat)]<- NA
    return(cormat)
  }
upper_tri <- get_upper_tri(cormat)
melted_cormat <- melt(upper_tri)
melted_cormat <- na.omit(melted_cormat)

ggheatmap<-ggplot(data = melted_cormat, aes(Var2, Var1, fill = value))+
geom_tile(color = "white")+
scale_fill_gradient2(low = "green", high = "red", mid = "white", 
midpoint = 0.5, limit = c(0,1), name="Pearson\nCorrelation") +
theme_minimal()+ 
theme(axis.text.x = element_text(angle = 45, vjust = 1, 
size = 12, hjust = 1))+
coord_fixed()

pdf(file="WHL_correlation.pdf")
ggheatmap+geom_text(aes(Var2, Var1, label = value), color = "black", size = 4) +
theme(
  axis.title.x = element_blank(),
  axis.title.y = element_blank(),
  panel.grid.major = element_blank(),
  panel.border = element_blank(),
  panel.background = element_blank(),
  axis.ticks = element_blank(),
  legend.justification = c(1, 0),
  legend.position = c(0.6, 0.7),
  legend.direction = "horizontal")+
  guides(fill = guide_colorbar(barwidth = 7, barheight = 1,
                title.position = "top", title.hjust = 0.5))
dev.off()
#将3／4合并到一起
combine_filtered_pcc_rename_rows_get_number_finakl_pcc.r

v1<-as.matrix(read.table("one_2.txt"))
cir1<-v1[,1:31860]
mrna<-read.table("/Share/home/wangdong/lss/project/cir/split_dir/cir1/selected_mRNA.txt",head=T,row.names=1,sep="\t")
t<-t(cir1)
dim(mrna)
dim(t)
rownames(t) <- rownames(mrna)
total_t<-t
number<-rep(0,31860)
maxi<-rep(0,31860)
mini<-rep(0,31860)
for (i in 1:31860) {
   m<-table(total_t[i,])
   mz<-m['0']
   number[i]<-mz
   maxi[i]<-max(total_t[i,])
   mini[i]<-min(total_t[i,])
}


sed '/V1/'d one_20.txt >> new_final.txt #将行去掉了


number<-rep(0,3)
maxi<-rep(0,3)
mini<-rep(0,3)
for (i in 1:3) {
   m<-table(x[i,])
   mz<-m['0']
   number[i]<-mz
   maxi[i]<-max(x[i,])
   mini[i]<-min(x[i,])
}
x<-cbind(x,number,maxi,mini)
new<-subset(x,number>1 | maxi >=4 | mini >=-0.9)
print (x)
> dim(new_cir1)
[1]  4152 31863
＃4. 统计出个数，利用基因进行过滤，实际上是为了统计个数
filter_filtered_pcc_with_gene_regulated_numbers10.r

total1<-as.matrix(read.table("/Share/home/wangdong/lss/project/cir/new_associate/new_cir2.txt"))
cir1<-total1[,1:31860]
mrna<-read.table("/Share/home/wangdong/lss/project/cir/new_associate/all_expression_signal_mRNA_edit.txt",head=T,row.names=1,sep="\t")
t<-t(cir1)
dim(mrna)
dim(t)
rownames(t) <- rownames(mrna)
total_t<-t
number<-rep(0,31860)
for (i in 1:31860) {
   m<-table(total_t[i,])
   mz<-m['0']
   number[i]<-mz
}
write.table(number,"abusolute_number.txt")
total_cir3<-cbind(total_t,number)
new_cir3<-subset(total_cir3,number<17143)
dim(new_cir3)
write.table(new_cir3,"pcc_absolute.txt",sep="\t")

＃3. 利用rowsums进行过滤，将同一个文件44个文件合并成一个，去除只有一个相互作用的 merge.r； merge_filter_pcc.r
cir<-read.table("cir1.txt",row.names=1)
v1<-as.matrix(read.table("result_1.txt"))
v2<-as.matrix(read.table("result_2.txt"))
v3<-as.matrix(read.table("result_3.txt"))
v4<-as.matrix(read.table("result_4.txt"))
v5<-as.matrix(read.table("result_5.txt"))
v6<-as.matrix(read.table("result_6.txt"))
v7<-as.matrix(read.table("result_7.txt"))
v8<-as.matrix(read.table("result_8.txt"))
v9<-as.matrix(read.table("result_9.txt"))
v10<-as.matrix(read.table("result_10.txt"))
v11<-as.matrix(read.table("result_11.txt"))
v12<-as.matrix(read.table("result_12.txt"))
v13<-as.matrix(read.table("result_13.txt"))
v14<-as.matrix(read.table("result_14.txt"))
v15<-as.matrix(read.table("result_15.txt"))
v16<-as.matrix(read.table("result_16.txt"))
v17<-as.matrix(read.table("result_17.txt"))
v18<-as.matrix(read.table("result_18.txt"))
v19<-as.matrix(read.table("result_19.txt"))
v20<-as.matrix(read.table("result_20.txt"))
v21<-as.matrix(read.table("result_21.txt"))
v22<-as.matrix(read.table("result_22.txt"))
v23<-as.matrix(read.table("result_23.txt"))
v24<-as.matrix(read.table("result_24.txt"))
v25<-as.matrix(read.table("result_25.txt"))
v26<-as.matrix(read.table("result_26.txt"))
v27<-as.matrix(read.table("result_27.txt"))
v28<-as.matrix(read.table("result_28.txt"))
v29<-as.matrix(read.table("result_29.txt"))
v30<-as.matrix(read.table("result_30.txt"))
v31<-as.matrix(read.table("result_31.txt"))
v32<-as.matrix(read.table("result_32.txt"))
v33<-as.matrix(read.table("result_33.txt"))
v34<-as.matrix(read.table("result_34.txt"))
v35<-as.matrix(read.table("result_35.txt"))
v36<-as.matrix(read.table("result_36.txt"))
v37<-as.matrix(read.table("result_37.txt"))
v38<-as.matrix(read.table("result_38.txt"))
v39<-as.matrix(read.table("result_39.txt"))
v40<-as.matrix(read.table("result_40.txt"))
v41<-as.matrix(read.table("result_41.txt"))
v42<-as.matrix(read.table("result_42.txt"))
v43<-as.matrix(read.table("result_43.txt"))
v44<-as.matrix(read.table("result_44.txt"))
total<-rbind(v1,v2,v3,v4,v5,v6,v7,v8,v9,v10,v11,v12,v13,v14,v15,v16,v17,v18,v19,v20,v21,v22,v23,v24,v25,v26,v27,v28,v29,v30,v31,v32,v33,v34,v35,v36,v37,v38,v39,v40,v41,v42,v43,v44)
rownames(total) <- rownames(cir)
number<-rep(0,8880)
maxi<-rep(0,8880)
mini<-rep(0,8880)
for (i in 1:8880) {
   m<-table(total[i,])
   mz<-m['0']
   number[i]<-mz
   maxi[i]<-max(total[i,])
   mini[i]<-min(total[i,])
}
total<-cbind(total,number,maxi,mini)
new<-subset(total,number<31859 | maxi >=0.9 | mini <=-0.9)
dim(new)
> dim(new_cir1)
[1]  4152 31863
> dim(new_cir2)
[1]  3079 31863
> dim(new_cir3)
[1]  5119 31863

cir<-read.table("cir2.txt",row.names=1)
v1<-as.matrix(read.table("result_1.txt"))
v2<-as.matrix(read.table("result_2.txt"))
v3<-as.matrix(read.table("result_3.txt"))
v4<-as.matrix(read.table("result_4.txt"))
v5<-as.matrix(read.table("result_5.txt"))
v6<-as.matrix(read.table("result_6.txt"))
v7<-as.matrix(read.table("result_7.txt"))
v8<-as.matrix(read.table("result_8.txt"))
v9<-as.matrix(read.table("result_9.txt"))
v10<-as.matrix(read.table("result_10.txt"))
v11<-as.matrix(read.table("result_11.txt"))
v12<-as.matrix(read.table("result_12.txt"))
v13<-as.matrix(read.table("result_13.txt"))
v14<-as.matrix(read.table("result_14.txt"))
v15<-as.matrix(read.table("result_15.txt"))
v16<-as.matrix(read.table("result_16.txt"))
v17<-as.matrix(read.table("result_17.txt"))
v18<-as.matrix(read.table("result_18.txt"))
v19<-as.matrix(read.table("result_19.txt"))
v20<-as.matrix(read.table("result_20.txt"))
v21<-as.matrix(read.table("result_21.txt"))
v22<-as.matrix(read.table("result_22.txt"))
v23<-as.matrix(read.table("result_23.txt"))
v24<-as.matrix(read.table("result_24.txt"))
v25<-as.matrix(read.table("result_25.txt"))
v26<-as.matrix(read.table("result_26.txt"))
v27<-as.matrix(read.table("result_27.txt"))
v28<-as.matrix(read.table("result_28.txt"))
v29<-as.matrix(read.table("result_29.txt"))
v30<-as.matrix(read.table("result_30.txt"))
v31<-as.matrix(read.table("result_31.txt"))
v32<-as.matrix(read.table("result_32.txt"))
v33<-as.matrix(read.table("result_33.txt"))
v34<-as.matrix(read.table("result_34.txt"))
v35<-as.matrix(read.table("result_35.txt"))
v36<-as.matrix(read.table("result_36.txt"))
v37<-as.matrix(read.table("result_37.txt"))
v38<-as.matrix(read.table("result_38.txt"))
v39<-as.matrix(read.table("result_39.txt"))
v40<-as.matrix(read.table("result_40.txt"))
v41<-as.matrix(read.table("result_41.txt"))
v42<-as.matrix(read.table("result_42.txt"))
v43<-as.matrix(read.table("result_43.txt"))
v44<-as.matrix(read.table("result_44.txt"))
total<-rbind(v1,v2,v3,v4,v5,v6,v7,v8,v9,v10,v11,v12,v13,v14,v15,v16,v17,v18,v19,v20,v21,v22,v23,v24,v25,v26,v27,v28,v29,v30,v31,v32,v33,v34,v35,v36,v37,v38,v39,v40,v41,v42,v43,v44)
dim(total)
dim(cir)
rownames(total) <- rownames(cir)
dim(total)
number<-rep(0,8800)
maxi<-rep(0,8800)
mini<-rep(0,8800)
for (i in 1:8800) {
   m<-table(total[i,])
   mz<-m['0']
   number[i]<-mz
   maxi[i]<-max(total[i,])
   mini[i]<-min(total[i,])
}
total_cir1<-cbind(total,number,maxi,mini)
new_cir1<-subset(total_cir1,number<31859 | maxi >=0.9 | mini <=-0.9)
dim(new_cir1)
write.table(new_cir1,"new_cir2.txt",sep="\t")

keep = rowSums(total)!=0
total = total[keep,]
t<-t(total) ＃把所有的merge到一起再去掉整个基因
mrna<-read.table("/Share/home/wangdong/lss/project/cir/split_dir/cir1/selected_mRNA.txt",head=T,row.names=1,sep="\t")
dim(t)
dim(mrna)
rownames(t) <- rownames(mrna)
keep = rowSums(t)!=0
t = t[keep,]
dim(t)
t<-replace(t,abs(t)<0.9,0)
a = rowSums(t)!=0
b = colSums(t)!=0
t = t[keep,]
t = t[keep,]
write.table(t,"cir20_total_merged_result_rm_row_col.txt",sep="\t")


number<-rep(0,8780)
maxi<-rep(0,8780)
mini<-rep(0,8780)
for (i in 1:8780) {
   m<-table(total[i,])
   mz<-m['0']
   number[i]<-mz
   maxi[i]<-max(total[i,])
   mini[i]<-min(total[i,])
}
total<-cbind(total,number,maxi,mini)
new<-subset(total,number>1 | maxi >=4 | mini >=-0.9)


R语言中，或且非，分别是：|，&，！
options(digits=3)
b1=as.vector(b)
#r中有用的统计函数
options(digits=3)
a=c(20,23,20,24,23,21,22,25,26,20,21,21,22,22,23,22,22,24,25,21,22,21,24,23)
b=table(a)
b1=as.vector(b) #将统计结果等有抬头的转化为
c=prop.table(table(a)) #计算频率
c1=as.vector(c)
x=data.frame(b1,c1) #将感兴趣向量合成数据框
dimnames(x)=list(c('20','21','22','23','24','25','26'),c("频数","频率")) 逗号前的向量可以做行名，逗号后的可以做列名


for(i in 1:ncol(y)){
table(y[,i])
}
> m
a
1 3 5 8 
1 1 1 2 
> m[1]
1 
1

> m['8']
8 
2 
> m['3']
3 
1 
> m['3']=1
> m['3']==1
   3 
TRUE 
> m['8']==1
    8 
FALSE

> a
 [1] 0 0 0 0 0 0 9 9 6 5 5 5 4 4 4 4 0 0 0
> m<-table(a)
> m
a
0 4 5 6 9 
9 4 3 1 2 
> m['0']==9
   0 
TRUE 
> m['0']==8
    0 
FALSE 
> m['0']>=8  #就这么来统计个数进行过滤
   0 
TRUE 
> a
     [,1] [,2] [,3]
[1,]    1    5    9
[2,]    2    6   10
[3,]    3    7   11
[4,]    4    8   12
 m<-table(a[,1])
> m

1 2 3 4 
1 1 1 1 
> m['1']==1
   1 
TRUE 
> m['1']==2
    1 
FALSE

用subset提取出来
> x<-data.frame(matrix(1:30,nrow=5,byrow=T))
> rownames(x)=c("one","two","three","four","five")
> colnames(x)=c("a","b","c","d","e","f")
> x
       a  b  c  d  e  f
one    1  2  3  4  5  6
two    7  8  9 10 11 12
three 13 14 15 16 17 18
four  19 20 21 22 23 24
five  25 26 27 28 29 30
> new<-subset(x,a>=14,select=a:f)


#进行合并，过滤等

wc -l 1.txt >> result_count.fa 
wc -l 2.txt >> result_count.fa 
wc -l 3.txt >> result_count.fa 
wc -l 4.txt >> result_count.fa 
wc -l 5.txt >> result_count.fa 
wc -l 6.txt >> result_count.fa 
wc -l 7.txt >> result_count.fa 
wc -l 8.txt >> result_count.fa 
wc -l 9.txt >> result_count.fa 
wc -l 10.txt >> result_count.fa
wc -l 11.txt >> result_count.fa
wc -l 12.txt >> result_count.fa
wc -l 13.txt >> result_count.fa
wc -l 14.txt >> result_count.fa
wc -l 15.txt >> result_count.fa
wc -l 16.txt >> result_count.fa
wc -l 17.txt >> result_count.fa
wc -l 18.txt >> result_count.fa
wc -l 19.txt >> result_count.fa
wc -l 20.txt >> result_count.fa
wc -l 21.txt >> result_count.fa
wc -l 22.txt >> result_count.fa
wc -l 23.txt >> result_count.fa
wc -l 24.txt >> result_count.fa
wc -l 25.txt >> result_count.fa
wc -l 26.txt >> result_count.fa
wc -l 27.txt >> result_count.fa
wc -l 28.txt >> result_count.fa
wc -l 29.txt >> result_count.fa
wc -l 30.txt >> result_count.fa
wc -l 31.txt >> result_count.fa
wc -l 32.txt >> result_count.fa
wc -l 33.txt >> result_count.fa
wc -l 34.txt >> result_count.fa
wc -l 35.txt >> result_count.fa
wc -l 36.txt >> result_count.fa
wc -l 37.txt >> result_count.fa
wc -l 38.txt >> result_count.fa
wc -l 39.txt >> result_count.fa
wc -l 40.txt >> result_count.fa
wc -l 41.txt >> result_count.fa
wc -l 42.txt >> result_count.fa
wc -l 43.txt >> result_count.fa
wc -l 44.txt >> result_count.fa

＃2，以0.8为cutoff，分开运行，只能运行15个的话
a<-as.matrix(read.table("/Share/home/wangdong/lss/project/cir/split_dir/cir8/correlation_xaa.txt"))
a<-replace(a,abs(a)<0.8,0)
write.table(a,"/Share/home/wangdong/lss/project/cir/split_dir/cir8/1.txt")
a<-as.matrix(read.table("/Share/home/wangdong/lss/project/cir/split_dir/cir8/correlation_xab.txt"))
a<-replace(a,abs(a)<0.8,0)
write.table(a,"/Share/home/wangdong/lss/project/cir/split_dir/cir8/2.txt")
a<-as.matrix(read.table("/Share/home/wangdong/lss/project/cir/split_dir/cir8/correlation_xac.txt"))
a<-replace(a,abs(a)<0.8,0)
write.table(a,"/Share/home/wangdong/lss/project/cir/split_dir/cir8/3.txt")
a<-as.matrix(read.table("/Share/home/wangdong/lss/project/cir/split_dir/cir8/correlation_xad.txt"))
a<-replace(a,abs(a)<0.8,0)
write.table(a,"/Share/home/wangdong/lss/project/cir/split_dir/cir8/4.txt")
a<-as.matrix(read.table("/Share/home/wangdong/lss/project/cir/split_dir/cir8/correlation_xae.txt"))
a<-replace(a,abs(a)<0.8,0)
write.table(a,"/Share/home/wangdong/lss/project/cir/split_dir/cir8/5.txt")
a<-as.matrix(read.table("/Share/home/wangdong/lss/project/cir/split_dir/cir8/correlation_xaf.txt"))
a<-replace(a,abs(a)<0.8,0)
write.table(a,"/Share/home/wangdong/lss/project/cir/split_dir/cir8/6.txt")
a<-as.matrix(read.table("/Share/home/wangdong/lss/project/cir/split_dir/cir8/correlation_xag.txt"))
a<-replace(a,abs(a)<0.8,0)
write.table(a,"/Share/home/wangdong/lss/project/cir/split_dir/cir8/7.txt")
a<-as.matrix(read.table("/Share/home/wangdong/lss/project/cir/split_dir/cir8/correlation_xah.txt"))
a<-replace(a,abs(a)<0.8,0)
write.table(a,"/Share/home/wangdong/lss/project/cir/split_dir/cir8/8.txt")
a<-as.matrix(read.table("/Share/home/wangdong/lss/project/cir/split_dir/cir8/correlation_xai.txt"))
a<-replace(a,abs(a)<0.8,0)
write.table(a,"/Share/home/wangdong/lss/project/cir/split_dir/cir8/9.txt")
a<-as.matrix(read.table("/Share/home/wangdong/lss/project/cir/split_dir/cir8/correlation_xaj.txt"))
a<-replace(a,abs(a)<0.8,0)
write.table(a,"/Share/home/wangdong/lss/project/cir/split_dir/cir8/10.txt")
a<-as.matrix(read.table("/Share/home/wangdong/lss/project/cir/split_dir/cir8/correlation_xak.txt"))
a<-replace(a,abs(a)<0.8,0)
write.table(a,"/Share/home/wangdong/lss/project/cir/split_dir/cir8/11.txt")
a<-as.matrix(read.table("/Share/home/wangdong/lss/project/cir/split_dir/cir8/correlation_xal.txt"))
a<-replace(a,abs(a)<0.8,0)
write.table(a,"/Share/home/wangdong/lss/project/cir/split_dir/cir8/12.txt")
a<-as.matrix(read.table("/Share/home/wangdong/lss/project/cir/split_dir/cir8/correlation_xam.txt"))
a<-replace(a,abs(a)<0.8,0)
write.table(a,"/Share/home/wangdong/lss/project/cir/split_dir/cir8/13.txt")
a<-as.matrix(read.table("/Share/home/wangdong/lss/project/cir/split_dir/cir8/correlation_xan.txt"))
a<-replace(a,abs(a)<0.8,0)
write.table(a,"/Share/home/wangdong/lss/project/cir/split_dir/cir8/14.txt")
a<-as.matrix(read.table("/Share/home/wangdong/lss/project/cir/split_dir/cir8/correlation_xao.txt"))
a<-replace(a,abs(a)<0.8,0)
write.table(a,"/Share/home/wangdong/lss/project/cir/split_dir/cir8/15.txt")
a<-as.matrix(read.table("/Share/home/wangdong/lss/project/cir/split_dir/cir8/correlation_xap.txt"))
a<-replace(a,abs(a)<0.8,0)
write.table(a,"/Share/home/wangdong/lss/project/cir/split_dir/cir8/16.txt")
a<-as.matrix(read.table("/Share/home/wangdong/lss/project/cir/split_dir/cir8/correlation_xaq.txt"))
a<-replace(a,abs(a)<0.8,0)
write.table(a,"/Share/home/wangdong/lss/project/cir/split_dir/cir8/17.txt")
a<-as.matrix(read.table("/Share/home/wangdong/lss/project/cir/split_dir/cir8/correlation_xar.txt"))
a<-replace(a,abs(a)<0.8,0)
write.table(a,"/Share/home/wangdong/lss/project/cir/split_dir/cir8/18.txt")
a<-as.matrix(read.table("/Share/home/wangdong/lss/project/cir/split_dir/cir8/correlation_xas.txt"))
a<-replace(a,abs(a)<0.8,0)
write.table(a,"/Share/home/wangdong/lss/project/cir/split_dir/cir8/19.txt")
a<-as.matrix(read.table("/Share/home/wangdong/lss/project/cir/split_dir/cir8/correlation_xat.txt"))
a<-replace(a,abs(a)<0.8,0)
write.table(a,"/Share/home/wangdong/lss/project/cir/split_dir/cir8/20.txt")
a<-as.matrix(read.table("/Share/home/wangdong/lss/project/cir/split_dir/cir8/correlation_xau.txt"))
a<-replace(a,abs(a)<0.8,0)
write.table(a,"/Share/home/wangdong/lss/project/cir/split_dir/cir8/21.txt")
a<-as.matrix(read.table("/Share/home/wangdong/lss/project/cir/split_dir/cir8/correlation_xav.txt"))
a<-replace(a,abs(a)<0.8,0)
write.table(a,"/Share/home/wangdong/lss/project/cir/split_dir/cir8/22.txt")
a<-as.matrix(read.table("/Share/home/wangdong/lss/project/cir/split_dir/cir8/correlation_xaw.txt"))
a<-replace(a,abs(a)<0.8,0)
write.table(a,"/Share/home/wangdong/lss/project/cir/split_dir/cir8/23.txt")
a<-as.matrix(read.table("/Share/home/wangdong/lss/project/cir/split_dir/cir8/correlation_xax.txt"))
a<-replace(a,abs(a)<0.8,0)
write.table(a,"/Share/home/wangdong/lss/project/cir/split_dir/cir8/24.txt")
a<-as.matrix(read.table("/Share/home/wangdong/lss/project/cir/split_dir/cir8/correlation_xay.txt"))
a<-replace(a,abs(a)<0.8,0)
write.table(a,"/Share/home/wangdong/lss/project/cir/split_dir/cir8/25.txt")
a<-as.matrix(read.table("/Share/home/wangdong/lss/project/cir/split_dir/cir8/correlation_xaz.txt"))
a<-replace(a,abs(a)<0.8,0)
write.table(a,"/Share/home/wangdong/lss/project/cir/split_dir/cir8/26.txt")
a<-as.matrix(read.table("/Share/home/wangdong/lss/project/cir/split_dir/cir8/correlation_xba.txt"))
a<-replace(a,abs(a)<0.8,0)
write.table(a,"/Share/home/wangdong/lss/project/cir/split_dir/cir8/27.txt")
a<-as.matrix(read.table("/Share/home/wangdong/lss/project/cir/split_dir/cir8/correlation_xbb.txt"))
a<-replace(a,abs(a)<0.8,0)
write.table(a,"/Share/home/wangdong/lss/project/cir/split_dir/cir8/28.txt")
a<-as.matrix(read.table("/Share/home/wangdong/lss/project/cir/split_dir/cir8/correlation_xbc.txt"))
a<-replace(a,abs(a)<0.8,0)
write.table(a,"/Share/home/wangdong/lss/project/cir/split_dir/cir8/29.txt")
a<-as.matrix(read.table("/Share/home/wangdong/lss/project/cir/split_dir/cir8/correlation_xbd.txt"))
a<-replace(a,abs(a)<0.8,0)
write.table(a,"/Share/home/wangdong/lss/project/cir/split_dir/cir8/30.txt")
a<-as.matrix(read.table("/Share/home/wangdong/lss/project/cir/split_dir/cir8/correlation_xbe.txt"))
a<-replace(a,abs(a)<0.8,0)
write.table(a,"/Share/home/wangdong/lss/project/cir/split_dir/cir8/31.txt")
a<-as.matrix(read.table("/Share/home/wangdong/lss/project/cir/split_dir/cir8/correlation_xbf.txt"))
a<-replace(a,abs(a)<0.8,0)
write.table(a,"/Share/home/wangdong/lss/project/cir/split_dir/cir8/32.txt")
a<-as.matrix(read.table("/Share/home/wangdong/lss/project/cir/split_dir/cir8/correlation_xbg.txt"))
a<-replace(a,abs(a)<0.8,0)
write.table(a,"/Share/home/wangdong/lss/project/cir/split_dir/cir8/33.txt")
a<-as.matrix(read.table("/Share/home/wangdong/lss/project/cir/split_dir/cir8/correlation_xbh.txt"))
a<-replace(a,abs(a)<0.8,0)
write.table(a,"/Share/home/wangdong/lss/project/cir/split_dir/cir8/34.txt")
a<-as.matrix(read.table("/Share/home/wangdong/lss/project/cir/split_dir/cir8/correlation_xbi.txt"))
a<-replace(a,abs(a)<0.8,0)
write.table(a,"/Share/home/wangdong/lss/project/cir/split_dir/cir8/35.txt")
a<-as.matrix(read.table("/Share/home/wangdong/lss/project/cir/split_dir/cir8/correlation_xbj.txt"))
a<-replace(a,abs(a)<0.8,0)
write.table(a,"/Share/home/wangdong/lss/project/cir/split_dir/cir8/36.txt")
a<-as.matrix(read.table("/Share/home/wangdong/lss/project/cir/split_dir/cir8/correlation_xbk.txt"))
a<-replace(a,abs(a)<0.8,0)
write.table(a,"/Share/home/wangdong/lss/project/cir/split_dir/cir8/37.txt")
a<-as.matrix(read.table("/Share/home/wangdong/lss/project/cir/split_dir/cir8/correlation_xbl.txt"))
a<-replace(a,abs(a)<0.8,0)
write.table(a,"/Share/home/wangdong/lss/project/cir/split_dir/cir8/38.txt")
a<-as.matrix(read.table("/Share/home/wangdong/lss/project/cir/split_dir/cir8/correlation_xbm.txt"))
a<-replace(a,abs(a)<0.8,0)
write.table(a,"/Share/home/wangdong/lss/project/cir/split_dir/cir8/39.txt")
a<-as.matrix(read.table("/Share/home/wangdong/lss/project/cir/split_dir/cir8/correlation_xbn.txt"))
a<-replace(a,abs(a)<0.8,0)
write.table(a,"/Share/home/wangdong/lss/project/cir/split_dir/cir8/40.txt")
a<-as.matrix(read.table("/Share/home/wangdong/lss/project/cir/split_dir/cir8/correlation_xbo.txt"))
a<-replace(a,abs(a)<0.8,0)
write.table(a,"/Share/home/wangdong/lss/project/cir/split_dir/cir8/41.txt")
a<-as.matrix(read.table("/Share/home/wangdong/lss/project/cir/split_dir/cir8/correlation_xbp.txt"))
a<-replace(a,abs(a)<0.8,0)
write.table(a,"/Share/home/wangdong/lss/project/cir/split_dir/cir8/42.txt")
a<-as.matrix(read.table("/Share/home/wangdong/lss/project/cir/split_dir/cir8/correlation_xbq.txt"))
a<-replace(a,abs(a)<0.8,0)
write.table(a,"/Share/home/wangdong/lss/project/cir/split_dir/cir8/43.txt")
a<-as.matrix(read.table("/Share/home/wangdong/lss/project/cir/split_dir/cir8/correlation_xbr.txt"))
a<-replace(a,abs(a)<0.8,0)
write.table(a,"/Share/home/wangdong/lss/project/cir/split_dir/cir8/44.txt")

梦翻领,可缩短时间5分钟
m<-list.files("/Share/home/wangdong/lss/cir/cir1/",pattern ="correlation_x")
for (i in 1:44){
    a<-as.matrix(read.table(m[i]),head=T,row.names=1) ＃变成矩阵可缩短时间5分钟
    a<-replace(a,abs(a)<0.8,0)
    print (m[i])
    write.table(a,paste("/Share/home/wangdong/lss/cir/cir1/",i,".txt",sep=''))
}

Error in file(file, “rt”) : cannot open the connection
You need to change directory <- ("./specdata") to directory <- ("./specdata/")
#查看行名
row.names(a)

a<-read.table("1.txt",head=T,row.names=1)
b<-read.table("2.txt",head=T,row.names=1)
total1_2<-rbind(a,b)
t<-t(total1_2)
write.table(t,head=T,row.names=1,sep="\t")
write.table(t,sep="\t")
write.table(t,"ttotal_1_2.txt",sep="\t")
names(t)
str(t)
history()



> write.table(t,"ttotal_1_2.txt",sep="\t")
> names(t)
NULL
> str(t) #显示其数据结构
 num [1:31860, 1:400] 0 0 0 0 0 0 0 0 0 0 ...
 - attr(*, "dimnames")=List of 2
  ..$ : chr [1:31860] "V1" "V2" "V3" "V4" ...
  ..$ : chr [1:400] "1" "2" "3" "4" ...


#2,将0.8作为相关系数的cutoff―final，但最终只能运行15个，所以分开运行

m<-list.files("/Share/home/wangdong/lss/project/cir/split_dir/cir10",pattern ="correlation_x")
for (i in 1:length(m)){
    a<-read.table(m[i],head=T,row.names=1)
    a<-replace(a,abs(a)<0.8,0)
    keep = rowSums(a)!=0
    write.table(a,paste("/Share/home/wangdong/lss/project/cir/split_dir/cir10/",i,".txt",sep=''))
}


m<-list.files("/Share/home/wangdong/lss/project/cir/split_dir/cir1",pattern ="correlation_x")
for (i in 1:length(m)){
    a<-read.table(m[i],head=T,sep="\t")
    a<-replace(a,abs(a)<0.8,0)
    keep = rowSums(a)!=0
    write.table(a,paste("/Share/home/wangdong/lss/project/cir/split_dir/cir1/",i,".txt"))
}


最终版本
＃过滤整个矩阵，批量输入输出

m<-list.files("/Share/home/wangdong/lss/project/cir/split_dir/cir2",pattern ="correlation_x")
for (i in 1:length(m)){
    a<-read.table(m[i],head=T,row.names=1)
    a<-replace(a,abs(a)<0.8,0)
    keep = rowSums(a)!=0
    write.table(a,paste("/Share/home/wangdong/lss/project/cir/split_dir/cir2/",i,".txt"))
}
＃过滤整个矩阵，批量输入输出
m<-list.files("/Share/home/wangdong/lss/project/cir/split_dir/cir1",pattern ="correlation_x")
for (i in 1:length(m)){
    a<-read.table(m[i],head=T,sep="\t")
    a<-replace(a,abs(a)<80,0)
    keep = rowSums(a)!=0
    write.table(a,paste("/Users/lishasha/Desktop/home/new_work/20151122_compound_comfirmation/total_plot_indensity/test/",i,".txt"))
}


＃过滤整个矩阵，批量输入输出
m<-list.files("/Users/lishasha/Desktop/home/new_work/20151122_compound_comfirmation/total_plot_indensity/test")
for (i in 1:length(m)){
    a<-read.table(m[i],head=T,sep="\t")
    a<-replace(a,abs(a)<0.8,0)
    keep = rowSums(a)!=0
    write.table(a,paste("/Users/lishasha/Desktop/home/new_work/20151122_compound_comfirmation/total_plot_indensity/test/",i,".txt"))
}

楼主: 张群0703
	
2914 8
[程序问答] R语言中批量导入同一文件夹下的文件并合并 
先用list.files()得到所有的文件名，然后用循环一个一个的读入，用rbind或者merge来合并
> list.files()
 [1] "xaa.sh" "xab.sh" "xac.sh" "xad.sh" "xae.sh" "xaf.sh" "xag.sh" "xah.sh"
 [9] "xai.sh" "xaj.sh" "xak.sh" "xal.sh" "xam.sh" "xan.sh" "xao.sh" "xap.sh"
[17] "xaq.sh" "xar.sh" "xas.sh" "xat.sh" "xau.sh" "xav.sh" "xaw.sh" "xax.sh"
[25] "xay.sh" "xaz.sh" "xba.sh" "xbb.sh" "xbc.sh" "xbd.sh" "xbe.sh" "xbf.sh"
[33] "xbg.sh" "xbh.sh" "xbi.sh" "xbj.sh" "xbk.sh" "xbl.sh" "xbm.sh" "xbn.sh"
[41] "xbo.sh" "xbp.sh" "xbq.sh"
> m<-list.files()
> m[1]
[1] "xaa.sh"


X<-subset(data,stkcd==1&trdynt==2000,select=c(yopnprc))

#1.批量算pcc，分成800份
＃1.ue里面将cir13.txt去掉列抬头
＃2.将cir13.txt划分
split -l 200 cir13.txt 
＃将含有分好文件的cir13文件夹拖到服务器,计算行数对否来检查相关性的脚本
wc -l x* >> result.fa
＃修改脚本，替换掉文件夹路径,存为cir13／total.r/
total.r
＃分割cir13／r/total.r/
split -l 10 total.r 
＃将文件夹下cir13／r/所有文件替换成.r后缀将r文件夹整个拖到服务器上
for i in *  
   do mv $i $i".r"  
done  
＃将文件夹下cir13／r/所有文件替换成.r后缀
＃修改total.sh
split -l 1 total.sh 
＃将文件夹下cir13／r/sh/所有文件替换成.sh后缀将r文件夹整个拖到服务器上
for i in *  
   do mv $i $i".sh"  
done  
＃批量提交一个文件夹即可并行运算了
perl ~/lkq/scripts/qsub.pl -d ./ -o ./


1.删除所有的 .xml 后缀：
[plain] view plaincopyprint?

    rename 's/\.xml$//' *.xml  

2.给当前目录下所有文件加后缀 .xml 。

[plain] view plaincopyprint?

    for i in *  
       do mv $i $i".xml"  
    done  

#检验相关系数
cir<-read.table("circRNA_less.txt",head=T,row.names=1,sep="\t")
mrna<-read.table("mRNA_less.txt",head=T,row.names=1,sep="\t")
b<-matrix(0,429,234)
c<-b
for (i in 1:429){
 for (j in 1:234){
   b[i,j]<-cor(as.numeric(cir[i,]),as.numeric(mrna[j,]))
   m<-cir[i,]
   n<-mrna[j,]
   cortest<-cor.test(as.numeric(m),as.numeric(n),alternative="two.side")
   c[i,j]<-as.numeric(cortest[3])
 }
}
write.table(b,"correlation.txt")
write.table(c,"pvalue.txt")
#查看r中某元素类型
mode(m)
#最终版本,在服务器上运行过，
cir<-read.table("circRNA_less.txt",head=T,row.names=1,sep="\t")
mrna<-read.table("mRNA_less.txt",head=T,row.names=1,sep="\t")
b<-matrix(0,429,234)
c<-b
for (i in 1:429){
 for (j in 1:234){
   b[i,j]<-cor(as.numeric(cir[i,]),as.numeric(mrna[j,]))
 }
}
write.table(b,"correlation.txt")
write.table(c,"pvalue.txt")


cir<-read.table("circRNA.txt",head=T,row.names=1,sep="\t")
mrna<-read.table("mRNA.txt",head=T,row.names=1,sep="\t")
a<-matrix(0,10000,13633)
p<-a
for (i in 1:10000){
 for (j in 1:13633){
   a[i,j]<-cor(as.numeric(cir[i,]),as.numeric(mrna[j,])) ##原因是由于cir[i, ]是一个行名字的list，需要转化为数值型
   p[i,j]<-as.numeric(cor.test(as.numeric(cir[i,]),as.numeric(mrna[j,]),alternative="two.side"))
 }
}
错误于a[i, j] <- cor(cir[i, ], mrna[j, ]) : 
  被替换的项目不是替换值长度的倍数 ##原因是由于cir[i, ]是一个行名字的list，需要转化为数值型
write.table(a,"correlation.txt")
write.table(p,"pvalue.txt")

5. 基本数据结构

 
数值型(numeric）
	
1,1.2,3.1415926
复数型(complex)
	
1+2i
字符型(character)
	
‘A’/ “hello world!”
逻辑型(logical)
	
TRUE / FALSE
6. 基本数据对象

向量(vector), 见下节
矩阵(matrix)： 
更一般的说数组是向量在多维情况下的一般形式。事实上它们是可以被两个或更多的指标索引的向量，并且以特定的方式被打印出来。
因子(factors) 提供了一种处理分类数据的更简介的方式。
列表(list)：  
是向量的一种一般形式，并不需要保证其中的元素都是相同的类型，而且其中的元素经常是向量和列表本身。
数据框(data frame)：
是一种与矩阵相似的结构，其中的列可以是不同的数据类型。可以把数据框看作一种数据"矩阵"，它的每行是一个观测单位，而且(可能)同时包含数值型和分类的变量。
函数( function)：
能够在R的workspace中存储的对象。我们可以通过函数来扩展R的功能
  
write.table(a,"correlation.txt")
write.table(p,"pvalue.txt")


results <- vector(length = 10, mode = "list")
for (j in 1:10) results[[j]] <- t.test(rnorm(100, mean = 1, sd = 1))

#计算每一个circRNA相对于所有mRNA在20个样本里的的相关系数，并对对相关系数进行检验

a<-matrix(0,175938,30000) #生成一个175938＊30000的矩阵，方便写入r
p<-a
cir<-read.table("cir.txt",head=T,row.names=1,sep="\t")
mrna<-read.table("mrna.txt",head=T,row.names=1,sep="\t")
for (i in 1:175938){
 for (j in 1:30000){
   a[i,j]<-cor(cir[i,],mrna[j,]) #求相关系数
   p[i,j]<-cor.test(cir[i,],mrna[j,],alternative="two.side") #对相关系数进行检验，对r不等于0的都可以进行检验
 }
}
write.table(a,"correlation.txt")
write.table(p,"pvalue.txt")


#对相关系数进行检验
cor.test(m,n,alternative="two.side")
建立一个空矩阵
a<-matrix(0,3,3)

fc<-read.table("count_the_detected_numbers.txt",head=T,row.names=1,sep="\t")
a<-matrix(rep(1:20,3), nrow = 3)
for (i in 1:20){
    a[,i]<-table(fc[,i])
    
    
    
    
fc<-read.table("count_the_detected_numbers.txt",head=T,row.names=1,sep="\t")
a<-matrix(rep(1:175938,3), nrow = 3)
for (i in 1:175938){
    a[,i]<-table(m[,i])
}

colnames(a) <- colnames(m)
write.table(a,"each_circRNAs.txt")

#对相关系数进行检验
cor.test(m,n,alternative="two.side")

for (i in 1:tmax){
 for (j in 1:10){

  if (pop_sims[i,j]*exp(r[i,j]) <2) {
    pop_sims[i+1,j]<- 0} 
  else {
      pop_sims[i+1,j] <- pop_sims[i,j]*exp(rnorm(1,mean=0.02, sd=0.1))}
}}

＃统计多少circRNA被监测到了
fc<-read.table("count_the_detected_numbers.txt",head=T,row.names=1,sep="\t")
a<-matrix(rep(1:20,3), nrow = 3)
for (i in 1:20){
    a[,i]<-table(fc[,i])
}

colnames(a) <- colnames(fc)
write.table(a,"count_the_detected_numbers_counted.txt")

WHY1.5倍cutoff
fc<-read.table("10000compounds_new_filtered_DEGs_new_order_siRNA.txt",head=T,row.names=1)
for (i in 1:10361){
    cutoff<-as.numeric(fc[,i])
    indexi<-which(cutoff>=0.5849625)
    Timei <- rep(1, length(cutoff))
    fc[indexi,i]<-as.numeric(Timei[indexi])
}
for (i in 1:10361){
    cutoff<-as.numeric(fc[,i])
    indexi<-which(cutoff<=-0.5849625)
    Timei <- rep(-1, length(cutoff))
    fc[indexi,i]<-as.numeric(Timei[indexi])
}
write.table(fc,"10000compounds_new_filtered_DEGs_new_order_siRNA_for_heatmap_log1.5.txt")

for (i in 1:10361){
    cutoff<-as.numeric(fc[,i])
    indexi<-which(cutoff <0.5849625 & cutoff >-0.5849625)
    Timei <- rep(0, length(cutoff))
    fc[indexi,i]<-as.numeric(Timei[indexi])
}
write.table(fc,"10000compounds_new_filtered_DEGs_new_order_siRNA_for_SCORING_log1.5.txt")  

#大表准备heatmap和scoring matrix
fc<-read.table("10000compounds_new_filtered_DEGs_new_order_siRNA.txt",head=T,row.names=1)
for (i in 1:10361){
    cutoff<-as.numeric(fc[,i])
    indexi<-which(cutoff>=1)
    Timei <- rep(1, length(cutoff))
    fc[indexi,i]<-as.numeric(Timei[indexi])
}
for (i in 1:10361){
    cutoff<-as.numeric(fc[,i])
    indexi<-which(cutoff<=-1)
    Timei <- rep(-1, length(cutoff))
    fc[indexi,i]<-as.numeric(Timei[indexi])
}
write.table(fc,"10000compounds_new_filtered_DEGs_new_order_siRNA_for_heatmap.txt")

for (i in 1:10361){
    cutoff<-as.numeric(fc[,i])
    indexi<-which(cutoff <1 & cutoff >-1)
    Timei <- rep(0, length(cutoff))
    fc[indexi,i]<-as.numeric(Timei[indexi])
}
write.table(fc,"10000compounds_new_filtered_DEGs_new_order_siRNA_for_SCORING.txt")  

.RAR的解压
rar e circRNA.rar
rar x circRNA.rar 保留以前目录结构

例2：解压缩abc.rar档案中的内容，可以使用e或x命令,假设abc.rar目录中有一个名为file1的文件和一个名为test的目录，test目录中有一个名为file2的文件，
$rar e abc.rar

说明：使用e命令，会将abc.rar中的file1文件连同test目录下的file2文件解压到当前目录。如果想保持abc.rar目录中的目录结构请使用x命令。
$rar x abc.rar

说明：此时会将file1文件和test目录解压到当前文件夹。

语法三:R语言的apply()函数顾名思义，会将某个函数“应用”在数据框（或者多种其它R数据结构，但我们目前姑且只关注数据框这一种）当中。它的语法与前两种函数相比要复杂一些，但在某些难度较高的计算过程中会起到重要作用。

apply() 的基本格式为:

    dataFrame$newColumn <- apply(dataFrame, 1, function(x) { . . . } ) 

以上代码行的作用是在数据框内创建一个名为“newColumn”的新column；其中的内容将由{…}的具体代码决定。

下面我们来分别解释以上代码行中各apply()参数的具体含义。第一项apply()参数代表着现有数据框。第二项参数，在本示例中为“1”，意思是“在row中应用一项函数”。如果该参数为2，则代表“在column中应用一项函数”。如果大家打算对当前column而非row进行求和或者求平均值，那么直接修改这条参数就能轻松达到目的。

第三项参数为function(x)，很明显具体内容有待写入。具体来说，在其它情况下function()部分将保持不变，但“x”则可以是任何变量名称。那在我们的示例中，x代表着什么呢？它的意思是将所有条目（row或者column）都将由apply()进行遍历。

最后，{…}代表我们要对遍历的每项条目进行何种操作。

请注意，apply()会对所有row或者column内的每一项条目进行查找发实现函数应用。如果大家应用的函数只能作用于数字条目、而当前数据框中某些column的内容并非数字，就可能引发问题。

我们的财务统计样本数据正好符合这种情况。对于数据变量来说，以下代码无法正常生效：

    apply(companiesData, 1, function(x) sum(x)) 

为什么会这样？因为apply()会试图将每一行中的条目进行相加，而公司名称是无法被计入公式的。

为了让apply()只作用于数据框中的特定column中，例如将营收与利润中的数值相加（当然，我承认这样的算法在现实世界的财务分析工作中不太可能发生），我们需要将对应的数据框子集作为我们的第一项参数。也就是说，我们不再让apply()作用于整套数据框，而只让它针对营收与利润两列起效，具体代码如下所示：

    apply(companiesData[,c('revenue', 'profit')], 1, function(x) sum(x)) 

请大家注意其中的[c('revenue', 'profit')]，这部分内容位于数据框名称之外，它的作用是强调求和操作“只作用于营收与利润两列”。

下面我们要做的是将apply的计算结果保嬖谛碌column当中，代码如下：

    companiesData$sums <- apply(companiesData[,c('revenue', 'profit')], 1, function(x) sum(x)) 

对于单纯的求和函数来说，这样的效果已经合格了。不过让我们再想想前面提到过的例子――根据每一行中的条目计算公司利润率。在这种情况下，我们需要将利润与营收以特定顺序加以排列――也就是用利润除以营收，而不是相反――然后再乘以100。

我们该如何利用匿名function(x)中让apply()以特定顺序处理多个条目？答案是将我们的匿名函数分别命名为x[1]、x[2]并以此类推，利用它们指代不同的条目：

    companiesData$margin <- apply(companiesData[,c('revenue', 'profit')], 1, function(x) { (x[2]/x[1]) * 100 } ) 

以上代码行创建出一项用第二条目除以第一条目的匿名函数――由于在companiesData[,c('revenue', 'profit')]当中，营收在前而利润在后，因此第二条目就指代利润而第一条目指代营收。这种方式在我们的示例中是可地的，因为其中只涉及两项条目，即营收与利润――请记住，我们一定要让apply()只作用于这两个对应column。

在R中很容易求得一个矩阵的各行的和、平均数与列的和、平均数，例如：
> A
  [,1] [,2] [,3] [,4]
[1,]   1   4   7   10
[2,]   2   5   8   11
[3,]   3   6   9   12
> rowSums(A)
[1] 22 26 30
> rowMeans(A)
[1] 5.5 6.5 7.5
> colSums(A)
[1] 6 15 24 33
> colMeans(A)
[1] 2 5 8 11
上述关于矩阵行和列的操作，还可以使用apply()函数实现。
args(apply)
function (X, MARGIN, FUN, ...)
其中：x为矩阵，MARGIN用来指定是对行运算还是对列运算，MARGIN＝1表示对行运算，MARGIN＝2表示对列运算，FUN用来指定运算函数，。...用来给定FUN中需要的其它的参数，例如：
> apply(A,1,sum)
[1] 22 26 30
> apply(A,1,mean)
[1] 5.5 6.5 7.5
> apply(A,2,sum)
[1] 6 15 24 33
> apply(A,2,mean)
[1] 2 5 8 11
apply()函数功能强大，我们可以对矩阵的行或者列进行其它运算，例如：
计算每一列的方差
> apply(A,2,var)

5   数与矩阵相乘
A为m×n矩阵，c>0，在R中求cA可用符号：“*”，例如：
> c=2
> c*A
        [,1] [,2] [,3] [,4]
[1,]   2   8   14   20
[2,]   4   10   16   22
[3,]   6   12   18   24
6   矩阵相乘
A为m×n矩阵，B为n×k矩阵，在R中求AB可用符号：“％*％”，例如：
> A=matrix(1:12,nrow=3,ncol=4)
> B=matrix(1:12,nrow=4,ncol=3)
> A%*%B
      [,1] [,2] [,3]
[1,]   70 158 246
[2,]   80 184 288
[3,]   90 210 330
7   矩阵对角元素相关运算
例如要取一个方阵的对角元素，
> A=matrix(1:16,nrow=4,ncol=4)
> A
      [,1] [,2] [,3] [,4]
[1,]   1   5   9   13
[2,]   2   6   10   14
[3,]   3   7   11   15
[4,]   4   8   12   16
> diag(A)
[1] 1 6 11 16
对一个向量应用diag()
函数将产生以这个向量为对角元素的对角矩阵
> diag(diag(A))
      [,1] [,2] [,3] [,4]
[1,]   1   0   0   0
[2,]   0   6   0   0
[3,]   0   0   11   0
[4,]   0   0   0   16
对一个正整数z应用diag()函数将产生以z维单位矩阵，例如：
> diag(3)
        [,1] [,2] [,3]
[1,]   1   0   0
[2,]   0   1   0
[3,]   0   0   1

选择几列同时满足某条件
aux = which(Mat[,'A'] == 10)
    aux = aux[which(Mat[aux,'B'] == 5)]
    aux = aux[which(Mat[aux,'C'] > 2)]
    Mat[aux, ]

subset(m, m[,4] == 16)

And this will select the last three.

subset(m, m[,4] > 17)


# Fancy way.
similarity.matrix<-apply(matrix,2,function(x)colSums(x==matrix))
diag(similarity.matrix)<-0


# More understandable. But verbose.
similarity.matrix<-matrix(nrow=ncol(matrix),ncol=ncol(matrix))
for(col in 1:ncol(matrix)){
  matches<-matrix[,col]==matrix
  match.counts<-colSums(matches)
  match.counts[col]<-0 # Set the same column comparison to zero.
  similarity.matrix[,col]<-match.countsrenyiliang
}


excel滚动每两列overlap
=SUMPRODUCT((B2:B10>=2)*(C2:C10>=2))
=SUMPRODUCT((B2:B10<>0)*(C2:C10<>0))
R脚本
treat<-read.table("7plates.txt",head=T,row.names=1,sep="\t")
fc<-matrix(rep(0), nrow = 1273,ncol=2564)
for (i in 1:2563){
    cutoffi<-as.numeric(treat[,i])
    cutoffm<-as.numeric(treat[,i+1])
    indexi<-which(cutoffi!=0 & cutoffm!=0)
    Timei <- rep(10, length(cutoffi))
    fc[indexi,i]<-as.numeric(Timei[indexi])
}

a<-matrix(rep(1:2564,2), nrow = 2)
for (i in 1:2564){
    a[,i]<-table(fc[,i])
}
colnames(a) <- colnames(treat)
write.table(a,"7plates_overlap.txt")



mydata <-read.table("total.txt",head=T)
d<-density(mydata$million)
plot(d)
polygon(d,col="red",border="blue")
rug(mydata$million,col="brown")

mydata <-read.table("total.txt",head=T)  #记得取log10图才会好看
d<-density(mydata$million) 
plot(d)
polygon(d,col="red",border="blue")
rug(mydata$million,col="brown")


ls -l | grep "sw*" | wc -l

> m
  a b c
a 1 3 6
b 2 5 3
c 3 4 5
> m["a","a"]
[1] 1
即可获得那个值
> m["a","b"]
[1] 3
> m["b","c"]
[1] 3
先行后列

fc<-read.table("plate3_fc.txt",head=T,row.names=1,sep="\t")
dmso<-read.table("plate3_nc.txt",head=T,row.names=1,sep="\t")
treat<-read.table("plate3_treat.txt",head=T,row.names=1,sep="\t")
stable<-read.table("plate3_stable.txt",head=T,row.names=1,sep="\t")
dmso1<-apply(dmso,1,mean)
me<-apply(stable,2,median)
dmso_cutoff<-as.numeric(dmso1)
for (i in 1:209){
    cutoff<-as.numeric(treat[,i])
    indexi<-which(cutoff<=100*10/me[i] & dmso_cutoff<=5.690731837)
    Timei <- rep(0, length(cutoff))
    fc[indexi,i]<-as.numeric(Timei[indexi])
}
write.table(fc,"plate3_fc_filtered_by_median5_both10.txt")

for (i in 1:209){
    cutoff<-as.numeric(fc[,i])
    indexi<-which(cutoff <1 & cutoff >-1)
    Timei <- rep(0, length(cutoff))
    fc[indexi,i]<-as.numeric(Timei[indexi])
}
write.table(fc,"plate3_fc_by_minor1to1_both10.txt")

for (i in 1:209){
    cutoff<-as.numeric(fc[,i])
    indexi<-which(cutoff!=0)
    Timei <- rep(10, length(cutoff))
    fc[indexi,i]<-as.numeric(Timei[indexi])
}
write.table(fc,"plate3_fc_modified_final_v2_both10.txt")

a<-matrix(rep(1:209,2), nrow = 2)
for (i in 1:209){
    a[,i]<-table(fc[,i])
}

colnames(a) <- colnames(fc)
write.table(a,"plate3_clasification_both10.txt")

epi差异表达
treat<-read.table("plate3_treat.txt",head=T,row.names=1)
control<-read.table("plate3_nc.txt",head=T,row.names=1)
treat<-(log2(treat/as.matrix(control[,1]))+log2(treat/as.matrix(control[,2]))+log2(treat/as.matrix(control[,3]))+log2(treat/as.matrix(control[,4]))+log2(treat/as.matrix(control[,5]))+log2(treat/as.matrix(control[,6]))+log2(treat/as.matrix(control[,7]))+log2(treat/as.matrix(control[,8]))++log2(treat/as.matrix(control[,9])))/9
write.table(treat,"plate3_fc.txt")


a<-read.table("barplot.txt",head=T,sep="\t")
pdf("MYC_barplot.pdf",height=10,width=20)
barplot(a$Log2,col="cyan",ylim=c(-4,4))
dev.off()

博奥fc过滤

fc<-read.table("plate4_20_fc.txt",head=T,row.names=1,sep="\t")
dmso<-read.table("plate4_20_dmso.txt",head=T,row.names=1,sep="\t")
treat<-read.table("plate4_20_treat.txt",head=T,row.names=1,sep="\t")
stable<-read.table("plate4_20_stable.txt",head=T,row.names=1,sep="\t")
dmso1<-apply(dmso,1,mean)
me<-apply(stable,2,median)
dmso_cutoff<-as.numeric(dmso1)
for (i in 1:107){
    cutoff<-as.numeric(treat[,i])
    indexi<-which(cutoff<=100*10/me[i] & dmso_cutoff<=6.407283293)
    Timei <- rep(0, length(cutoff))
    fc[indexi,i]<-as.numeric(Timei[indexi])
}
write.table(fc,"plate4_20_fc_filtered_by_median5_both10.txt")

for (i in 1:107){
    cutoff<-as.numeric(fc[,i])
    indexi<-which(cutoff <1 & cutoff >-1)
    Timei <- rep(0, length(cutoff))
    fc[indexi,i]<-as.numeric(Timei[indexi])
}
write.table(fc,"plate4_20_fc_by_minor1to1_both10.txt")  ＃可以不用输出，没什么用

for (i in 1:107){
    cutoff<-as.numeric(fc[,i])
    indexi<-which(cutoff!=0)
    Timei <- rep(10, length(cutoff))
    fc[indexi,i]<-as.numeric(Timei[indexi])
}
write.table(fc,"plate4_20_fc_modified_final_v2_both10.txt") ＃可以不用输出，没什么用

a<-matrix(rep(1:107,2), nrow = 2)
for (i in 1:107){
    a[,i]<-table(fc[,i])
}

colnames(a) <- colnames(fc)
write.table(a,"plate4_20_clasification_both10.txt")

#对于>1 <－1的基因，均变成1，－1，为了heatmap更加明显,同样也可以将lane7_plate1_fc_by_minor1to1.txt变成打分矩阵
fc<-read.table("total_fc.txt",head=T,row.names=1)
for (i in 1:2564){
    cutoff<-as.numeric(fc[,i])
    indexi<-which(cutoff>=1)
    Timei <- rep(1, length(cutoff))
    fc[indexi,i]<-as.numeric(Timei[indexi])
}
write.table(fc,"total_above1.txt")

for (i in 1:2564){
    cutoff<-as.numeric(fc[,i])
    indexi<-which(cutoff<=-1)
    Timei <- rep(-1, length(cutoff))
    fc[indexi,i]<-as.numeric(Timei[indexi])
}
write.table(fc,"total_above1_below1.txt")


a<-read.table("plot2.txt",head=T,sep="\t")
pdf("scatter_plot7.pdf")
plot(a$Lane8_plate2_I1,a$Lane8_plate2_G24,col=as.factor(a$factor),lty=2,pch=20,cex=0.5,xlab="Argatroban",ylab="dmso")
plot(a$Lane8_plate2_I1,a$Lane8_plate2_G24,col=as.factor(a$factor),lty=2,pch=20,cex=0.5,xlab="Argatroban",ylab="dmso")
plot(a$Lane8_plate2_I1,a$Lane8_plate2_G24,col=as.factor(a$factor),lty=2,pch=20,cex=0.5,xlab="Argatroban",ylab="dmso")
plot(a$Lane8_plate2_I1,a$Lane8_plate2_G24,col=as.factor(a$factor),lty=2,pch=20,cex=0.5,xlab="Argatroban",ylab="dmso")

#改变自己想变色的点，先做图，然后piont加颜色
plot（）
> z<-6:10
> y<-6:10
> points(y,z,col="yellow") 可按照区间输入多个点，也可按照点输入单个点

#对dmso和样本只去掉dmso小于5的

fc<-read.table("plate4_20_fc.txt",head=T,row.names=1,sep="\t")
dmso<-read.table("plate4_20_dmso.txt",head=T,row.names=1,sep="\t")
treat<-read.table("plate4_20_treat.txt",head=T,row.names=1,sep="\t")
stable<-read.table("plate4_20_stable2.txt",head=T,row.names=1,sep="\t")
dmso1<-apply(dmso,1,mean)
me<-apply(stable,2,median)
dmso_cutoff<-as.numeric(dmso1)
index1<-which(dmso_cutoff<=3.20364165)
for (i in 1:107){
    cutoff<-as.numeric(treat[,i])
    indexi<-which(cutoff<=100*5/me[i] & dmso_cutoff<=3.20364165)
    Timei <- rep(0, length(cutoff))
    fc[indexi,i]<-as.numeric(Timei[indexi])
}
write.table(fc,"plate4_20_fc_filtered_by_median5_both5.txt")

for (i in 1:107){
    cutoff<-as.numeric(fc[,i])
    indexi<-which(cutoff <1 & cutoff >-1)
    Timei <- rep(0, length(cutoff))
    fc[indexi,i]<-as.numeric(Timei[indexi])
}
write.table(fc,"plate4_20_fc_by_minor1to1_both5.txt")

for (i in 1:107){
    cutoff<-as.numeric(fc[,i])
    indexi<-which(cutoff!=0)
    Timei <- rep(10, length(cutoff))
    fc[indexi,i]<-as.numeric(Timei[indexi])
}
write.table(fc,"plate4_20_fc_modified_final_v2_both5.txt")

a<-matrix(rep(1:107,2), nrow = 2)
for (i in 1:107){
    a[,i]<-table(fc[,i])
}

colnames(a) <- colnames(fc)
write.table(a,"plate4_20_clasification_both5.txt")

可用boxplot来挑dmso

library(ggplot2)

a<- read.csv("mRNA_right.csv",head=TRUE,sep=";")
P.Value <- c(a$PValue)
FC <- c(a$FC)
df <- data.frame(P.Value, FC)


thred = c()
for (i in 1:dim(df)[1]) {

    if ((df$FC[i] < 4 & df$FC[i] > 2) & df$P.Value[i] > 1.30103) {
        thred = c(thred,"red4")
        
    } else if ((df$FC[i] < 2 & df$FC[i] > 1) & df$P.Value[i] > 1.30103) {
        thred = c(thred,"red")
        
    } else if ((df$FC[i] < -1 & df$FC[i] > -2) & df$P.Value[i] > 1.30103) {
        thred = c(thred,"green")  
          
    } else if ((df$FC[i] < -2 & df$FC[i] > -4) & df$P.Value[i] > 1.30103) {
        thred = c(thred,"green4") 
           
    } else if ((df$FC[i] < 1 & df$FC[i] > -1) & df$P.Value[i] > 1.30103) {
        thred = c(thred,"yellow")  
          
    } else if ((df$FC[i] < 1 & df$FC[i] > -1) & df$P.Value[i] < 1.30103) {
        thred = c(thred,"pink")
        
    } else {
        thred = c(thred, "black")
    }
}

#df$threshold = as.factor(((df$FC<4 & df$FC >2) & df$P.Value > 1.30103) & ((df$FC<2 & df$FC > 1) & df$P.Value > 1.30103) & ((df$FC<-1 & df$FC > -2) & df$P.Value > 1.30103) & ((df$FC<-2 & df$FC > -4) & df$P.Value > 1.30103) & ((df$FC<1 & df$FC > -1) & df$P.Value > 1.30103) & ((df$FC<1 & df$FC > -1) & df$P.Value < 1.30103))

df$threshold = as.factor(thred)

pdf("Results.pdf")
g = ggplot(data=df, aes(x=FC, y=P.Value, colour=threshold)) + geom_point(alpha=0.4, size=1.75) + theme(legend.position = "none") +  xlim(c(-3, 4)) + ylim(c(0, 8)) + xlab("log2 fold change") + ylab("-log10 p-value")

g
dev.off()




筛选化合物数目统计，筛选化合物为8199化合物，另外有5个阳性化合物


a<-read.table("LDLR.txt",head=T,sep="\t")
pdf("LDLR2.pdf",height=10,width=20)
barplot(a$BA_NEW_DESIGN_V2_LDLR_A_2,col="cyan",ylim=c(-5,4))  ＃坐标轴范围
dev.off()


library(pheatmap)

data<-read.table("heatmap_remove0.txt",head=T,row.names=1,sep='\t')
data<-data.matrix(data)
tiff(filename = "heatmap_remove0_2_test.tif",width = 3,height = 1,units ="cm",compression="lzw",bg="white",res=600)
pheatmap(data,cluster_row=TRUE,cluster_col=TRUE,show_rownames=F,show_colnames=F,color = colorRampPalette(c("blue", "white", "red"))(50),cellwidth=0.04,cellheight=0.04)
dev.off()

A集群已恢复 （node01-node10已隔离测试），请登录login02（外网:166.111.30.161 :12222。      内网:10.10.0.58）提交作业。login01与mgt不在做为登录点。

＃改变图片分辨率
library(pheatmap)

data<-read.table("heatmap_remove0.txt",head=T,row.names=1,sep='\t')
data<-data.matrix(data)
tiff(filename = "10000compounds_heatmap_remove0.tif",width = 15,height = 18,units ="cm",compression="lzw",bg="white",res=600)
pheatmap(data,cluster_row=TRUE,cluster_col=TRUE,color = colorRampPalette(c("blue", "white", "red"))(50),cellwidth=0.2,cellheight=0.2)
dev.off()


fc<-read.table("capital_bio_heatmap.txt",head=T,row.names=1,sep='\t')
keep = rowSums(fc)!=0
fc = fc[keep,]
write.table(fc,"capital_bio_heatmap_remove0.txt")

library(pheatmap)

data<-read.table("thp1_total.txt",head=T,row.names=1,sep='\t')
data<-data.matrix(data)
ann_col<-read.table("thp1_total_ano_col.txt",head=T,row.names=1,sep='\t')
ann_row<-read.table("ano_row_all.txt",head=T,row.names=1,sep='\t')


annotation_col = data.frame(
Time=factor(ann_col$time,levels=c("12h","24h")),Med_property=factor(ann_col$property,levels=c("kh","gw","kw"))
)

rownames(annotation_col)=rownames(ann_col)

annotation_row = data.frame(
Gene_pathway=factor(ann_row$pathway,levels=c("cancer","immune","lipid_metabolism","glycometabolism"))
)

rownames(annotation_row) =rownames(ann_row)

ann_colors = list(
Med_property = c(gw = "tomato", kh = "darkolivegreen2", kw = "blue"), Gene_pathway = c(cancer = "black", immune = "red", lipid_metabolism = "springgreen", glycometabolism = "blue")
)

pdf(file="thp1_total_named.pdf",height=8,width=22)
pheatmap(data,cluster_row=TRUE,cluster_col=TRUE,show_rownames=F,color = colorRampPalette(c("blue", "white", "red"))(50),cellwidth=6,cellheight=4,fontsize_col=8,annotation_col = annotation_col,annotation_row = annotation_row,annotation_colors = ann_colors)
dev.off()



＃指定注释的颜色

library(pheatmap)

data<-read.table("hepg2_total.txt",head=T,row.names=1,sep='\t')
data<-data.matrix(data)
ann_col<-read.table("hepg2_total_ano_col.txt",head=T,row.names=1,sep='\t')
ann_row<-read.table("ano_row_all.txt",head=T,row.names=1,sep='\t')


annotation_col = data.frame(
Time=factor(ann_col$time,levels=c("12h","24h")),Med_property=factor(ann_col$property,levels=c("kh","gw","kw"))
)

rownames(annotation_col)=rownames(ann_col)

annotation_row = data.frame(
Gene_pathway=factor(ann_row$pathway,levels=c("cancer","immune","lipid_metabolism","glycometabolism"))
)

rownames(annotation_row) =rownames(ann_row)

ann_colors = list(
Med_property = c(gw = "darkred", kh = "gray55", kw = "darkorchid4"), Gene_pathway = c(cancer = "darkred", immune = "gray55", lipid_metabolism = "darkorchid4", glycometabolism = "cyan")   #  不能是12h= "darkorchid4" 这样以数字开头
)

pdf(file="hepg2_total3.pdf",height=8,width=22)
pheatmap(data,cluster_row=TRUE,cluster_col=TRUE,show_rownames=F,show_colnames=F,color = colorRampPalette(c("blue", "white", "red"))(50),cellwidth=6,cellheight=4,fontsize_col=8,annotation_col = annotation_col,annotation_row = annotation_row,annotation_colors = ann_colors)
dev.off()


library(pheatmap)

data<-read.table("231_pathway.txt",head=T,row.names=1,sep='\t')
data<-data.matrix(data)
ann_col<-read.table("ano_col_231.txt",head=T,row.names=1,sep='\t')
ann_row<-read.table("ano_row_all.txt",head=T,row.names=1,sep='\t')


annotation_col = data.frame(
Time=factor(ann_col$time),Med_property=factor(ann_col$property)
)

rownames(annotation_col)=rownames(ann_col)

annotation_row = data.frame(
Gene_pathway=factor(ann_row$pathway)
)

rownames(annotation_row) =rownames(ann_row)

pdf(file="231_pathway2.pdf",height=8,width=22)
pheatmap(data,cluster_row=TRUE,cluster_col=TRUE,show_rownames=F,show_colnames=F,color = colorRampPalette(c("blue", "white", "red"))(50),cellwidth=6,cellheight=4,fontsize_col=8,annotation_col = annotation_col,annotation_row = annotation_row)
dev.off()


library(pheatmap)

data<-read.table("hepg2_pathway.txt",head=T,row.names=1,sep='\t')
data<-data.matrix(data)
ann_col<-read.table("ano_col_hepg2.txt",head=T,row.names=1,sep='\t')
ann_row<-read.table("ano_row_all.txt",head=T,row.names=1,sep='\t')


annotation_col = data.frame(
Time=factor(ann_col$time),Med_property=factor(ann_col$property)
)

rownames(annotation_col)=rownames(ann_col)

annotation_row = data.frame(
Gene_pathway=factor(ann_row$pathway)
)

rownames(annotation_row) =rownames(ann_row)

pdf(file="test_pathway2.pdf",height=8,width=22)
pheatmap(data,cluster_row=TRUE,cluster_col=TRUE,show_rownames=F,show_colnames=F,cellwidth=6,cellheight=4,fontsize_col=8,annotation_col = annotation_col,annotation_row = annotation_row)
dev.off()

library(pheatmap)

data<-read.table("thp1_pathway_24h.txt",head=T,row.names=1,sep='\t')
data<-data.matrix(data)
ann_col<-read.table("ano_col_thp1_24h.txt",head=T,row.names=1,sep='\t')
ann_row<-read.table("ano_row_all.txt",head=T,row.names=1,sep='\t')


annotation_col = data.frame(
Time=factor(ann_col$time),Med_property=factor(ann_col$property)
)

rownames(annotation_col)=rownames(ann_col)

annotation_row = data.frame(
Gene_pathway=factor(ann_row$pathway)
)

rownames(annotation_row) =rownames(ann_row)

pdf(file="thp1_pathway_24h.pdf",height=8,width=22)
pheatmap(data,cluster_row=TRUE,cluster_col=FALSE,show_rownames=F,show_colnames=F,color = colorRampPalette(c("navy", "white", "firebrick3"))(50),cellwidth=6,cellheight=4,fontsize_col=8,annotation_col = annotation_col,annotation_row = annotation_row)
dev.off()

pheatmap改变字体
library(pheatmap)

data<-read.table("ctnnb_total_ranked.txt",head=T,row.names=1,sep='\t')
data<-data.matrix(data)
pdf(file="ctnnb_total_ranked3.pdf")
pheatmap(data,cluster_row=FALSE,cluster_col=TRUE,cellwidth=2,cellheight=2,fontsize_col=2,fontsize_row=2)
dev.off()


mydata <-read.table("total.txt",head=T)
d<-density(mydata$million)
plot(d)
polygon(d,col="red",border="blue")
rug(mydata$million,col="brown")

library(grid)                                                                                                                                                                                                                                             
library(VennDiagram)                                                                                                                                                                                                                                      
                                                                                                                                                                                                                                                          
mydata1<-read.table("/Users/lishasha/Desktop/home/new_work/20150904_7000compound/capital_bio/plot_for_shanghai/microarray/RASL_YA.txt",head=T)                                                                                                            
mydata2<-read.table("/Users/lishasha/Desktop/home/new_work/20150904_7000compound/capital_bio/plot_for_shanghai/microarray/array_YA.txt",head=T)                                                                                                           
venn.diagram(list("RASL"=mydata1$RASL,"Aarray"=mydata2$Microarray),fill=c("paleturquoise2","thistle1"),"/Users/lishasha/Desktop/home/new_work/20150904_7000compound/capital_bio/plot_for_shanghai/microarray/DY.png",alpha = c(0.6, 0.6), cex = 2,cat.fontface = 4,lty =2)

a<-matrix(1:20,nrow=4,ncol=5)
＃pip, setuptools, rpy2-2.7.1安装


  508  cd home/tools/
  526  cd pip-7.0.3/
  527  ls
  528  python setup.py install
  529  sudo python setup.py install
  530  cd ..
  531  ls
  532  unzip -d setuptools-18.4.zip 
  533  ls -t
  534  unzip setuptools-18.4.zip -d setuptools-18.4
  535  ls -t
  536  cd setuptools-18.4
  537  ls
  538  cd setuptools-18.4/
  539  ls
  540  python setup.py install
  541  sudo python setup.py install
  542  ls
  543  cd ..
  544  ls
  545  cd ..
  546  ls
  547  cd pip-7.0.3/
  548  ls
  549  ll
  550  sudo python setup.py install
  551  which python
  552  which pip
  553  which pip/
  554  vim ~/.bashrc 
  555  source ~/.bashrc 

  sudo easy_install rpy2
  
  
Windows环境下安装pip，方便你的开发
python , Python

1.在以下地址下载最新的PIP安装文件：http://pypi.python.org/pypi/pip#downloads

2.解压安装

3.下载Windows的easy installer，然后安装：http://pypi.python.org/pypi/setuptools

4.安装setuptools工具

5.命令行工具cd切换到pip的目录，找到setup.py文件，然后输入python setup.py install，运行即可（之所以茉诵姓獠剑是因为之前安装的setuptools工具，以后就可以随意安装python的库了，只要找对setup.py文件的路径，运行上述命令，就可以方便的沧傲耍17

6.把python的安装路径添加到环境变量path中，例如G:\python2.6\Scripts

7.完成！


另：安装完pip和easy_installer工具后，以后再安装python其他库就方便了



将3列数据变成矩阵
最简单的方法，先把所有column加入元组（无重复），排个序，然后根据kegg作为键，把所有行分类作为列表的元素，然后循环字典的键，查找每个值（column）在元组里的index，然后把nes写到该位置

subset(airquality, Temp > 80, select = c(Ozone, Temp)) #选取Temp>80的列，只在Ozone, Temp 两列里面选取
subset(airquality, Day == 1, select = -Temp) #选取Day == 1的列，不选Temp 
subset(airquality, select = Ozone:wind) #在 Ozone到wind里面的列选
new<-subset(ttoal,Median>5,select=probe_name:Median)


 Use biased variances. When calculating the ranking metrics, as described in Metrics for Ranking Genes, GSEA uses an unbiased variance to calculate standard deviation. Select this option to have GSEA use a biased variance instead. By default, this option is not selected.


 Use biased variances. When calculating the ranking metrics, as described in Metrics for Ranking Genes, GSEA uses an unbiased variance to calculate standard deviation. Select this option to have GSEA use a biased variance instead. By default, this option is not selected.

   Enrichment statistic. To calculate the enrichment score, GSEA first walks down the ranked list of genes increasing a running-sum statistic when a gene is in the gene set and decreasing it when it is not. The enrichment score is the maximum deviation from zero encountered during that walk. This parameter affects the running-sum statistic used for the analysis. The last section of the Gene Set Enrichment Analysis PNAS paper shows the mathematical descriptions of the methods used in GSEA. This option controls the value of p used in the enrichment score calculation shown there:

74          classic: p=0

74          weighted (default): p=1

74          weighted_p2: p=2

74          weighted_p1.5: p=1.5
Metric for ranking genes. GSEA ranks the genes in the expression dataset and then analyzes that ranked list of genes. Use this parameter to select the metric used to score and rank the genes; use the Gene list sorting mode parameter to determine whether to sort the genes using the real (default) or absolute value of the metric score; and use the Gene list ordering mode parameter to determine whether to sort the genes in descending (default) or ascending order. For descriptions of the ranking metrics, see Metrics for Ranking Genes.

Note: The default metric for ranking genes is the signal-to-noise ratio. To use this metric, your phenotype file must define at least two categorical phenotypes and your expression dataset must contain at least three (3) samples for each phenotype. If you are using a continuous phenotype or your expression dataset contains fewer than three samples per phenotype, you must choose a different ranking metric. If your expression dataset contains only one sample, you must rank the genes and use the GSEAPreranked Page to analyze the ranked list; none of the GSEA metrics for ranking genes can be used to rank genes based on a single sample.
f your dataset contains only one sample, GSEA cannot rank the genes; however, you can rank the genes and then use the GSEAPreranked Page to analyze your ranked list of genes.

 Use median instead of mean for metrics. For categorical phenotypes, by default, GSEA calculates differential expression based on the mean expression value for each phenotype.
 For categorical phenotypes, GSEA determines a gene’s mean expression value for each phenotype and then uses one of the following metrics to calculate the gene’s differential expression with respect to the two phenotypes
 

Optionally, use the Cparam_file parameter to specify a parameter file, which can contain any parameter except Cparam_file. If you specify the same parameter on the command line and in the parameter file, the value on the command line takes precedence. A parameter file is a text file that defines one parameter per line. Each line contains a parameter name (without the initial hyphen), a tab (not spaces), and the parameter value. For example: GSEAParameters.txt.
1.       Following is a command line that might appear when you click the Command button in GSEA. To run the command from the command line, you must add the Ccp parameter. In this example, the Cgmx and Cchip parameters reference files on the GSEA ftp site. You must download these files from the GSEA web site (http://www.broadinstitute.org/gsea/downloads.jsp) and update the command line to reference the downloaded files. If necessary, quote file names that include spaces and/or remove hyphens from the file names.

java -Xmx512m xtools.gsea.Gsea 
-res \\Krypton\GSEATest\DataSets\P53_hgu95av2.gct 
-cls \\Krypton\GSEATest\DataSets\P53.cls#MUT_versus_WT 
-gmx ftp.broadinstitute.org://pub/gsea/gene_sets/c1.v2.symbols.gmt 
-chip ftp.broadinstitute.org://pub/gsea/annotations/HG_U95Av2.chip 
-collapse true -mode Max_probe -norm meandiv -nperm 1000 -permute phenotype 
-rnd_type no_balance -scoring_scheme weighted -rpt_label my_analysis 
-metric Signal2Noise -sort real -order descending -include_only_symbols true 
-make_sets true -median false -num 100 -plot_top_x 20 -rnd_seed timestamp 
-save_rnd_lists false -set_max 500 -set_min 15 -zip_report false 
-out C:\Program Files\gsea_home\dec18 -gui false

2.       Following is a command line that assumes that the identifiers in your dataset match those in your gene sets:

java C Xmx1024m -cp /xchip/projects/xtools/gsea2.jar xtools.gsea.Gsea 
-res test.gct -cls test.cls -gmx test.gmx -collapse false

3.       Following is a command line that assumes that your dataset uses HG_U133A probe identifiers and your gene sets use gene symbols, so you want to collapse your dataset:

java -Xmx1024m -cp /xchip/projects/xtools/gsea2.jar xtools.gsea.Gsea 
-res foo.gct -cls foo.cls -gmx foo.gmx 
-chip ftp.broadinstitute.org://pub/gsea/annotations/HG_U133A.chip



gsea命令行
java -Xmx512m xtools.gsea.Gsea -res /Users/lishasha/Desktop/home/new_work/20150801_3000drugs/gsea/20150812talk/10084.gct -cls /Users/lishasha/Desktop/home/new_work/20150801_3000drugs/gsea/20150812talk/10084.cls#Treat_versus_Nontreat -gmx /Users/lishasha/Desktop/home/new_work/20150801_3000drugs/gsea/20150812talk/myc.gmt -collapse false -mode Max_probe -norm meandiv -nperm 1000 -permute phenotype -rnd_type no_balance -scoring_scheme weighted -rpt_label 10084 -metric log2_Ratio_of_Classes -sort real -order descending -include_only_symbols true -make_sets true -median false -num 100 -plot_top_x 20 -rnd_seed timestamp -save_rnd_lists false -set_max 500 -set_min 0 -zip_report false -out /Users/lishasha/gsea_home/output/八月12 -gui false
gsea与我们数据相关的几点
Can I use GSEA to analyze SNP, SAGE, ChIP-Seq or RNA-Seq data?
GSEA requires as input an expression dataset, which contains expression profiles for multiple samples. While the software supports multiple input file formats for these datasets, the tab-delimited GCT format is the most common. The first column of the GCT file contains feature identifiers (genes or probes in the case of data derived from DNA microarray experiments, genes or transcripts in the case of data derived from RNA-Seq experiments). The second column contains a description of the feature; this column is ignored by GSEA. Subsequent columns contain the expression values for each feature, with one sample's expression profile per column.
It is important to note that there are no hard and fast rules regarding how a GCT file's expression values are derived. The important point is that they are comparable to one another across features within a sample and comparable to one another across samples. In the case of expression data derived from RNA-seq experiments, it is not required that the expression values be in units of FPKM. RSEM quantification produces expression estimates in several units:
expected counts
transcripts per million (TPM)
FPKM
Expression data in units of FPKM has been normalized to support comparisons of a feature's expression levels across samples, which is important for GSEA; however, there are other RNA-seq normalization methods (e.g., TMM, geometric mean) that could be applied to the expected counts prior to the incorporation of the expression estimates into a GCT expression data file.
The GSEA algorithm ranks the features listed in a GCT file. It provides a number of alternative statistics that can be used for feature ranking. But in all cases (or at least in the cases where the dataset represents expression profiles for differing categorical phenotypes) the ranking statistics capture some measure of genes' differential expression between a pair of categorical phenotypes. The GSEA team has yet to determine whether any of these ranking statistics, originally selected for their effectiveness when used with expression data derived from DNA Microarray experiments, are appropriate for use with expression data derived from RNA-seq experiments. We hopefully will be able to devote some time to investigating this, but in the mean time, we are recommending use of the GSEAPreranked tool for conducting gene set enrichment analysis of data derived from RNA-seq experiments.
In particular:
Prior to conducting gene set enrichment analysis, conduct your differential expression analysis using any of the tools developed by the bioinformatics community (e.g., cuffdiff, edgeR, DESeq, etc).
Based on your differential expression analysis, rank your features and capture your ranking in an RNK-formatted file. The ranking metric can be whatever measure of differential expression you choose from the output of your selected DE tool. For example, cuffdiff provides the (base 2) log of the fold change.
Run GSEAPreranked, but make sure to select "classic" for your enrichment score (thus, not weighting each gene's contribution to the enrichment score by the value of its ranking metric).
Please note that if you choose to use any of the gene sets available from MSigDB in your analysis, you need to make sure that the features listed in your RNK file are genes, and the genes are identified by their HUGO gene symbols. All gene symbols listed in the RNK file must be unique, and we recommend the values of the ranking metrics be unique.

Can I use GSEA to analyze a dataset that contains a single sample?
Yes.  However, GSEA has no way of ranking the genes in such a dataset. Therefore, you must rank the genes and then use GSEA to analyze the ranked list of genes. For more information, see the GSEA Preranked Page in the GSEA User Guide.
Can I use GSEA to analyze time series data?
Yes. The phenotype labels (.cls) file defines the experimental phenotypes and associates each sample in your dataset with one of those phenotypes. To analyze time course data, use a continuous phenotype label. For more information, see Phenotype Labels in the GSEA User Guide. When you run the GSEA analysis, select Pearson in the Metric for ranking genes parameter. This is the only metric that can be used with time series data.
Can I use GSEA with gene sets that have both up- and down-regulated genes?
The GSEA software does not yet support this, but you can use the enrichment statistic with gene sets that include both up- and down-regulated genes. For one approach, see Lamb, et al 2006.

The rank file is a list of detected genes and a rank metric score. At the top of the list are genes with the "strongest" up-regulation, at the bottom of the list are the genes with the "strongest" down-regulation and the genes not changing are in the middle. The metric score I like to use is the sign of the fold change multiplied by the inverse of the p-value, although there may be better methods out there (link).
#!/bin/bash
DGE=$1
RNK=`echo $DGE | sed 's/.xls/.rnk/'`
sed 1d $DGE \
| sort -k7g \
| cut -d '_' -f2- \
| awk '!arr[$1]++' \
| awk '{OFS="\t"}
{ if ($6>0) printf "%s\t%4.3e\n", $1, 1/$7 ;
else printf "%s\t%4.3e\n", $1, -1/$7 }' \
| sort -k2gr > $RNK

And here's the top and bottom 10 lines of the rank file:

$cat DEB_DESeq.rnk | (head;tail)
AC011899.9 3.352e+64
CAMK1D 1.471e+51
HSD11B1 1.919e+48
GNG4 2.279e+43
CPLX1 3.873e+40
CGNL1 1.200e+39
APCDD1 1.237e+35
MANEAL 9.303e+32
MKX 4.441e+28
HOXA13 1.007e+26
HLA-B -2.908e+132
PRAME -3.063e+165
LYZ -9.701e+185
CPXM1 -1.169e+186
HSH2D -1.658e+186
TGFBI -8.856e+210
APOC2 -9.633e+243
APOC4-APOC2 -1.984e+245
COL1A2 -1.298e+274
SLC12A3 -5.422e+304

How to generate a rank file from gene expression data
Turning a gene expression profile into a ranked list is useful for comparing with other profiling data sets as well as an input for preranked GSEA analysis (example here). In this post, I describe a simple bash script called rnkgen.sh that can take gene expression data from a range of sources, such as edgeR, DESeq, GEO2R, etc., and generate a ranked list of genes from most up-expressed to most down-expressed based on the p-value.

#!/bin/bash
#rnkgen.sh converts a differential gene expression spreadsheet (XLS) into a rank file (RNK)

#Specify the input file
XLS=$1
#Specify the gene ID column
ID=$2
#Specify the fold change value column
FC=$3
#Specify the raw p-value column
P=$4

sed 1d $XLS | tr -d '"' \
| awk -v I=$ID -v F=$FC -v P=$P '{FS="\t"} $I!="" {print $I, $F, $P}' \
| awk '$2!="NA" && $3!="NA"' \
| awk '{s=1} $2<0{s=-1} {print $1"\t"s*-1*log($3)/log(10)}' \
| awk '{print $1,$2,$2*$2}' | sort -k3gr \
| awk '{OFS="\t"} !arr[$1]++ {print $1,$2}' \
| sort -k2gr > ${XLS}.rnk

After you save the script to a file, allow it to be executed


chmod +x rnkgen.sh

The usage of the script is as follows:
./rnkgen.sh /path/to/spreadsheet.xls GeneID_column FoldChange_column P-value_column

So for a spreadsheet which looks like this one generated by Degust previously:
gene c aza FDR C1 C2 C3 A1 A2 A3
LYZ 0 -1.65809730597384 4.70051409587021e-16 17393 14675 16300 4951 5064 4841
IFI27 0 -3.40698193025161 1.01987358298544e-15 5695 5552 4630 475 532 442
SLC12A3 0 -3.46002052417193 6.78905193176251e-14 1129 1051 991 79 109 92
HLA-B 0 -1.41354726616356 6.78905193176251e-14 6678 5597 5567 2167 2281 2042
IFI6 0 -2.76026110717513 7.38087480096726e-14 5884 5044 4171 764 764 624
OAS3 0 -2.64899272132832 1.17759867975735e-13 5196 5812 4621 830 824 753
HLA-C 0 -1.39473306959746 1.18784239161695e-13 3715 3324 3382 1305 1350 1193
IGFBP5 0 -5.1043039727239 1.61487772234777e-13 1706 1282 1643 39 39 52
MX1 0 -2.84129882209861 1.61487772234777e-13 2161 2245 1909 277 339 242

So in this case we run it as follows:
./rnkgen.sh degust.xls 1 3 4 

And have a look at the top and bottom 5 lines of output:
CAMK1D 9.80546
AC011899.9 9.76077
CGNL1 9.56363
GNG4 9.42442
MKX 8.93011

IFI6 -13.1319
HLA-B -13.1682
SLC12A3 -13.1682
IFI27 -14.9915
LYZ -15.3279

We see that the result is just what we're after.


GSEA

Turning a gene expression profile into a ranked list is useful for comparing with other profiling data sets as well as an input for preranked GSEA analysis (example here). In this post, I describe a simple bash script called rnkgen.sh that can take gene expression data from a range of sources, such as edgeR, DESeq, GEO2R, etc., and generate a ranked list of genes from most up-expressed to most down-expressed based on the p-value.



1
 gravatar for lkmklsmn
10 months ago by
lkmklsmn 61 450
United States
The GSEA algorithm is based on the Kolmogorov-Smirnov statistical test. This method test for a shift in ranks between a set of interest and the background. You would basically be asking the question, is this particular set of genes enriched among the top genes in the ranked list of all genes?  

This is fairly simple to do in R. The code would look like this (not run):  

scores<- a numeric vector of your scores (prevalence of variants) of all genes in your dataset  

ranking<-rank(scores)  

ind<- a numeric vector containing the indices of your gene set in scores  

geneset<-ranking[ind]  

background<-ranking[-ind]  

ks.test(geneset,background)  

Actually it does not know which is what phenotype, since we do not provide cls file as one would do during normal GSEA, hence phenotype label is na. But, as you have mentioned, based on log fold changes it assumes those genes with positive fold changes are phenotype 1(na_pos) and those with negatives are phenotype 2 (na_neg). (I think you can change this by reversing the rank order, I am not sure) 

Ranking by significance is better than fold change; just think about those lowly expressed genes with extreme fold changes and high p-values. Checkout this NAR which discusses the difference.

The metric that you selected in the Metric for ranking genes parameter (Signal2Noise or tTest) requires that you have at least three samples for each phenotype. You have too few samples for at least one of the phenotypes selected in the Phenotype labels parameter .

To analyze a categorical phenotype that has fewer than three samples, use one of the following ranking metrics:

    Ratio_of_classes
    log2_Ratio_of_classes
    Diff_of_classes
按行操作，每个dmso被同一个数除
 Filtering based on expression values. For many analytical algorithms, such as clustering, it makes sense to preprocess a dataset. For example, before running hierarchical clustering, you might remove genes that have low variance across the dataset. This prevents flat genes from driving the clustering result and improves processing time by focusing on a smaller number of interesting genes.

The GSEA algorithm does not filter the expression dataset and does not benefit from your filtering of the expression dataset. During the analysis, genes that are poorly expressed or that have low variance across the dataset populate the middle of the ranked gene list and the use of a weighted statistic ensures that they do not contribute to a positive enrichment score. By removing such genes from your dataset, you may actually reduce the power of the statistic. Processing time is rarely a factor; GSEA can easily analyze 22,000 genes with even modest processing power.
Filtering based on expression values. For many analytical algorithms, such as clustering, it makes sense to preprocess a dataset. For example, before running hierarchical clustering, you might remove genes that have low variance across the dataset. This prevents flat genes from driving the clustering result and improves processing time by focusing on a smaller number of interesting genes.

The GSEA algorithm does not filter the expression dataset and does not benefit from your filtering of the expression dataset. During the analysis, genes that are poorly expressed or that have low variance across the dataset populate the middle of the ranked gene list and the use of a weighted statistic ensures that they do not contribute to a positive enrichment score. By removing such genes from your dataset, you may actually reduce the power of the statistic. Processing time is rarely a factor; GSEA can easily analyze 22,000 genes with even modest processing power.


total<-read.table("Lane7_plate1_D7_control.txt",head=T,row.names=1)
control<-total[1:5]
treat<-total[6:384]
for (i in 1:1273){
    control[i,]=log2(treat[i,]/control[i,])

The GSEA PNAS 2005 paper introduced a method where a running sum statistic is incremented by the absolute value of the ranking metric when a gene belongs to the set. This method has proven to be efficient and facilitates intuitive interpretation of ranking metrics that reflect correlation of gene expression with phenotype. In the case of GSEAPreranked, you should make sure that this weighted scoring scheme applies to your choice of ranking statistic. When in doubt, we recommend using a more conservative scoring approach by setting Enrichment statistic to classic. Please refer to the GSEA PNAS 2005 paper for further details.


> control<-read.table("Lane7_plate1_D7_control.txt",head=T,row.names=1)
> treat<-read.table("Lane7_plate1_D7_treat.txt",head=T,row.names=1)
for (i in 1:1273){
    control[i,]=log2(treat[i,]/control[i,])
}
write.table(control,"treatvs_control_log2.txt")

stable<-read.table("stable_1.txt",head=T,row.names=1)
total<-read.table("total_1.txt",head=T,row.names=1)
me<-apply(stable,2,median)
head(me)
for (i in 1:2704){
    total[,i]=100*total[,i]/me[i]
}
write.table(total,"normalized_by_median100.txt")


............*AAAAAAgrep出6A前面的有>15个任意字符的正则表达式

stable<-read.table("compound60.txt",head=T)
total<-read.table("compound60_total.txt",head=T)
me<-apply(stable,2,median)
for (i in 1:64){
    total[,i]=log2(total[,i]/me[i]+1) #再想想加1是不是合适
}
write.table(total,"normalized_by_median_of_stable_genes2.txt")

stable<-read.table("stable.txt",header=T,row.names=1)
head(stable)
total<-read.table("total.txt",header=T,row.names=1)
head(total)
me<-apply(stable,2,median)
me
dim(stable)
dim(total)
for (i in 1:2074){
    total[,i]=total[,i]/me[i]
}
write.table(total,"normalized_by_median_of_stable_genes_no_log.txt")

stable<-read.table("stable.txt",header=T,row.names=1)
total<-read.table("total.txt",header=T,row.names=1)
me<-apply(stable,2,median)
for (i in 1:2074){
    total[,i]=log2(total[,i]/me[i]+1)
}
write.table(total,"normalized_by_median_of_stable_genes_no_log.txt")
gsea报错，因为名字gct,rnk,gmt不一致,
None of the gene sets that you specified passed the size threshold. Check that the selected gene sets:


alias les='less -N'less可以显示行号
统计某文件夹下文件的个数
ls -l |grep "^-"|wc -l

统计某文件夹下目录的个数
ls -l |grep "^ｄ"|wc -l

统计文件夹下文件的个数，包括子文件夹里的
ls -lR|grep "^-"|wc -l

如统计/home/han目录(包含子目录)下的所有js文件则：
ls -lR /home/han|grep js|wc -l 或 ls -l "/home/han"|grep "js"|wc -l

统计当前文件下所有含有lane为前缀的文件个数

ls -lR ./ | grep Lane | wc -l

python SyntaxError: EOL while scanning string literal 字符串末尾忘记加引号
fastx_trimmer -f 1 -l 51 -Q 33 -i {fastq} -o rasl_lab2_AGTGCAT_L005_R1_51nb.fastq

合并文件夹
sudo cp -r fastq_lane7_corrected/* fastq_lane8_corrected/* merged_lane7_lane8/

报错
 ERROR: barcode TGAATTC for lane 7 has length 7: expected barcode lenth (including delimiters) is 14
将I7n 改为I7即可和sample sheet没有关系
configureBclToFastq.pl --input-dir /data/150728_C00126_0206_AC5409ACXX/Data/Intensities/BaseCalls/ --output /data/150728_C00126_0206_AC5409ACXX/unligned_by_barcode8 --sample-sheet /data/150728_C00126_0206_AC5409ACXX/SampleSheet_lane8.csv --no-eamss --use-bases-mask Y101,I7,Y101

SampleSheet_barcode.csv
configureBclToFastq.pl --input-dir /data/150728_C00126_0206_AC5409ACXX/Data/Intensities/BaseCalls/ --output /data/150728_C00126_0206_AC5409ACXX/unligned_by_barcode8 --sample-sheet /data/150728_C00126_0206_AC5409ACXX/SampleSheet_lane8.csv --no-eamss --use-bases-mask Y101,I7n,Y101

configureBclToFastq.pl --input-dir /data/150728_C00126_0206_AC5409ACXX/Data/Intensities/BaseCalls/ --output /data/150728_C00126_0206_AC5409ACXX/unligned_by_barcode3 --sample-sheet /data/150728_C00126_0206_AC5409ACXX/SampleSheet_lane7_8.csv --no-eamss --use-bases-mask Y101,I7n,Y101
cd /data/150728_C00126_0206_AC5409ACXX/unligned_by_barcode
nohup make -j 8

grep -A 3 ':N:0:TGTTGTT' ./lane8_R1.fastq | awk '$1 != "--"' >> ./Lane8_plate1_A1_tmp.fastq
grep ':N:0:TGTTGTT' ./lane8_R1.fastq >> tmp.fasta

mac中去掉^M
在vim中 %s/\r/\r\n/g
               %s/^@//g (^@为control ＋shift＋2)
               
configureBclToFastq.pl --input-dir /data/150728_C00126_0206_AC5409ACXX/Data/Intensities/BaseCalls/ --output /data/150728_C00126_0206_AC5409ACXX/Unaligned --sample-sheet /data/150728_C00126_0206_AC5409ACXX/SampleSheet_undetermined.csv --no-eamss --use-bases-mask Y101,I7n,Y101
cd /data/150728_C00126_0206_AC5409ACXX/Unaligned
nohup make -j 8


setwd('C:\\Documents and Settings\\Administrator\\桌面\\Day5') ;
dat <- read.table("OV.clin.merged.picked.txt",header=T,row.names=1,sep='\t') ;
ov_cl <- t(dat) ;
dat1 <- ov_cl[,c(3:5,14)] ;
toupper(rownames(dat1))-> dat2;
dat2-> rownames(dat1) ;

Vital_status <- as.numeric(dat1[,1]) ;
index1 <- which(Vital_status==1) ;
index2 <- which(Vital_status==0) ;
Time <- rep(0, length(Vital_status)) ;
Time[index1] <- as.numeric(dat1[,2][index1]) ;
Time[index2] <- as.numeric(dat1[,3][index2]) ;

clinical <- data.frame(Time, Vital_status) ;
rownames(clinical) <- rownames(dat1) ;

library(survival) ;
type <- c(rep(1, 290), rep(2, 290)) ;
clinical$type <- type;

surv_curv <- survfit( Surv(Time, Vital_status)~type, data =clinical) ;
plot(surv_curv ,col=c("red", "blue"), main="test") ;

t_results <- survdiff(Surv(Time, Vital_status)~type, data =clinical);
p<-pchisq(t_results$chisq, 1 , lower.tail = F) ;



exp_data <- read.table("OV.uncv2.mRNAseq_RSEM_all.txt",header=T, row.names=1, sep='\t') ;

surv_mode <- coxph(Surv(days, event)~type ,data=data.frame((mer))) ;
summary(surv_mode) ;


ayi
indentical(mutation2[, "Tumor_Seq_Allele1"], mutation2[, "Tumor_Seq_Allele2"])
identical(mutation2[, "Tumor_Seq_Allele1"], mutation2[, "Tumor_Seq_Allele2"])
identical(mutation2[, "Reference_Allele"], mutation2[, "Tumor_Seq_Allele1"])
head(rank(num))
result <- rank(num)
top(result)
barplot(result[1:10])
barplot(result[1:10], pos=2)
barplot(result[1:10], las=2)
barplot(result[1:10])
barplot(result[1:10], las=2)
num <- table(mutation2[,1])
head( table(mutation2[,1]))
num <- table(mutation2[,1])
result <- sort(num, decreasing=T)
head(result)
barplot(result[1:10])
barplot(result[1:10], las=2)
head(order(num))
head(rank(num))
?order
a <- read.table( "BRCA.maf", header=TRUE, sep="\t", as.is=T, quote="" );
mutation_level2 <- a[,c("Hugo_Symbol","Entrez_Gene_Id", "Tumor_Sample_Barcode","Variant_Classification",
 "Reference_Allele",  "Tumor_Seq_Allele1", "Tumor_Seq_Allele2", "Variant_Type")] ;
head(mutation_level2)

diag(m1)<-c(2,2)
可以用来自己定义对角上是2，2
usr。一个形式为c(x1, x2, y1, y2)的向量，表示当前绘图区域的坐标值范围：c(xleft, xright, ybottom, ytop)
segments(x0, y0,x1, y1)从(x0,y0)各点到(x1,y1)各点画线段
x<-rbind(matrix(rnorm(100,sd=0.3),ncol=2),matrix(rnorm(100,mean=1,sd=0.3),ncol=2))
> x
               [,1]         [,2]
  [1,]  0.075809032 -0.404080927
  [2,] -0.169217512  0.024630198
  [3,] -0.326937759 -0.068034088
  [4,] -0.171604128  0.215880739
  [5,]  0.054431972  0.351992275
  [6,] -0.173686287 -0.223952908
  [7,]  0.576472874 -0.219211894
  [8,] -0.067776176  0.492954623
  [9,]  0.277901424 -0.010828356
 [10,]  0.206216625  0.243999254
 [11,] -0.049710020  0.030814346
 [12,] -0.417457200  0.224153542
 [13,] -0.434220743 -0.004177382
 [14,] -0.306631901  0.020381852
 [15,] -0.378454487  0.189841490
 [16,] -0.610371588 -0.239180151
 [17,]  0.233768075  0.074860149
 [18,] -0.024903611 -0.027231788
 [19,] -0.465638102 -0.194418106
 [20,] -0.085820736 -0.331151136
 [21,] -0.189881969  0.051737234
 [22,] -0.315167714 -0.323539376
 [23,]  0.140299351 -0.399533695
 [24,] -0.133015220  0.100039819
 [25,] -0.005198166 -0.053234057
 [26,] -0.098172165 -0.336291389
 [27,]  0.455299180  0.240420169
 [28,] -0.171613843 -0.048417871
 [29,] -0.331062919 -0.064269605
 [30,]  0.085592035 -0.114455008
 [31,]  0.114850089  0.092184222
 [32,]  0.199626606 -0.428192373
 [33,]  0.063885459 -0.333225502
 [34,] -0.420853809  0.319425097
 [35,] -0.224300697 -0.141702796
 [36,]  0.169283536  0.495873724
 [37,] -0.276329627 -0.178151078
 [38,]  0.112015148 -0.026264697
 [39,]  0.019418563  0.223021695
 [40,]  0.242171256  0.109160957
 [41,] -0.149711206  0.092786656
 [42,] -0.286986763 -0.198599716
 [43,]  0.122501635 -0.171359457
 [44,] -0.429046107  0.643727210
 [45,]  0.010416601 -0.163280718
 [46,] -0.168532997  0.379974408
 [47,] -0.046058655 -0.414097070
 [48,]  0.018146006  0.118730073
 [49,] -0.008583462  0.309292921
 [50,]  0.172602507 -0.204626754
 [51,]  0.961742541  0.820614274
 [52,]  1.598841567  1.577888202
 [53,]  0.897445050  1.168759085
 [54,]  1.013799067  1.091085291
 [55,]  0.511657562  1.385365124
 [56,]  0.782030252  0.930881428
 [57,]  1.197374783  1.298786510
 [58,]  0.817749961  1.162679101
 [59,]  0.794933521  0.610055485
 [60,]  0.725761177  1.137923744
 [61,]  0.763168255  0.691686276
 [62,]  1.105607229  0.928403626
 [63,]  1.279447009  1.244126881
 [64,]  1.011831494  1.112167507
 [65,]  1.397522323  0.836428417
 [66,]  0.993731985  0.581916940
 [67,]  1.321884256  0.640129329
 [68,]  1.045486889  1.180896862
 [69,]  1.390668410 -0.169915471
 [70,]  1.226201415  1.090748552
 [71,]  1.407584183  1.441503876
 [72,]  1.270954711  0.701952863
 [73,]  0.721389589  0.972209906
 [74,]  0.738783669  1.283238226
 [75,]  1.012990155  0.664711906
 [76,]  0.711501650  0.079330601
 [77,]  0.708253032  1.480811101
 [78,]  0.515331180  0.257406825
 [79,]  1.128984919  0.577839141
 [80,]  1.249673633  0.848643705
 [81,]  1.103466520  1.400149290
 [82,]  0.650010708  1.079139687
 [83,]  1.008084515  0.962062097
 [84,]  1.384351199  1.332199415
 [85,]  0.788717321  0.710343418
 [86,]  0.817884051  1.123601903
 [87,]  1.418158751  0.923528762
 [88,]  0.466508204  1.414134615
 [89,]  1.202763623  1.317636399
 [90,]  0.874880578  0.588613253
 [91,]  0.778131987  0.798397963
 [92,]  1.191823337  0.854885861
 [93,]  0.715928728  1.596909605
 [94,]  0.854245888  0.611078775
 [95,]  1.525909593  1.530199090
 [96,]  1.136020739  1.085209413
 [97,]  0.902053599  0.859028973
 [98,]  0.738916686  1.050634323
 [99,]  1.325679879  0.906956396
[100,]  0.742656350  1.604796094
> c1<-kmeans(x,2,20)
> c1
K-means clustering with 2 clusters of sizes 48, 52

Cluster means:
         [,1]        [,2]
1  1.01461856 1.021688400
2 -0.04537317 0.001175234

Clustering vector:
  [1] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
 [35] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 [69] 1 1 1 1 1 1 1 2 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1

Within cluster sum of squares by cluster:
[1] 9.329080 7.273998
 (between_SS / total_SS =  76.5 %)

Available components:

[1] "cluster"      "centers"      "totss"        "withinss"    
[5] "tot.withinss" "betweenss"    "size"         "iter"        
[9] "ifault"      
> ??kmeans
> c
function (..., recursive = FALSE)  .Primitive("c")
> c1$cluster
  [1] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
 [35] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 [69] 1 1 1 1 1 1 1 2 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
> plot(x,col=c1$cluster,pch=3,lwd=1)
> c1$centers ＃kmean 算出来的中心
         [,1]        [,2]
1  1.01461856 1.021688400
2 -0.04537317 0.001175234
> c1
K-means clustering with 2 clusters of sizes 48, 52

Cluster means:
         [,1]        [,2]
1  1.01461856 1.021688400
2 -0.04537317 0.001175234

Clustering vector:
  [1] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
 [35] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 [69] 1 1 1 1 1 1 1 2 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1

Within cluster sum of squares by cluster:
[1] 9.329080 7.273998
 (between_SS / total_SS =  76.5 %)

Available components:

[1] "cluster"      "centers"      "totss"        "withinss"    
[5] "tot.withinss" "betweenss"    "size"         "iter"        
[9] "ifault"      
> points(c1$centers,col=1:2,pch=7,lwd=3)
> ??points
segments(x[c1$cluster==1,][,1],x[c1$cluster==1,][,2],c1$centers[1,1],c1$centers[1,2]) #c1$cluster==1,]第一类的分类情况，x[c1$cluster==1,][,1]第一类对应的数x[c1$cluster==1,][,1]第一列 就是将分类1里是true的取出来，然后确定center，然后做center到点的直线，center和实体点都是两个数字决定一个点
segments(x[c1$cluster==2,][,1],x[c1$cluster==2,][,2],c1$centers[2,1],c1$centers[2,2],col=2) # segments(x0, y0,x1, y1)从(x0,y0)各点到(x1,y1)各点画线段
 hcluster 聚类
 > n<-seq(1,50,by=4) ＃可利用这种方式做index
> n
 [1]  1  5  9 13 17 21 25 29 33 37 41 45 49
> x<-USArrests[n,] #然后这样按行取，不用循环，如我60个药,n<-seq(1,60,by=1) total[,n]=log(me[,n]
> x
               Murder Assault UrbanPop Rape
Alabama          13.2     236       58 21.2
California        9.0     276       91 40.6
Florida          15.4     335       80 31.9
Illinois         10.4     249       83 24.0
Kentucky          9.7     109       52 16.3
Massachusetts     4.4     149       85 16.3
Missouri          9.0     178       70 28.2
New Hampshire     2.1      57       56  9.5
North Carolina   13.0     337       45 16.1
Oregon            4.9     159       67 29.3
South Dakota      3.8      86       45 12.8
Vermont           2.2      48       32 11.2
Wisconsin         2.6      53       66 10.8


hc1<-hclust(dist(x),method= "complete")
> dist(x) 可用这种方法算出两个之间的距离
                 Alabama California   Florida  Illinois  Kentucky Massachusetts
California      55.52477                                                       
Florida        102.00162   60.98073                                            
Illinois        28.45488   32.71880  86.55871                                  
Kentucky       127.28417  173.20791 228.33276 143.59937                        
Massachusetts   91.64851  129.52471 187.04374 100.49522  52.12571              
Missouri        59.78829  100.98891 157.49175  72.31597  72.29869      35.05382
New Hampshire  179.73620  224.05539 280.24748 194.60766  53.14132      96.72916
North Carolina 101.96102   80.33212  38.52791  96.21419 228.13139     192.40062
Oregon          78.38686  120.03958 176.81066  91.72971  54.00963      24.35672
South Dakota   151.08911  197.52438 252.43884 167.87495  25.00120      74.71017
Vermont        190.37069  237.43546 292.02008 207.92566  64.83255     114.19654
Wisconsin      183.77573  226.45750 283.42380 197.33241  58.41798      98.03311
                Missouri New Hampshire North Carolina    Oregon South Dakota
California                                                                  
Florida                                                                     
Illinois                                                                    
Kentucky                                                                    
Massachusetts                                                               
Missouri                                                                    
New Hampshire  123.42731                                                    
North Carolina 161.45715     280.50556                                      
Oregon          19.69822     104.52215      180.02180                       
South Dakota    96.71194      31.23748      251.19023  78.01577             
Vermont        136.67202      25.68852      289.53523 117.81723     40.22586
Wisconsin      126.43069      10.86002      285.01447 107.63150     39.18469
                 Vermont
California              
Florida                 
Illinois                
Kentucky                
Massachusetts           
Missouri                
New Hampshire           
North Carolina          
Oregon                  
South Dakota            
Vermont                 
Wisconsin       34.37034
>hc2<-hclust(dist(scale(x)),method= "complete") #scale(x, center = TRUE, scale = TRUE)默认算的是zscore
> hc3<-hclust(dist(x),method= "ave")
> layout(matrix(c(1,1,2,3),nrow=2,byrow=T))
> matrix(c(1,1,2,3),nrow=2,byrow=T)(看出来layout按照矩阵来，查一查怎么按照列聚类)
     [,1] [,2]
[1,]    1    1
[2,]    2    3
代表图的顺序是1，1，2，3
所以根据matrix来改变顺序和安排个数
> plot(hc1)
> plot(hc2)
> plot(hc3)
用cluster包做聚类
> library(cluster)
> clusplot(x,pam(x,2)$clustering)
> pam(x,2)
Medoids:
           ID Murder Assault UrbanPop Rape
California  2    9.0     276       91 40.6
Kentucky    5    9.7     109       52 16.3
Clustering vector:
       Alabama     California        Florida       Illinois       Kentucky  Massachusetts       Missouri 
             1              1              1              1              2              2              2 
 New Hampshire North Carolina         Oregon   South Dakota        Vermont      Wisconsin 
             2              1              2              2              2              2 
Objective function:
   build     swap 
56.58520 46.87565 

Available components:
 [1] "medoids"    "id.med"     "clustering" "objective"  "isolation"  "clusinfo"   "silinfo"    "diss"      
 [9] "call"       "data"      
> pam(x,2)$clustering
       Alabama     California        Florida       Illinois       Kentucky  Massachusetts       Missouri 
             1              1              1              1              2              2              2 
 New Hampshire North Carolina         Oregon   South Dakota        Vermont      Wisconsin 
             2              1              2              2              2              2 
>

points(x, y)添加点(可以使用选项type=)

龙星课程学的几个重要函数
data<-read.csv("BRCA2.maf",sep="\t",quote="")
table(data[,"Variant_Classification"]) # 统计这列里元素的个数
index<-which(data[,"Variant_Classification"]=="Silent")利用等号进行匹配  #which实际是返回索引
index<-which(data[,"Variant_Classification"]%in%c("RNA","Silent"))利用正则表达式匹配"Variant_Classification"这列中为英豪中的内容
data1<-data[-index,]提取出不是INDEX的数据
result<-table(data1[,"Hugo_Symbol"]) # 统计这列里元素的个数
 result2<-sort(result,decreasing=T)讲叙排列
pdf("barplot")
barplot(result2[1:10],las=2)字体是斜体
dev.off()

> a
     [,1] [,2] [,3]
[1,]    1    1    1
[2,]    2    3    4
[3,]    2    3    4
> layout(a)
> table(a[,1])

1 2 
1 2 
> table(a[,3])

1 4 
1 2 

table会生成一个含有因子和因子个数的表格 


stable<-read.table("compound60.txt",head=T)
total<-read.table("compound60_total.txt",head=T)
me<-apply(stable,2,median)
for (i in 1:64){
    total[,i]=log2(total[,i]/(me[i]+1)+1) #再想想加1是不是合适
}
write.table(total,"normalized_by_median_of_stable_genes2.txt")
写法2

stable<-read.table("compound60.txt",head=T)
total<-read.table("compound60_total.txt",head=T)
me<-apply(stable,2,median)
n<-seq(1,64,by=1) 
total[,n]=log2(total[,n]/(me[n]+1)+1)
write.table(total,"normalized_by_median_of_stable_genes2.txt")

m<-cbind(a,b)  
 c1<-kmeans(x,2,20) x是这个matrix，2是2类，20是循环次数
 
plot(x,sin(x),type="o",bg=par("bg"))
with(iris,plot(Sepal.Length,Sepal.Width,pch=as.numeric(Species),cex=1.2)) #pch=as.numeric(Species)这个做法很茫就是将species里面的因子转化为数字，有几类可以成为图形的种类，所以颜色也可以这么表示
legend(6.1,4.4,c("setosa","versicolor","virginica"),cex=1.5,pch=1:3) #　在图上添加描述6.1 x轴上面， 4.4  y最上面，其实就是左上角的点
按颜色标记
> with(iris,plot(Sepal.Length,Sepal.Width,col=as.numeric(Species),cex=1.2))
> legend(6.1,4.4,c("setosa","versicolor","virginica"),cex=1.5,fill=1:3) ＃fill=v,v是一个和legend长度一直的向量，
也可以根pch=v,来按照符号就行标记
> with(iris,plot(Sepal.Length,Sepal.Width,pch=as.numeric(Species),cex=1.2))
> legend(6.1,4.4,c("setosa","versicolor","virginica"),cex=1.5,pch=1:3)
axis(1,at=year,label=year);axis(2) 设置坐标轴
mtext("UR(%)",4,3,col="red") ＃可在右边又添加一个标题
par(new=T,mar=c(10,4,10,6)+0.1) #mar控制图形边空的有4个值的向量c(bottom, left, top, right), 缺省值 为c(5.1, 4.1, 4.1, 2.1)
axis(4,col="red",col.axis="red") # 两句一起可以又添加一个坐标
 plot(0,main=paste(strwrap("This is a really long title that
+ 2 i can not type it properly",width=50),collapse="\n"))
标题太长可以利strwrap进行自定义段落格式
> barplot(x,col=rev(heat.colors(10)))  c o l = rev ( heat . c o l o r s ( 1 0 ) )还是反着来
> barplot(x,col=gray((1:10)/10))
barplot(x,col=rainbow(10))
text(2.5,2.5,expression(z[i]==sqrt(x[i]^2+y[i]^2)))  #利用expression添加公式
里面的10一定要指定变几种颜色
x<-1:5
> y<-c(1,3,4,2.5,2)
> plot(x,y)
> sp<-spline(x,y,n=50)
> lines(sp) 模拟出链接这几个点的曲线
x<-1:10
y<-runif(10)
symbols(x,y,circles=y/2,inches=F,bg=x)
# This function draws symbols on a plot. One of six symbols; circles, squares, rectangles, stars, thermometers, and boxplots, can be plotted at a specified set of x and y coordinates. Specific aspects of the symbols, such as relative size, can be customized by additional parameters.
symbols(x, y = NULL, circles, squares, rectangles, stars,
        thermometers, boxplots, inches = TRUE, add = FALSE,
        fg = par("col"), bg = NA,
        xlab = NULL, ylab = NULL, main = NULL,
        xlim = NULL, ylim = NULL, ...)
        
x, y	
the x and y co-ordinates for the centres of the symbols. They can be specified in any way which is accepted by xy.coords.

circles	
a vector giving the radii of the circles.

a<-c(1,2,3,4)
hist(a)
op<-par(fig=c(.02,.5,.5,.98),new=TRUE) #利用fig。一个数值向量，形式为c(x1, x2, y1, y2)，用于设定当前图形在绘图设备中所占区域，注意需要满足x1<x2,y1<y2。如果修改参数fig，会自动打开一个新的绘图设备，而若希望在原来的绘图设备中添加新的图形，需要和参数new=TRUE一起使用
boxplot(x)


correlation的图不能取两次log，脚本里是1为底的对数
60个药物作图所在位置
C cluster
/Share/home/wangdong/lss/project/20150430_sw_probes_screen/plot
A集群
./data/sw_20150110/SW_16_Samples_R/
所以根据自己数据的具体情况，初步定义为
log2(m[,i]/q[i]+1) q为含有每一列即每个药stable gene的中位值
find -name all_61samples_3.pdf
利用find -name来查找某个文件所在的位置

apply(m,1,sum,na.rm=TRUE)
R 语言中提供了四类有关统计分布的函数（密度函数，累计分布函数，分位函数，随机数函数）。分别在代表该分布的R函数前加上相应前缀获得 (d，p，q，r)。如正态分布的函数是norm，命令dnorm(0)就可以获得正态分布的密度函数在0处的值(0.3989)(默认为标准正态分 布)。同理pnorm(0)是0.5就是正态分布的累计密度函数在0处的值。而qnorm(0.5)则得到的是0，即标准正态分布在0.5处的分位数是 0（在来个比较常用的：qnorm(0.975)就是那个估计中经常用到的1.96了）。最后一个rnorm(n)则是按正态分布随机产生n个数据。上面 正态分布的参数平均值和方差都是默认的0和１，你可以通过在函数里显示指定这些参数对其进行更改。如dnorm(0,1,2)则得出的是均值为1，标准差 为2的正态分布在0处的概率值。要注意的是()内的顺序不能颠倒。
feelings<-c("sad","afraid","happy","angry")
for (i in feelings)
print(switch(i,happy="i am glad you are happy",afraid="there is nothing to fear",sad="cheer up",angry="calm down"))



bamToBed -i accepted_hits.bam | awk 'BEGIN{FS="\t";OFS="\t"}{if($1=="MT"){print "chrM",$2,$3,$4,$5,$6}else{print "chr"$1,$2,$3,$4,$5,$6}}' > accepted_hits.bed
makeTagDirectory tags2/ accepted_hits.bed -genome /Work1/home/wangdong/ensemble/genome.fa -sspe
removeOutOfBoundsReads.pl tags/ /Work1/home/wangdong/ensemble/genome.fa -chromSizes /Work1/home/wangdong/ensemble/hg19.chrom.sizes     
makeUCSCfile tags2 -o auto -bigWig 	 -norm 1e7 -strand both -fsize 1e20 -fragLength given > tags2/tags.trackInfo2.txt
可视化流程过程(swc2_swc3_swnc_swm1_swm2全按下面流程，在A集群上)
bamToBed -i /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_sh2/accepted_hits.bam > /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_sh2/accepted_hits.bed
makeTagDirectory /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_sh2/tags2/ /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_sh2/accepted_hits.bed -genome /Share/home/lulab1/users/liyang/project/Genome/human_hg19/bowtie2_index/human_genome_hg19.fa -sspe
bamToBed -i /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_sh3/accepted_hits.bam > /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_sh3/accepted_hits.bed
makeTagDirectory /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_sh3/tags/ /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_sh3/accepted_hits.bed -genome /Share/home/lulab1/users/liyang/project/Genome/human_hg19/bowtie2_index/human_genome_hg19.fa -sspe
removeOutOfBoundsReads.pl /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_sh3/tags/ /Share/home/lulab1/users/liyang/project/Genome/human_hg19/bowtie2_index/human_genome_hg19.fa -chromSizes /Share/home/wangdong/lss/hg19/hg19.chrom.sizes     
makeUCSCfile /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_sh3/tags/ -o auto -bigWig /Share/home/wangdong/lss/hg19/hg19.chrom.sizes -norm 1e8 -strand both -fsize 1e20 > /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_sh3/tags/tags.trackInfo.txt
bamToBed -i /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_NC/accepted_hits.bam > /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_NC/accepted_hits.bed
makeTagDirectory /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_NC/tags/ /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_NC/accepted_hits.bed -genome /Share/home/lulab1/users/liyang/project/Genome/human_hg19/bowtie2_index/human_genome_hg19.fa -sspe
removeOutOfBoundsReads.pl /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_NC/tags/ /Share/home/lulab1/users/liyang/project/Genome/human_hg19/bowtie2_index/human_genome_hg19.fa -chromSizes /Share/home/wangdong/lss/hg19/hg19.chrom.sizes     
makeUCSCfile /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_NC/tags/ -o auto -bigWig /Share/home/wangdong/lss/hg19/hg19.chrom.sizes -norm 1e8 -strand both -fsize 1e20 > /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_NC/tags/tags.trackInfo.txt
bamToBed -i /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/wqy_2M/accepted_hits.bam > /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/wqy_2M/accepted_hits.bed
makeTagDirectory /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/wqy_2M/tags/ /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/wqy_2M/accepted_hits.bed -genome /Share/home/lulab1/users/liyang/project/Genome/human_hg19/bowtie2_index/human_genome_hg19.fa -sspe
removeOutOfBoundsReads.pl /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/wqy_2M/tags/ /Share/home/lulab1/users/liyang/project/Genome/human_hg19/bowtie2_index/human_genome_hg19.fa -chromSizes /Share/home/wangdong/lss/hg19/hg19.chrom.sizes     
makeUCSCfile /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/wqy_2M/tags/ -o auto -bigWig /Share/home/wangdong/lss/hg19/hg19.chrom.sizes -norm 1e8 -strand both -fsize 1e20 > /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/wqy_2M/tags/tags.trackInfo.txt


张柳佳rnaseq重新分析
samtools sort -n /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_sh2/accepted_hits.bam /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_sh2/bamsorted
samtools view -h /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_sh2/bamsorted.bam > /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_sh2/pca3_sh2.sam
htseq-count -s no -t gene /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_sh2/pca3_sh2.sam /Share/home/wangdong/data/human_annotation/gencode19.lite.gtf > /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_sh2/pca3_sh2_2.count
samtools sort -n /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_sh3/accepted_hits.bam /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_sh3/bamsorted
samtools view -h /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_sh3/bamsorted.bam > /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_sh3/pca3_sh3.sam
htseq-count -s no -t gene /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_sh3/pca3_sh3.sam /Share/home/wangdong/data/human_annotation/gencode19.lite.gtf > /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_sh3/pca3_sh3.count
samtools sort -n /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_NC/accepted_hits.bam /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_NC/bamsorted
samtools view -h /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_NC/bamsorted.bam > /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_NC/pca3_NC.sam
htseq-count -s no -t gene /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_NC/pca3_NC.sam /Share/home/wangdong/data/human_annotation/gencode19.lite.gtf > /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_NC/pca3_NC.count

htseq-count -s yes /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_sh2/pca3_sh2.sam /Share/home/wangdong/data/human_annotation/gencode19.lite.gff > /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_sh2/pca3_sh2_3.count
htseq-count -s yes /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_sh3/pca3_sh3.sam /Share/home/wangdong/data/human_annotation/gencode19.lite.gff > /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_sh3/pca3_sh3.count
htseq-count -s yes /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_NC/pca3_NC.sam /Share/home/wangdong/data/human_annotation/gencode19.lite.gff > /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_NC/pca3_NC.count

数了后没有结果
正确做法
-s  yes两特异性的 pairend 很关键，-t gene不选择  默认是exon
htseq-count -s yes /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_sh2/pca3_sh2.sam /Share/home/wangdong/data/human_annotation/gencode19.lite.gff > /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_sh2/pca3_sh2_3.count
htseq-count -s yes /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_sh3/pca3_sh3.sam /Share/home/wangdong/data/human_annotation/gencode19.lite.gff > /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_sh3/pca3_sh3.count
htseq-count -s yes /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_NC/pca3_NC.sam /Share/home/wangdong/data/human_annotation/gencode19.lite.gff > /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_NC/pca3_NC.count

统计探针的组成情况
sort worked_termini.fa |uniq -c |sort -g -r -o worked_terminal_result.fa
sort acceptor_2bp_w.fa |uniq -c |sort -g -r -o acceptor_2bp_w_result.fa
sort donor_2bp_w.fa |uniq -c |sort -g -r -o donor_2bp_w_result.fa
sort acceptor_2bp_nw.fa |uniq -c |sort -g -r -o acceptor_2bp_nw_result.fa
sort donor_2bp_nw.fa |uniq -c |sort -g -r -o donor_2bp_nw_result.fa

uniq [选项] 文件

说明：这个命令读取输入文件，并比较相邻的行。在正常情况下，第二个及以后更多个重复行将被删去，行比较是根据所用字符集的排序序列进行的。该命令加工后的结果写到输出文件中。输入文件和输出文件必须不同。如果输入文件用“- ”表示，则从标准输入读取。

该命令各选项含义如下：、

C c 显示输出中，在每行行首加上本行在文件中出现的次数。它可取代- u和- d选项。

C d 只显示重复行。

C u 只显示文件中不重复的各行。


col=c("red","blue"),ylab="The Tm Value of probes",las=1,font.lab=2
作tm值的图
a<-read.table("worked_sw_probe_list_tm.txt",head=T,row.names=1)
a<-read.table("worked_sw_probe_list_tm.txt",head=T)
head(a)
b<-read.table("not_worked_sw_probe_list_tm.txt",head=T)
b<-read.table("not_worked_sw_probe_list_tm.txt",head=T)
b<-read.table("not_worked_sw_probe_list_tm.txt",head=T)
head(b)
pdf("Tm.pdf")
boxplot(a$Tm_a,a$Tm_d,b$Tm_a,b&Tm_d),col=c("red","blue"),ylab="The Tm Value of probes",las=1,font.lab=2))
boxplot(a$Tm_a,a$Tm_d,b$Tm_a,b&Tm_d,col=c("red","blue"),ylab="The Tm Value of probes",las=1,font.lab=2)
boxplot(a$Tm_a,a$Tm_d,b$Tm_a,b$Tm_d,col=c("red","blue"),ylab="The Tm Value of probes",las=1,font.lab=2)
dev.off()
history()

作表达值的图
a<-read.table("a.txt",head=T,row.names=1)
head(a)
b<-read.table("b.txt",head=T,row.names=1)
c<-read.table("c.txt",head=T,row.names=1)
pdf("sw_boxplot.pdf")
boxplot(a$MDA231,a$sw620,a$thp.1,a$hepg2,a$hp7702,b$MDA231,b$sw620,b$thp.1,b$hepg2,b$hp7702,c$MDA231,c$sw620,c$thp.1,c$hepg2,c$hp7702,col=c("mediumturquoise","pink","red"),ylab="normalized Indensity Values",las=1,font.lab=2)
dev.off()
pdf("sw_boxplot2.pdf")
boxplot(a$MDA231,a$sw620,a$thp.1,a$hepg2,a$hp7702,b$MDA231,b$sw620,b$thp.1,b$hepg2,b$hp7702,c$MDA231,c$sw620,c$thp.1,c$hepg2,c$hp7702,col=c("mediumturquoise","pink","red","yellow","blue"),ylab="normalized Indensity Values",las=1,font.lab=2)dev.off()
history()

source("http://bioconductor.org/biocLite.R")
microarry normalization

biocLite("preprocessCore")
library(preprocessCore)

data<-read.table("four_cell_line.txt",header=T,row.names=1)
head(data);dim(data)
data.<-normalize.quantiles(as.matrix(data))

par(mfrow=c(1,2))
boxplot(log2(data))
boxplot(log2(data.quantile))

boxplot(finaldata,col=c("mediumturquoise"),ylab="normalized Indensity Values",las=1,font.lab=2)
pdf("cutoff20_exon.pdf")
boxplot(b$worked,a$not_worked,col=c("mediumturquoise"),ylab="The distribution of gene exon number",las=1,font.lab=2)
boxplot(x1,x2,x3,xlim=c(0,4),ylim=c(0,15),main="白鼠伤寒杆菌实验分析",xlim=c(0,4),ylim=c(0,15),xlab="实验菌种",ylab="平均存活天数",pch=19,col=c("red","green","blue"),xaxs="i",yaxs="i",names=c('菌种一','菌种二','菌种三'))

ttest(array1,array2,2,3)双尾（可比对照大可小），方差不等
A集群提交任务
$bqueues
$bjobs
bkill
$perl bsub.pl bamtobed.sh 1 TINY ./source.txt

.rar解压
安装rarosx-5.2.1.tar.gz

例2：解压缩abc.rar档案中的内容，可以使用e或x命令,假设abc.rar目录中有一个名为file1的文件和桓雒为test的目录，test目录中有一个名为file2的文件，
$rar e abc.rar

说明：使用e命令，会将abc.rar中的file1文件连同test目录下的file2文件解压到当前目录。如果想保持abc.rar目录中的目录结构请使用x命令。
$rar x abc.rar


bamToBed -i /Work1/home/wangdong/lss/zlj_rnaseq/zlj_sh2/accepted_hits.bam > /Work1/home/wangdong/lss/zlj_rnaseq/bamtobed/zlj_sh2.bed
bamToBed -i /Work1/home/wangdong/lss/zlj_rnaseq/zlj_sh3/accepted_hits.bam > /Work1/home/wangdong/lss/zlj_rnaseq/bamtobed/zlj_sh3.bed
bamToBed -i /Work1/home/wangdong/lss/zlj_rnaseq/zlj_NC/accepted_hits.bam > /Work1/home/wangdong/lss/zlj_rnaseq/bamtobed/zlj_NC.bed
bamToBed -i /Work1/home/wangdong/lss/zlj_rnaseq/wqy_2M/accepted_hits.bam > /Work1/home/wangdong/lss/zlj_rnaseq/bamtobed/wqy_2M.bed


makeTagDirectory /Work1/home/wangdong/lss/zlj_rnaseq/maketagdir/maketags_zlj_sh2/ /Work1/home/wangdong/lss/zlj_rnaseq/bamtobed/zlj_sh2.bed -genome /Work1/home/wangdong/ensemble/genome.fa -sspe
makeTagDirectory /Work1/home/wangdong/lss/zlj_rnaseq/maketagdir/maketags_zlj_sh3/ /Work1/home/wangdong/lss/zlj_rnaseq/bamtobed/zlj_sh3.bed -genome /Work1/home/wangdong/ensemble/genome.fa -sspe
makeTagDirectory /Work1/home/wangdong/lss/zlj_rnaseq/maketagdir/maketags_zlj_NC/ /Work1/home/wangdong/lss/zlj_rnaseq/bamtobed/zlj_NC.bed -genome /Work1/home/wangdong/ensemble/genome.fa -sspe
makeTagDirectory /Work1/home/wangdong/lss/zlj_rnaseq/maketagdir/maketags_wqy_2M/ /Work1/home/wangdong/lss/zlj_rnaseq/bamtobed/wqy_2M.bed -genome /Work1/home/wangdong/ensemble/genome.fa -sspe

removeOutOfBoundsReads.pl /Work1/home/wangdong/lss/zlj_rnaseq/maketagdir/maketags_zlj_sh2/ /Work1/home/wangdong/ensemble/genome.fa -chromSizes /Work1/home/wangdong/ensemble/hg19.chrom.sizes
removeOutOfBoundsReads.pl /Work1/home/wangdong/lss/zlj_rnaseq/maketagdir/maketags_zlj_sh3/ /Work1/home/wangdong/ensemble/genome.fa -chromSizes /Work1/home/wangdong/ensemble/hg19.chrom.sizes
removeOutOfBoundsReads.pl /Work1/home/wangdong/lss/zlj_rnaseq/maketagdir/maketags_zlj_NC/ /Work1/home/wangdong/ensemble/genome.fa -chromSizes /Work1/home/wangdong/ensemble/hg19.chrom.sizes
removeOutOfBoundsReads.pl /Work1/home/wangdong/lss/zlj_rnaseq/maketagdir/maketags_wqy_2M/ /Work1/home/wangdong/ensemble/genome.fa -chromSizes /Work1/home/wangdong/ensemble/hg19.chrom.sizes

makeUCSCfile /Work1/home/wangdong/lss/zlj_rnaseq/maketagdir/maketags_zlj_sh2/ -o auto -bigWig /Work1/home/wangdong/ensemble/hg19.chrom.sizes -norm 1e7 -strand both -fsize 1e20 > /Work1/home/wangdong/lss/zlj_rnaseq/maketagdir/maketags_zlj_sh2/tags.trackInfo.txt
makeUCSCfile /Work1/home/wangdong/lss/zlj_rnaseq/maketagdir/maketags_zlj_sh3/ -o auto -bigWig /Work1/home/wangdong/ensemble/hg19.chrom.sizes -norm 1e7 -strand both -fsize 1e20 > /Work1/home/wangdong/lss/zlj_rnaseq/maketagdir/maketags_zlj_sh3/tags.trackInfo.txt
makeUCSCfile /Work1/home/wangdong/lss/zlj_rnaseq/maketagdir/maketags_zlj_NC/ -o auto -bigWig /Work1/home/wangdong/ensemble/hg19.chrom.sizes -norm 1e7 -strand both -fsize 1e20 > /Work1/home/wangdong/lss/zlj_rnaseq/maketagdir/maketags_zlj_NC/tags.trackInfo.txt
makeUCSCfile /Work1/home/wangdong/lss/zlj_rnaseq/maketagdir/maketags_wqy_2M/ -o auto -bigWig /Work1/home/wangdong/ensemble/hg19.chrom.sizes -norm 1e7 -strand both -fsize 1e20 > /Work1/home/wangdong/lss/zlj_rnaseq/maketagdir/maketags_wqy_2M/tags.trackInfo.txt

从maketag开始，重新做makeUCSCfile，生成bedgraph,而做可视化是用bigwig
makeTagDirectory /Work1/home/wangdong/lss/zlj_rnaseq/maketagdir/maketags_zlj_sh2_2/ /Work1/home/wangdong/lss/zlj_rnaseq/bamtobed/zlj_sh2.bed -genome /Work1/home/wangdong/ensemble/genome.fa -sspe
makeTagDirectory /Work1/home/wangdong/lss/zlj_rnaseq/maketagdir/maketags_zlj_sh3_2/ /Work1/home/wangdong/lss/zlj_rnaseq/bamtobed/zlj_sh3.bed -genome /Work1/home/wangdong/ensemble/genome.fa -sspe
makeTagDirectory /Work1/home/wangdong/lss/zlj_rnaseq/maketagdir/maketags_zlj_NC_2/ /Work1/home/wangdong/lss/zlj_rnaseq/bamtobed/zlj_NC.bed -genome /Work1/home/wangdong/ensemble/genome.fa -sspe
makeTagDirectory /Work1/home/wangdong/lss/zlj_rnaseq/maketagdir/maketags_wqy_2M_2/ /Work1/home/wangdong/lss/zlj_rnaseq/bamtobed/wqy_2M.bed -genome /Work1/home/wangdong/ensemble/genome.fa -sspe
makeUCSCfile /Work1/home/wangdong/lss/zlj_rnaseq/maketagdir/maketags_zlj_sh2_2/ -fragLength given -o auto
makeUCSCfile /Work1/home/wangdong/lss/zlj_rnaseq/maketagdir/maketags_zlj_sh3_2/ -fragLength given -o auto
makeUCSCfile /Work1/home/wangdong/lss/zlj_rnaseq/maketagdir/maketags_zlj_NC_2/ -fragLength given -o auto
makeUCSCfile /Work1/home/wangdong/lss/zlj_rnaseq/maketagdir/maketags_wqy_2M_2/ -fragLength given -o auto

analyzeRepeats.pl rna hg19 -strand both -condenseGenes -count exons -d /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/result/maketags_zlj_sh2_2 /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/result/maketags_zlj_sh3_2 /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/result/maketags_zlj_NC_2 -rpkm > /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/result/condensed_rpkm.txt
analyzeRepeats.pl rna hg19 -strand both -count exons -condenseGenes -d /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/result/maketags_zlj_sh2_2 /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/result/maketags_zlj_sh3_2 /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/result/maketags_zlj_NC_2 -noadj > /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/result/shRNA1_shRNA2_NC_raw.txt
getDiffExpression.pl /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/result/shRNA1_shRNA2_NC_raw.txt shRNA shRNA NC -repeats > /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/result/shRNA1_shRNA2_NC_diffOutput.txt
[wangdong@cluster ~/lss/project/zlj_pcat1_RNASEQ/result]$getDiffExpression.pl /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/result/shRNA1_shRNA2_NC_raw.txt shRNA shRNA NC -repeats > /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/result/shRNA1_shRNA2_NC_diffOutput.txt

   
        Treating input as file generated by analyzeRepeats.pl (-repeats)
        Using edgeR to calculate differential expression/enrichment...
Loading required package: limma

可以解决batch effect 也可以调用deseq

By default the program calls on EdgeR to perform the differential expression calculations.  In the experiment has no replication, you may want to set the dispersion by using "-dispersion <#>" (default is 0.05).  To use DESeq instead of EdgeR, specify "-DESeq".

If your samples are paired in anyway you may want to try to account for batch effects.  EdgeR allows you to apply a generalized model to try to remove effects caused by analyzing data on a different day or slightly different batch.  For example, lets say you did an experiment with a control and a drug treatment, then did a second experiment two weeks later with a different library preparation protocol, etc.  You can tell the program that the samples are linked by specifying a batch code - the code is like the experiment annotation in that it applies to each experiment in the same order that they were listed when preparing the gene expression matrix:
analyzeRepeats.pl rna hg19 -noadj -d Week1-Ctrl/ Week1-Drug/ Week2-Ctrl/ Week2-Drug/ > output.txt
getDiffExpression.pl output.txt -repeats ctrl drug ctrl drug -batch 1 1 2 2 > diffOutput.txt

到c集群，因为a上没有homer的基因组
下面脚本报错
analyzeRepeats.pl rna hg19 -strand both -condenseGenes -count exons -d /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mktagsdir/maketags_zlj_sh2_2/ /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mktagsdir/maketags_zlj_sh3_2/ /Work1/home/wangdong/lss/zlj_rnaseq/maketagdir/maketags_zlj_NC_2/ -rpkm > /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mktagsdir/result/condensed_rpkm.txt
单个做依旧报错
analyzeRepeats.pl rna hg19 -strand both -condenseGenes -count exons -d /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mktagsdir/maketags_zlj_sh2_2/ /Work1/home/wangdong/lss/zlj_rnaseq/maketagdir/maketags_zlj_NC_2/ -rpkm > /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mktagsdir/result/condensed_rpkm_sh2_nc.txt

analyzeRepeats.pl rna hg19 -strand both -count exons -condenseGenes -d /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mktagsdir/maketags_zlj_sh2_2/ /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mktagsdir/maketags_zlj_sh3_2/ /Work1/home/wangdong/lss/zlj_rnaseq/maketagdir/maketags_zlj_NC_2/ -noadj > ./shRNA1_shRNA2_NC_raw.txt
Use of uninitialized value in addition (+) at /Share/home/wangdong/packages/homer/bin/analyzeRepeats.pl line 612.
Use of uninitialized value in addition (+) at /Share/home/wangdong/packages/homer/bin/analyzeRepeats.pl line 613.
Use of uninitialized value in addition (+) at /Share/home/wangdong/packages/homer/bin/analyzeRepeats.pl line 614.
最终解决
maketagdirectory一步开始重新做，可能原因是这一步像上面用的李洋下面的hg19，而在analyzeRepeats一步却用的homer自己的，若不提供，调用homer自己的
makeTagDirectory /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/result/maketags_zlj_sh2 /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_sh2/accepted_hits.bam -sspe
makeTagDirectory /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/result/maketags_zlj_sh3 /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_sh3/accepted_hits.bam -sspe
makeTagDirectory /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/result/maketags_zlj_NC /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_NC/accepted_hits.bam -sspe
makeUCSCfile /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/result/maketags_zlj_sh2_2 -fragLength given -o auto
makeUCSCfile /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/result/maketags_zlj_sh3_2 -fragLength given -o auto
makeUCSCfile /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/result/maketags_zlj_NC_2 -fragLength given -o auto
analyzeRepeats.pl rna hg19 -strand both -condenseGenes -count exons -d /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/result/maketags_zlj_sh2_2 /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/result/maketags_zlj_sh3_2 /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/result/maketags_zlj_NC_2 -rpkm > /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/result/condensed_rpkm.txt


zip文件大于4g不可以用unzip .gz
Zip file too big (greater than 4294959102 bytes)
if you've got Java on the box, you can use
 -x[file] 从标准输入提取所有文件，或只提取指定的文件。如果省略了file，则提取所有文件；否则只提取指定文件。
 -f 第二个参数指定要处理的jar文件(文件列表中的第一个元素是要创建或访问的存档文件名字)。在-c(创建)情形中，第二个参数指的是要创建的jar文件的名称(不是在标准输出上)。在-t(表(或-x(抽取)这两种情形中，第二个参数指定要列出或抽取的jar文件。
 
利用  jar解压
jar xf test.zip
也可以进行压缩
4: “jar Ccvf m n”    ：将文件打包成jar压缩包

                            m: 要生成jar包的名字

                            n: 要压缩文件的文件名（可以是多个文件或一个目录）

生成的jar文件实际上就是一个普通的zip压缩文件

xxx批量在r中做图脚本
args=commandArgs( T )
pdf( 'tmp.pdf' )
layout( mat=matrix(c( 1:12 ),nrow=6,ncol=2),heights=c(3,1,3,1,3,1),widths=c( 1,1 ) )
Box=read.table( args[ 1 ],sep='\t',fill=T )
Files=strsplit(args[ 2 ],',')[[1]]
for ( Fi in 1:length(Files) ){
    Data=read.table( Files[ Fi ],sep='\t',row.names=1 )
    Mean=colMeans( Data )
    par( mar=c( 1,4,2,1 ) )
    plot( 1:150,Mean,pch='.',col='red',xaxt='n',xlab=NULL,ylab='Mean Methylation Level',main=paste( 'cluster',as.character( Fi-1 ),sep='_' ),ylim=c( 0,0.05 ) )
    lines( 1:150,Mean,col='red',lwd=2 )
    axis( side=1,at=c( 1,26,125,150 ),labels=c( '-2.5k','TSS','TTS','+2.5k' ) )
    abline( v=26,lty=2,col='grey' )
    abline( v=125,lty=2,col='grey' )
    par( mar=c( 2,4,1,1 ) )
    boxplot(t( Box[ Fi, ] ),horizontal=T,notch=T,ylim=c( 0,15 ),ylab='log2RNA')
}
dev.off(  )


python操作excel
import xlrd
data = xlrd.open_workbook('1274_final_table.xlsx')
table = data.sheet_by_index(0)
nrows = table.nrows
ncols = table.ncols
m=table.row_values(1)
n=table.col_values(1)
求一列的和
sum(n[1:])


方式一:try语句:

1使用try和except语句来捕获异常

try:
   block
except [exception,[data…]]:
   block

try:
block
except [exception,[data...]]:
   block
else:
   block

该种异常处理语法的规则是：

・   执行try下的语句，如果引发异常，则执行过程会跳到第一个except语句。

・   如果第一个except中定义的异常与引发的异常匹配，则执行该except中的语句。

・   如果引发的异常不匹配第一个except，则会搜索第二个except，允许编写的except数量没有限制。

・   如果所有的except都不匹配，则异常会传递到下一个调用本代码的最高层try代码中。

・   如果没有发生异常，则执行else块代码。

例:

try:

   f = open(“file.txt”,”r”)
except IOError, e:
   print e

捕获到的IOError错误的详细原因会被放置在对象e中,然后运行该异常的except代码块

捕获所有的异常

try:
   a=b
   b=c
except Exception,ex:
   print Exception,":",ex

使用except子句需要注意的事情，就是多个except子句截获异常时，如果各个异常类之间具有继承关系，则子类应该写在前面，否则父类将会直接截获子类异常。放在后面的子类异常也就不会执行到了。

2 使用try跟finally:

return语句用来从一个函数 返回 即跳出函数。我们也可选从函数 返回一个值 。

python中if __name__ == '__main__': 的解析

当你打开一个.py文件时,经常会在代码的最下面看到if __name__ == '__main__':,现在就来介 绍一下它的作用.

        模块是对象，并且所有的模块都有一个内置属性 __name__。一个模块的 __name__ 的值取决于您如何应用模块。如果 import 一个模块，那么模块__name__ 的值通常为模块文件名，不带路径或者文件扩展名。但是您也可以像一个标准的程序样直接运行模块，在这 种情况下, __name__ 的值将是一个特别缺省"__main__"。

///////////////////////////////////////////////////////////////////////////////////////////////////

在cmd 中直接运行.py文件,则__name__的值是'__main__';

而在import 一个.py文件后,__name__的值就不是'__main__'了;

从而用if __name__ == '__main__'来判断是否是在直接运行该.py文件

如:

#Test.py

class Test:

    def __init(self):pass

    def f(self):print 'Hello, World!'

if __name__ == '__main__':

    Test().f()

#End

 

你在cmd中输入:

C:>python Test.py

Hello, World!

说明:"__name__ == '__main__'"是成立的

 

你再在cmd中输入:

C:>python

>>>import Test

>>>Test.__name__                #Test模块的__name__

'Test'

>>>__name__                       #当前程序的__name__

'__main__'

无论怎样,Test.py中的"__name__ == '__main__'"都不会成立的!

所以,下一行代码永远不会运行到!



python xlrd安装，可读取excel（Mac 终端里）
 513  cd Downloads/
  514  ls
  515  cd xlrd-0.9.3
  516  ls
  517  cd ..
  518  ls
  519  chmod 777 xlrd-0.9.3
  520  cd xlrd-0.9.3
  521  ls
  522  python setup.py install
  523  sudo python setup.py install
  

IndentationError: unexpected indent
python缩进问题
IndentationError: unexpected indent
xlwt安装，python中excel写
安装过程如下自己Mac下安装
  cd setuptools-18.0.1 #可以安装许多python库了
  623  sudo python setup.py install

  634  cd pip-7.0.3/ 安装xlwt需要？也许有setuptools就可以了
  637  sudo python setup.py install
  649  cd xlwt-1.0.0
  652  sudo pip install xlwt （没起作用，直接用 python setup.py install即可以安装了）
  653  sudo python setup.py install
  654  python
  
 在python中写excel表要用到xlwt[1] 模块，大致使用流程如下：
1、导入模块
import xlwt
2、创建workbook（其实就是excel，后来保存一下就行）
workbook = xlwt.Workbook(encoding = 'ascii')
3、创建表
worksheet = workbook.add_sheet('My Worksheet')
4、往单元格内写入内容
worksheet.write(0, 0, label = 'Row 0, Column 0 Value')
5、保存
workbook.save('Excel_Workbook.xls')
以上是最基本的功能。
ef write_excel():
    f = xlwt.Workbook() #创建工作簿

    '''
    创建第一个sheet:
        sheet1
    '''
    sheet1 = f.add_sheet(u'sheet1',cell_overwrite_ok=True) #创建sheet
    row0 = [u'业务',u'状态',u'北京',u'上海',u'广州',u'深圳',u'状态小计',u'合计']
    column0 = [u'机票',u'船票',u'火车票',u'汽车票',u'其它']
    status = [u'预订',u'出票',u'退票',u'业务小计']

    #生成第一行
    for i in range(0,len(row0)):
        sheet1.write(0,i,row0[i],set_style('Times New Roman',220,True))
        

bowtie -n 3 /Share/home/wangdong/lss/project/tmp/yy/first/mapping/ref_plus /Share/home/wangdong/lss/project/tmp/yy/first/Input_WFY-2/WYF-2_CGATGT_L007_R2_001_21nt_plus.fastq -S  /Share/home/wangdong/lss/project/tmp/yy/first/mapping/WYF-2_CGATGT_L007_R2_001_21nt_plus.sam
bowtie -n 3 /Share/home/wangdong/lss/project/tmp/yy/first/mapping/ref_plus /Share/home/wangdong/lss/project/tmp/yy/first/TFH_WFY-4/WYF-4_TGACCA_L007_R2_001_21nt_plus.fastq -S  /Share/home/wangdong/lss/project/tmp/yy/first/mapping/WYF-4_TGACCA_L007_R2_001_21nt_plus.sam
bowtie -n 3 /Share/home/wangdong/lss/project/tmp/yy/first/mapping/ref_plus /Share/home/wangdong/lss/project/tmp/yy/first/TH1_WFY-3/WYF-3_TTAGGC_L007_R2_001_21nt_plus.fastq -S  /Share/home/wangdong/lss/project/tmp/yy/first/mapping/WYF-3_TTAGGC_L007_R2_001_21nt_plus.sam
bowtie -n 3 /Share/home/wangdong/lss/project/tmp/yy/first/mapping/ref_rc /Share/home/wangdong/lss/project/tmp/yy/first/Input_WFY-2/WYF-2_CGATGT_L007_R2_001_21nt_rc.fastq -S  /Share/home/wangdong/lss/project/tmp/yy/first/mapping/WYF-2_CGATGT_L007_R2_001_21nt_rc.sam
bowtie -n 3 /Share/home/wangdong/lss/project/tmp/yy/first/mapping/ref_rc /Share/home/wangdong/lss/project/tmp/yy/first/TFH_WFY-4/WYF-4_TGACCA_L007_R2_001_21nt_rc.fastq -S  /Share/home/wangdong/lss/project/tmp/yy/first/mapping/WYF-4_TGACCA_L007_R2_001_21nt_rc.sam
bowtie -n 3 /Share/home/wangdong/lss/project/tmp/yy/first/mapping/ref_rc /Share/home/wangdong/lss/project/tmp/yy/first/TH1_WFY-3/WYF-3_TTAGGC_L007_R2_001_21nt_rc.fastq -S  /Share/home/wangdong/lss/project/tmp/yy/first/mapping/WYF-3_TTAGGC_L007_R2_001_21nt_rc.sam



cmap 密码和账号
lss
kaxmls0426

	自动化流程
0.或者利用extract_sample_with_barcode.py将.R直接提取出来
extract_sample_with_barcode.py
1.解压，合并fastq
unzip_after_extract_for_each_barcode.py
cat_after_extract_for_each_barcode.py
2.mapping
mapping.py
python /Share/home/wangdong/lss/project/compand_60/mapping/tmp/tmp_sub/mapping.py (写到mapping_rasl_test.sh里面，将这个提交即可)
3.sam获得统计结果
count_sam.py
4.将mapping的细节每个探针的值合并为一个大的excel表格
state_rasl_result_final.py
5.获得bowtie mapping ratio，将屏幕上打印出来的结果直接复制，或者将mapping.py放在一个.sh里面提交，即可将结果输出到log里面，利用下面脚本获得想要的mapping情况和raw reads
get_the_mapping_ratio.py（本地运行即可）
或者利用r

tabel_result<-read.table("final_result_table3.txt",head=T,row.names=1)
sum1<-apply(tabel_result,2,sum)
sum2<-t(sum1)
write.table(sum2,"sum.txt")

6.wc -l统计探针检测到值>1的个数
wc -l *.sam_3_result.fa >> result2.fa
所有以上结果均可以进行放在.sh里面提交

6.5去除stable median<5的列（此时的median没有将0全变为1）
stable<-read.table("stable_1.txt",head=T,row.names=1)
算好的各列的median当作一行，放入最后一行，进行转置将化合物名变成行
total<-read.table("total_1.txt",head=T,row.names=1)
ttoal<-t(total)
dim(ttoal)
row.names(ttoal)
ttoal
write.table(ttoal,'total_new_withme.txt')
写出来后进行将转置导致的第一行不是名字的去掉，让探针名和median上前成为title，然后就可以利用Median这一列进行过滤了
new1<-read.table('total_new_withme.txt',head=T,row.names=1)
dim(new1)
new<-subset(new1,Median>5)
dim(new)
tnew=t(new)
write.table(tnew,"new_filter_total.txt")

＃改进后的版本
abnormal_samples_filtering.r
stable<-read.table("stable_1.txt",head=T,row.names=1)
Median<-apply(stable,2,median)
total<-read.table("total_1.txt",head=T,row.names=1)
total<-rbind(total,Median)
rownames(total[nrow(total),])<-c("Median")
new1<-t(total)
new<-subset(new1,Median>5)
dim(new)
tnew=t(new)
write.table(tnew,"new_filter_total.txt")

7.normalizastion R.script
#RASL_Seq_normalize.r
stable<-read.table("stable.txt",head=T,row.names=1)
total<-read.table("total.txt",head=T,row.names=1)
me<-apply(stable,2,median)
for (i in 1:2704){
    total[,i]=log2(total[,i]/(me[i]+1)+1)
}
write.table(total,"normalized_by_median_of_stable_genes2.txt")
或者

stable<-read.table("stable_1.txt",head=T,row.names=1)
total<-read.table("total_1.txt",head=T,row.names=1)
me<-apply(stable,2,median)
head(me)
options(digits =2 )
for (i in 1:2564){
    total[,i]=100*total[,i]/me[i]
}
round(total,digits=2)
write.table(total,"normalized_by_median100_3.txt")

将rasl结果进行统计
 
8.计算每一列fc，但是将4个dmso变化来
#get_fc_treat_vs_dmso.r
最终用方法
treat<-read.table("lane7_plate1_treat.txt",head=T,row.names=1)
control<-read.table("lane7_plate1_dmso.txt",head=T,row.names=1)
treat<-(log2(treat/as.matrix(control[,1]))+log2(treat/as.matrix(control[,2]))+log2(treat/as.matrix(control[,3]))+log2(treat/as.matrix(control[,4]))+log2(treat/as.matrix(control[,5])))/5
write.table(treat,"lane7_plate1_fc.txt")

treat<-read.table("lane7_plate2_treat.txt",head=T,row.names=1)
control<-read.table("lane7_plate2_dmso.txt",head=T,row.names=1)
treat<-(log2(treat/as.matrix(control[,1]))+log2(treat/as.matrix(control[,2]))+log2(treat/as.matrix(control[,3]))+log2(treat/as.matrix(control[,4])))/4
write.table(treat,"lane7_plate2_fc.txt")

treat<-read.table("lane7_plate3_treat.txt",head=T,row.names=1)
control<-read.table("lane7_plate3_dmso.txt",head=T,row.names=1)
treat<-(log2(treat/as.matrix(control[,1]))+log2(treat/as.matrix(control[,2]))+log2(treat/as.matrix(control[,3]))+log2(treat/as.matrix(control[,4]))+log2(treat/as.matrix(control[,5])))/5
write.table(treat,"lane7_plate3_fc.txt")

treat<-read.table("lane7_plate4_treat.txt",head=T,row.names=1)
control<-read.table("lane7_plate4_dmso.txt",head=T,row.names=1)
treat<-(log2(treat/as.matrix(control[,1]))+log2(treat/as.matrix(control[,2]))+log2(treat/as.matrix(control[,3]))+log2(treat/as.matrix(control[,4]))+log2(treat/as.matrix(control[,5])))/5
write.table(treat,"lane7_plate4_fc.txt")

treat<-read.table("lane8_plate2_treat.txt",head=T,row.names=1)
control<-read.table("lane8_plate2_dmso.txt",head=T,row.names=1)
treat<-(log2(treat/as.matrix(control[,1]))+log2(treat/as.matrix(control[,2]))+log2(treat/as.matrix(control[,3]))+log2(treat/as.matrix(control[,4]))+log2(treat/as.matrix(control[,5])))/5
write.table(treat,"lane8_plate2_fc_3.txt")

treat<-read.table("lane8_plate3_treat.txt",head=T,row.names=1)
control<-read.table("lane8_plate3_dmso.txt",head=T,row.names=1)
treat<-(log2(treat/as.matrix(control[,1]))+log2(treat/as.matrix(control[,2]))+log2(treat/as.matrix(control[,3]))+log2(treat/as.matrix(control[,4])))/4
write.table(treat,"lane8_plate3_fc.txt")

treat<-read.table("lane8_plate4_treat.txt",head=T,row.names=1)
control<-read.table("lane8_plate4_dmso.txt",head=T,row.names=1)
treat<-(log2(treat/as.matrix(control[,1]))+log2(treat/as.matrix(control[,2]))+log2(treat/as.matrix(control[,3]))+log2(treat/as.matrix(control[,4])))/4
write.table(treat,"lane8_plate4_fc_4.txt")


total<-read.table("Lane7_plate1_D7_control.txt",head=T,row.names=1)
control<-total[1:5]
treat<-total[6:384]
for (i in 1:1273){
    control[i,]=log2(treat[i,]/control[i,])
write.table(control,"treatvs_control_log2.txt")
新编融合fc为一个

以前方法
total<-read.table("Lane7_plate1_D7_control.txt",head=T,row.names=1)
control<-total[1:5]
for (j in 1:2705){
    treat<-total[,j]
    for (i in 1:1273){
        control[i,]=log2(treat[i,]/control[i,])
        }
    total[,j]<-apply(control,1,mean)
}
write.table(control,"treatvs_control_log2.txt")

9.将大excel表格按药进行拆分得到各自的txt
> a<-read.table("lane7_plate1_fc_test.txt",head=T,row.names=1)
> dim(a)
[1] 1273  375
> write.table(a,"lane7_plate1_fc_test2.txt",sep="\t")
先在r里面改变分隔符, 不要列标题，因为.rnk里直接是以基因开始,可以有标题，中间要有‘\t’隔开
python
split_excel_to_files_by_compound.py
paste.py
（
cut -f 1 total2.txt > name.txt
for i in 
cut -f 3 total2.txt > test_col3.txt
paste test.txt test_col3.txt > col3.txt
cut -f 1 total2.txt > test.txt
cut -f 1 test.txt
paste file1 file2 > file3

 paste [-d] file1 file2
选项与参数：
-d  ：后面可以接分隔字符。默认是以 [tab] 来分隔的！通过交换文件名即可指定哪一列先粘：）

#有时候去掉差异是0的基因，还是要对.rnk作以下处理
remove_abnormal_fc.py
remove_abnormal_fc_excute.py


10.安装gsea 命令行的软件，直接下载gsea2-2.2.0.jar，然后告诉路径在哪儿即可，如下面的命令（适合自己的）
例子
java -cp /Users/lishasha/Desktop/home/tools/gsea2-2.2.0.jar -Xmx512m xtools.gsea.GseaPreranked -gmx /Users/lishasha/Desktop/home/new_work/20150801_3000drugs/gsea/lung.gmt -collapse false -mode Max_probe -norm meandiv -nperm 1000 -rnk /Users/lishasha/Desktop/home/new_work/20150801_3000drugs/gsea/fc.rnk -scoring_scheme classic -rpt_label lung -include_only_symbols true -make_sets true -plot_top_x 20 -rnd_seed timestamp -set_max 500 -set_min 0 -zip_report false -out /Users/lishasha/gsea_home/output -gui false
正式写入python的脚本
java -cp /Users/lishasha/Desktop/home/tools/gsea2-2.2.0.jar -Xmx512m xtools.gsea.GseaPreranked -gmx /Users/lishasha/Desktop/home/new_work/20150801_3000drugs/gsea/plan1/again/new/fc/run_gsea/CTNNB1_UP.gmt -collapse false -mode Max_probe -norm meandiv -nperm 1000 -rnk /Users/lishasha/Desktop/home/new_work/20150801_3000drugs/gsea/fc.rnk{} -scoring_scheme classic -rpt_label {*} -include_only_symbols true -make_sets true -plot_top_x 20 -rnd_seed timestamp -set_max 500 -set_min 0 -zip_report false -out /Users/lishasha/gsea_home/output/gsea -gui false
c集群上
java -cp /Share/home/wangdong/local/app/gsea/gsea2-2.2.0.jar -Xmx512m xtools.gsea.GseaPreranked -gmx /Users/lishasha/Desktop/home/new_work/20150801_3000drugs/gsea/plan1/again/new/fc/run_gsea/CTNNB1_UP.gmt -collapse false -mode Max_probe -norm meandiv -nperm 1000 -rnk /Users/lishasha/Desktop/home/new_work/20150801_3000drugs/gsea/fc.rnk{} -scoring_scheme classic -rpt_label {*} -include_only_symbols true -make_sets true -plot_top_x 20 -rnd_seed timestamp -set_max 500 -set_min 0 -zip_report false -out /Users/lishasha/gsea_home/output/gsea -gui false
python /Share/home/wangdong/lss/script/run_gsea.py
AttributeError: 'tuple' object has no attribute 'write'
A集群
perl bsub.pl one.sh 1 TINY ./source.txt
perl bsub.pl f500.sh 1 TINY ./source.txt
perl bsub.pl f1001.sh 1 TINY ./source.txt
perl bsub.pl f1501.sh 1 TINY ./source.txt
perl bsub.pl f2000.sh 1 TINY ./source.txt

11.提取出gsea里的数据，可以分成多份运行
在a集群上可以运行40份没有问题
 for k in {1..40}; do python /Work1/home/wangdong/lss/20150801compoud/run_gsea.py & done
 python /Work1/home/wangdong/lss/20150801compoud/lung/run_gsea_up.py
for k in {1..40}; do python /Work1/home/wangdong/lss/20150801compoud/lung/run_gsea_up.py & done
for k in {1..40}; do python /Work1/home/wangdong/lss/20150801compoud/lung/run_gsea_down.py & done
lung_up.sh
perl bsub.pl lung_up.sh 1 TINY ./source.txt
perl bsub.pl lung_down.sh 1 TINY ./source.txt

python /Work1/home/wangdong/lss/20150801compoud/lung/lung_down_one.py
python /Work1/home/wangdong/lss/20150801compoud/lung/lung_down_500.py
python /Work1/home/wangdong/lss/20150801compoud/lung/lung_down_2000.py
python /Work1/home/wangdong/lss/20150801compoud/lung/lung_down_1500.py
python /Work1/home/wangdong/lss/20150801compoud/lung/lung_down_1000.py
python /Work1/home/wangdong/lss/20150801compoud/lung/lung_up_1000.py
python /Work1/home/wangdong/lss/20150801compoud/lung/lung_up_1500.py
python /Work1/home/wangdong/lss/20150801compoud/lung/lung_up_2000.py
python /Work1/home/wangdong/lss/20150801compoud/lung/lung_up_500.py
python /Work1/home/wangdong/lss/20150801compoud/lung/lung_up_one.py


perl bsub.pl lung_down_one.sh 1 TINY ./source.txt
perl bsub.pl lung_down_500.sh 1 TINY ./source.txt
perl bsub.pl lung_down_2000.sh 1 TINY ./source.txt
perl bsub.pl lung_down_1500.sh 1 TINY ./source.txt
perl bsub.pl lung_down_1000.sh 1 TINY ./source.txt
perl bsub.pl lung_up_1000.sh 1 TINY ./source.txt
perl bsub.pl lung_up_1500.sh 1 TINY ./source.txt
perl bsub.pl lung_up_2000.sh 1 TINY ./source.txt
perl bsub.pl lung_up_500.sh 1 TINY ./source.txt	
perl bsub.pl lung_up_one.sh 1 TINY ./source.txt

FOR capitalbio data
挑选出30个stable gene, 15个样本，挑选>10的
#mapping上reads作图
hist(a$differential,breaks=6,col="gray",xlab="log10(number of aligned reads)",main="The distribution of number of aligned reads")
＃运行结束后提取结果成excel表格
get_gsea_result_test.py

6.5去除stable median<5的列（此时的median没有将0全变为1）
stable<-read.table("stable.txt",head=T,row.names=1)
me<-apply(stable,2,median)
write.table(me,"me_30_stable_genes.txt")

算好的各列的median当作一行，放入最后一行，进行转置将化合物名变成行
total<-read.table("stable.txt",head=T,row.names=1)
ttoal<-t(total)
dim(ttoal)
row.names(ttoal)
ttoal
write.table(ttoal,'total_new_withme.txt')
写出来后进行将转置导致的第一行不是名字的去掉，让探针名和median上前成为title，然后就可以利用Median这一列进行过滤了
new1<-read.table('total_new_withme.txt',head=T,row.names=1)
dim(new1)
new<-subset(new1,Median>5)
dim(new)
write.table(new,"new_filter_total.txt")

7.normalizastion R.script
stable<-read.table("stable_filtered.txt",head=T,row.names=1)
me<-apply(stable,2,median)
total<-read.table("total_filtered.txt",head=T,row.names=1)
for (i in 1:901){
    total[,i]=log2(total[,i]/me[i])
}
write.table(total,"normalized_by_median_of_stable_genes2.txt")
或者

stable<-read.table("stable_filtered.txt",head=T,row.names=1)
total<-read.table("total_filtered.txt",head=T,row.names=1)
me<-apply(stable,2,median)
head(me)
for (i in 1:901){
    total[,i]=round(100*total[,i]/me[i],digits=2) ＃也可以round(100*total[,i]/me[i],2)
}
write.table(total,"normalized_by_median100.txt")

将rasl结果进行统计
 
8.计算每一列fc，但是将4个dmso变化来
先将dmso去一去,其实大可不必将dmso分出来，直接给一个表，然后告诉位置即可

#edit for each plate split

total<-read.table("normalized_by_median100.txt",head=T,row.names=1)
	ttotal<-t(total)
	write.table(ttotal,"normalized_by_median100_t.txt")
#heatmap for dmso

library(pheatmap)
pdf(file="log(dmso)_final.pdf")
data<-read.table("log(dmso).txt",head=T)
data<-data.matrix(data)
pheatmap(data,cluster_row=TRUE,clustering_method="average",cluster_col=FALSE,cellwidth=4,show_rownames=F,fontsize_col=6,las=2)
dev.off()

最终用方法
treat<-read.table("plate1_8_treat.txt",head=T,row.names=1)
control<-read.table("plate1_8_dmso.txt",head=T,row.names=1)
treat<-(log2(treat/as.matrix(control[,1]))+log2(treat/as.matrix(control[,2]))+log2(treat/as.matrix(control[,3]))+log2(treat/as.matrix(control[,4]))+log2(treat/as.matrix(control[,5])))/5
write.table(treat,"plate1_8_fc.txt")

treat<-read.table("plate1_20_treat.txt",head=T,row.names=1)
control<-read.table("plate1_20_dmso.txt",head=T,row.names=1)
treat<-(log2(treat/as.matrix(control[,1]))+log2(treat/as.matrix(control[,2]))+log2(treat/as.matrix(control[,3]))+log2(treat/as.matrix(control[,4]))+log2(treat/as.matrix(control[,5]))+log2(treat/as.matrix(control[,6])))/6
write.table(treat,"plate1_20_fc.txt")

treat<-read.table("plate2_8_treat.txt",head=T,row.names=1)
control<-read.table("plate2_8_dmso.txt",head=T,row.names=1)
treat<-(log2(treat/as.matrix(control[,1]))+log2(treat/as.matrix(control[,2]))+log2(treat/as.matrix(control[,3]))+log2(treat/as.matrix(control[,4]))+log2(treat/as.matrix(control[,5]))+log2(treat/as.matrix(control[,6])))/6
write.table(treat,"plate2_8_fc_2.txt")

treat<-read.table("plate2_20_treat.txt",head=T,row.names=1)
control<-read.table("plate2_20_dmso.txt",head=T,row.names=1)
treat<-(log2(treat/as.matrix(control[,1]))+log2(treat/as.matrix(control[,2]))+log2(treat/as.matrix(control[,3]))+log2(treat/as.matrix(control[,4]))+log2(treat/as.matrix(control[,5]))+log2(treat/as.matrix(control[,6])))/6
write.table(treat,"plate2_20_fc.txt")

treat<-read.table("plate3_8_treat.txt",head=T,row.names=1)
control<-read.table("plate3_8_dmso.txt",head=T,row.names=1)
treat<-(log2(treat/as.matrix(control[,1]))+log2(treat/as.matrix(control[,2]))+log2(treat/as.matrix(control[,3]))+log2(treat/as.matrix(control[,4]))+log2(treat/as.matrix(control[,5]))+log2(treat/as.matrix(control[,6])))/6
write.table(treat,"plate3_8_fc.txt")

treat<-read.table("plate3_20_treat.txt",head=T,row.names=1)
control<-read.table("plate3_20_dmso.txt",head=T,row.names=1)
treat<-(log2(treat/as.matrix(control[,1]))+log2(treat/as.matrix(control[,2]))+log2(treat/as.matrix(control[,3]))+log2(treat/as.matrix(control[,4]))+log2(treat/as.matrix(control[,5]))+log2(treat/as.matrix(control[,6])))/6
write.table(treat,"plate3_20_fc.txt")

treat<-read.table("plate4_8_treat.txt",head=T,row.names=1)
control<-read.table("plate4_8_dmso.txt",head=T,row.names=1)
treat<-(log2(treat/as.matrix(control[,1]))+log2(treat/as.matrix(control[,2]))+log2(treat/as.matrix(control[,3]))+log2(treat/as.matrix(control[,4]))+log2(treat/as.matrix(control[,5]))+log2(treat/as.matrix(control[,6])))/6
write.table(treat,"plate4_8_fc.txt")

treat<-read.table("plate4_20_treat.txt",head=T,row.names=1)
control<-read.table("plate4_20_dmso.txt",head=T,row.names=1)
treat<-(log2(treat/as.matrix(control[,1]))+log2(treat/as.matrix(control[,2]))+log2(treat/as.matrix(control[,3]))+log2(treat/as.matrix(control[,4]))+log2(treat/as.matrix(control[,5]))+log2(treat/as.matrix(control[,6])))/6
write.table(treat,"plate4_20_fc.txt")

total<-read.table("Lane7_plate1_D7_control.txt",head=T,row.names=1)
control<-total[1:5]
treat<-total[6:384]
for (i in 1:1273){
    control[i,]=log2(treat[i,]/control[i,])
write.table(control,"treatvs_control_log2.txt")
新编融合fc为一个

以前方法
total<-read.table("Lane7_plate1_D7_control.txt",head=T,row.names=1)
control<-total[1:5]
for (j in 1:2705){
    treat<-total[,j]
    for (i in 1:1273){
        control[i,]=log2(treat[i,]/control[i,])
        }
    total[,j]<-apply(control,1,mean)
}
write.table(control,"treatvs_control_log2.txt")

8.5. 先保证结果是可信的，过滤低表达的基因
13.另外的途径获得每个药严格的signature
(1)从normalize后的数据开始进行计算
#利用每个板的5dmso的平均值作为一列进行过滤
fc<-read.table("lane7_plate_fc.txt",head=T,row.names=1,sep="\t")
dmso<-read.table("lane7_plate1_dmso_average.txt",head=T,row.names=1,sep="\t")
treat<-read.table("lane7_plate1_treat.txt",head=T,row.names=1,sep="\t")
stable<-read.table("lane7_plate1_stable.txt",head=T,row.names=1,sep="\t")
me<-apply(stable,2,median)
dmso_cutoff<-as.numeric(dmso[,1])
index1<-which(dmso_cutoff<=8.677707)             #8.677707=(5*100/63+5*100/51+5*100/63+5*100/55+5*100/58)/5 即将5个dmso的stable gene的中位值找出来 
Time <- rep(0, length(dmso_cutoff))
fc[index1,]<-as.numeric(Time[index1])

#利用treat自己对fc进行过滤，变成0
for (i in 1:375){
    cutoff<-as.numeric(treat[,i])
    indexi<-which(cutoff<=100*5/me[i])
    Timei <- rep(0, length(cutoff))
    fc[indexi,i]<-as.numeric(Timei[indexi])
}
write.table(fc,"lane7_plate1_fc_filtered_by_median5.txt")  #gsea可能用这个ranked list


9.将大excel表格按药进行拆分得到各自的txt
> a<-read.table("lane7_plate1_fc_test.txt",head=T,row.names=1)
> dim(a)
[1] 1273  375
> write.table(a,"total_r.txt",sep="\t")
先在r里面改变分隔符, 不要列标题，因为.rnk里直接是以基因开始,可以有标题，中间要有‘\t’隔开
python
split_excel_to_files_by_compound.py
paste.py
（
cut -f 1 total2.txt > name.txt
for i in 
cut -f 3 total2.txt > test_col3.txt
paste test.txt test_col3.txt > col3.txt
cut -f 1 total2.txt > test.txt
cut -f 1 test.txt
paste file1 file2 > file3

 paste [-d] file1 file2
选项与参数：
-d  ：后面可以接分隔字符。默认是以 [tab] 来分隔的！通过交换文件名即可指定哪一列先粘：）

10.安装gsea 命令行的软件，直接下载gsea2-2.2.0.jar，然后告诉路径在哪儿即可，如下面的命令（适合自己的）
例子
java -cp /Users/lishasha/Desktop/home/tools/gsea2-2.2.0.jar -Xmx512m xtools.gsea.GseaPreranked -gmx /Users/lishasha/Desktop/home/new_work/20150801_3000drugs/gsea/lung.gmt -collapse false -mode Max_probe -norm meandiv -nperm 1000 -rnk /Users/lishasha/Desktop/home/new_work/20150801_3000drugs/gsea/fc.rnk -scoring_scheme classic -rpt_label lung -include_only_symbols true -make_sets true -plot_top_x 20 -rnd_seed timestamp -set_max 500 -set_min 0 -zip_report false -out /Users/lishasha/gsea_home/output -gui false
正式写入python的脚本
java -cp /Users/lishasha/Desktop/home/tools/gsea2-2.2.0.jar -Xmx512m xtools.gsea.GseaPreranked -gmx /Users/lishasha/Desktop/home/new_work/20150801_3000drugs/gsea/plan1/again/new/fc/run_gsea/CTNNB1_UP.gmt -collapse false -mode Max_probe -norm meandiv -nperm 1000 -rnk /Users/lishasha/Desktop/home/new_work/20150801_3000drugs/gsea/fc.rnk{} -scoring_scheme classic -rpt_label {*} -include_only_symbols true -make_sets true -plot_top_x 20 -rnd_seed timestamp -set_max 500 -set_min 0 -zip_report false -out /Users/lishasha/gsea_home/output/gsea -gui false
c集群上
java -cp /Share/home/wangdong/local/app/gsea/gsea2-2.2.0.jar -Xmx512m xtools.gsea.GseaPreranked -gmx /Users/lishasha/Desktop/home/new_work/20150801_3000drugs/gsea/plan1/again/new/fc/run_gsea/CTNNB1_UP.gmt -collapse false -mode Max_probe -norm meandiv -nperm 1000 -rnk /Users/lishasha/Desktop/home/new_work/20150801_3000drugs/gsea/fc.rnk{} -scoring_scheme classic -rpt_label {*} -include_only_symbols true -make_sets true -plot_top_x 20 -rnd_seed timestamp -set_max 500 -set_min 0 -zip_report false -out /Users/lishasha/gsea_home/output/gsea -gui false
python /Share/home/wangdong/lss/script/run_gsea.py
AttributeError: 'tuple' object has no attribute 'write'
A集群
perl bsub.pl one.sh 1 TINY ./source.txt
perl bsub.pl f500.sh 1 TINY ./source.txt
perl bsub.pl f1001.sh 1 TINY ./source.txt
perl bsub.pl f1501.sh 1 TINY ./source.txt
perl bsub.pl f2000.sh 1 TINY ./source.txt

11.提取出gsea里的数据，可以分成多份运行
在a集群上可以运行40份没有问题
 for k in {1..40}; do python /Work1/home/wangdong/lss/20150801compoud/run_gsea.py & done
 python /Work1/home/wangdong/lss/20150801compoud/lung/run_gsea_up.py
for k in {1..40}; do python /Work1/home/wangdong/lss/20150801compoud/lung/run_gsea_up.py & done
for k in {1..40}; do python /Work1/home/wangdong/lss/20150801compoud/lung/run_gsea_down.py & done
lung_up.sh
perl bsub.pl lung_up.sh 1 TINY ./source.txt
perl bsub.pl lung_down.sh 1 TINY ./source.txt

python /Work1/home/wangdong/lss/20150801compoud/lung/lung_down_one.py
python /Work1/home/wangdong/lss/20150801compoud/lung/lung_down_500.py
python /Work1/home/wangdong/lss/20150801compoud/lung/lung_down_2000.py
python /Work1/home/wangdong/lss/20150801compoud/lung/lung_down_1500.py
python /Work1/home/wangdong/lss/20150801compoud/lung/lung_down_1000.py
python /Work1/home/wangdong/lss/20150801compoud/lung/lung_up_1000.py
python /Work1/home/wangdong/lss/20150801compoud/lung/lung_up_1500.py
python /Work1/home/wangdong/lss/20150801compoud/lung/lung_up_2000.py
python /Work1/home/wangdong/lss/20150801compoud/lung/lung_up_500.py
python /Work1/home/wangdong/lss/20150801compoud/lung/lung_up_one.py


perl bsub.pl lung_down_one.sh 1 TINY ./source.txt
perl bsub.pl lung_down_500.sh 1 TINY ./source.txt
perl bsub.pl lung_down_2000.sh 1 TINY ./source.txt
perl bsub.pl lung_down_1500.sh 1 TINY ./source.txt
perl bsub.pl lung_down_1000.sh 1 TINY ./source.txt
perl bsub.pl lung_up_1000.sh 1 TINY ./source.txt
perl bsub.pl lung_up_1500.sh 1 TINY ./source.txt
perl bsub.pl lung_up_2000.sh 1 TINY ./source.txt
perl bsub.pl lung_up_500.sh 1 TINY ./source.txt	
perl bsub.pl lung_up_one.sh 1 TINY ./source.txt

或者利用一下的方法进行分份
get_the_sh.py
然后在split到100个文件里面，批量添加后缀.sh然后提交2563/50=51个sh,14:57开始,
split -l 50 try.sh gsea
用find和xargs添加后缀名
find . -type f |xargs -t -i mv {} {}.sh
find . -type f |xargs -i mv {} {}.txt

.  当前目录
type  查找某一类型的文件，诸如：
b - 块设备文件。
d - 目录。
c - 字符设备文件。
p - 管道文件。
l - 符号链接文件。
f - 普通文件。
＃
将上述列表作为参数进行传递，一次传递一个。xargs 命令允许你这样做。最后一部分，xargs ls -ltr，用于接收输出并对其执行 ls -ltr 命令，如下所示：
$ ls | xargs -t -i mv {} {}.bak
-i 选项告诉 xargs 用每项的名称替换 {}。-t 选项指示 xargs 先打印命令，然后再执行。


12将gsea数据提取出来
extract_gsea_result.py

13.另外的途径获得每个药严格的signature
博奥自己的计算方式
 #下次这个值直接用公式算,确定好stable中dmso的位置，可以给一个没按照板分的没过滤的stable中dmso的位置信息，不过得将0变成1，每个板子不一样，下一次看看可不可以放在最前面，
所以这个5下次可以定义成Ntotal*8.8%/Gtotal（上次的5平均值摸索为0.088）,如本次为0.09*Ntotal／2033，（上次的5中位值摸索为0.09）0.004426955*Ntotal*100=0.4426955*Ntotal
0.4426955*Ntotal＊1/5*(1/27+1/13.5+1/24+1/20.5+1/18),  最终0.257113821＊0.000885391*Ntotal就是lane6_plate1_8的大约是5的值，而每块版只需要改（1/27+1/13.5+1/24+1/20.5+1/18)和1/5，而Ntotal还需要准备一个没有哦normal的总的值
#本次先用5,原因是对于总reads更小的过滤太不严格了如0.02m 20000／2033=9.8条平均 
这样做有问题，还需要重新过滤，dmso都排到前5去了，且基因太多1000个
fc<-read.table("plate4_20_fc.txt",head=T,row.names=1,sep="\t")
dmso<-read.table("plate4_20_dmso.txt",head=T,row.names=1,sep="\t")
treat<-read.table("plate4_20_treat.txt",head=T,row.names=1,sep="\t")
stable<-read.table("plate4_20_stable.txt",head=T,row.names=1,sep="\t")
dmso1<-apply(dmso,1,mean)
me<-apply(stable,2,median)
dmso_cutoff<-as.numeric(dmso1)
index1<-which(dmso_cutoff<=3.20364165)
Time <- rep(0, length(dmso_cutoff))
fc[index1,]<-as.numeric(Time[index1])
write.table(fc,"test.txt") # 看看dmso过滤情况
for (i in 1:107){
    cutoff<-as.numeric(treat[,i])
    indexi<-which(cutoff<=100*5/me[i])
    Timei <- rep(0, length(cutoff))
    fc[indexi,i]<-as.numeric(Timei[indexi])
}
write.table(fc,"plate4_20_fc_filtered_by_median5.txt")

for (i in 1:107){
    cutoff<-as.numeric(fc[,i])
    indexi<-which(cutoff <1 & cutoff >-1)
    Timei <- rep(0, length(cutoff))
    fc[indexi,i]<-as.numeric(Timei[indexi])
}
write.table(fc,"plate4_20_fc_by_minor1to1.txt")

for (i in 1:107){
    cutoff<-as.numeric(fc[,i])
    indexi<-which(cutoff!=0)
    Timei <- rep(10, length(cutoff))
    fc[indexi,i]<-as.numeric(Timei[indexi])
}
write.table(fc,"plate4_20_fc_modified_final_v2.txt")

a<-matrix(rep(1:107,2), nrow = 2)
for (i in 1:107){
    a[,i]<-table(fc[,i])
}

colnames(a) <- colnames(fc)
write.table(a,"plate4_20_clasification.txt")

#对dmso和样本只去掉dmso小于5的,进一步优化的结果

fc<-read.table("plate4_20_fc.txt",head=T,row.names=1,sep="\t")
dmso<-read.table("plate4_20_dmso.txt",head=T,row.names=1,sep="\t")
treat<-read.table("plate4_20_treat.txt",head=T,row.names=1,sep="\t")
stable<-read.table("plate4_20_stable2.txt",head=T,row.names=1,sep="\t")
dmso1<-apply(dmso,1,mean)
me<-apply(stable,2,median)
dmso_cutoff<-as.numeric(dmso1)
index1<-which(dmso_cutoff<=3.20364165)
for (i in 1:107){
    cutoff<-as.numeric(treat[,i])
    indexi<-which(cutoff<=100*5/me[i] & dmso_cutoff<=3.20364165)  #对dmso和样本只去掉dmso小于5的
    Timei <- rep(0, length(cutoff)) #Timei与之前的Time等长，所以用一个即可
    fc[indexi,i]<-as.numeric(Timei[indexi])
}
write.table(fc,"plate4_20_fc_filtered_by_median5_both5.txt")

for (i in 1:107){
    cutoff<-as.numeric(fc[,i])
    indexi<-which(cutoff <1 & cutoff >-1)
    Timei <- rep(0, length(cutoff))
    fc[indexi,i]<-as.numeric(Timei[indexi])
}
write.table(fc,"plate4_20_fc_by_minor1to1_both5.txt")

for (i in 1:107){
    cutoff<-as.numeric(fc[,i])
    indexi<-which(cutoff!=0)
    Timei <- rep(10, length(cutoff))
    fc[indexi,i]<-as.numeric(Timei[indexi])
}
write.table(fc,"plate4_20_fc_modified_final_v2_both5.txt")

a<-matrix(rep(1:107,2), nrow = 2)
for (i in 1:107){
    a[,i]<-table(fc[,i])
}

colnames(a) <- colnames(fc)
write.table(a,"plate4_20_clasification_both5.txt")
(1)从normalize后的数据开始进行计算
#利用每个板的5dmso的平均值作为一列进行过滤
fc<-read.table("lane7_plate1_fc.txt",head=T,row.names=1,sep="\t")
dmso<-read.table("lane7_plate1_dmso_average.txt",head=T,row.names=1,sep="\t")
treat<-read.table("lane7_plate1_treat.txt",head=T,row.names=1,sep="\t")
stable<-read.table("lane7_plate1_stable.txt",head=T,row.names=1,sep="\t")
me<-apply(stable,2,median)
dmso_cutoff<-as.numeric(dmso[,1])
index1<-which(dmso_cutoff<=5.677707)
Time <- rep(0, length(dmso_cutoff))
fc[index1,]<-as.numeric(Time[index1])

#利用treat自己对fc进行过滤，变成0
for (i in 1:375){
    cutoff<-as.numeric(treat[,i])
    indexi<-which(cutoff<=100*5/me[i])
    Timei <- rep(0, length(cutoff))
    fc[indexi,i]<-as.numeric(Timei[indexi])
}
write.table(fc,"lane7_plate1_fc_filtered_by_median5.txt")  #gsea可能用这个ranked list，可以从这儿变成－1至+1间用于heatmap

#将-1<fc<1为0，可以将全部的板子合在一起一起过滤
for (i in 1:375){
    cutoff<-as.numeric(fc[,i])
    indexi<-which(cutoff <1 & cutoff >-1)
    Timei <- rep(0, length(cutoff))
    fc[indexi,i]<-as.numeric(Timei[indexi])
}
write.table(fc,"lane7_plate1_fc_by_minor1to1.txt") #gene signature细节看这个，可以从这儿去变成－1，＋1，0的打分矩阵

#将fc变成>1 <-1为一类
for (i in 1:375){
    cutoff<-as.numeric(fc[,i])
    indexi<-which(cutoff!=0)
    Timei <- rep(10, length(cutoff))
    fc[indexi,i]<-as.numeric(Timei[indexi])
}
write.table(fc,"lane7_plate1_fc_modified_final_v2.txt") #gene signature筛选看这个,可以不用输出这个

a<-matrix(rep(1:375,2), nrow = 2)
for (i in 1:375){
    a[,i]<-table(fc[,i])
}

colnames(a) <- colnames(fc)
write.table(a,"lane7_plate1_clasification.txt")
#对于>1 <－1的基因，均变成1，－1，为了heatmap更加明显,同样也可以将lane7_plate1_fc_by_minor1to1.txt变成打分矩阵
fc<-read.table("total_fc.txt",head=T,row.names=1)
for (i in 1:2564){
    cutoff<-as.numeric(fc[,i])
    indexi<-which(cutoff>=1)
    Timei <- rep(1, length(cutoff))
    fc[indexi,i]<-as.numeric(Timei[indexi])
}
write.table(fc,"total_above1.txt")

for (i in 1:2564){
    cutoff<-as.numeric(fc[,i])
    indexi<-which(cutoff<=-1)
    Timei <- rep(-1, length(cutoff))
    fc[indexi,i]<-as.numeric(Timei[indexi])
}
write.table(fc,"total_above1_below1.txt")

#利用fc作heatmap去除所有样品中均是0的基因
a<- matrix(1:12,nrow=3,ncol=4)
keep = rowSums(a)!=26
p = a[keep1,]
#上面实验例子,下面正式运行，利用fc作heatmap去除所有样品中均是0的基因，对于>1 <－1的基因，均变成1，－1，为了heatmap更加明显
fc<-read.table("test.txt",head=T,row.names=1,sep='\t')
keep = rowSums(fc)!=0
fc = fc[keep,]
write.table(fc,"test_remove0.txt")

for (i in 1:107){
    cutoff<-as.numeric(fc[,i])
    indexi<-which(cutoff>=1)
    Timei <- rep(1, length(cutoff))
    fc[indexi,i]<-as.numeric(Timei[indexi])
}
write.table(fc,"total_above1.txt")

for (i in 1:107){
    cutoff<-as.numeric(fc[,i])
    indexi<-which(cutoff<=-1)
    Timei <- rep(-1, length(cutoff))
    fc[indexi,i]<-as.numeric(Timei[indexi])
}
write.table(fc,"total_above1_below1.txt")

#pheatmap进行标注，两个方向
library(pheatmap)

data<-read.table("7702final.txt",head=T,row.names=1,sep='\t')
data<-data.matrix(data)
ann_col<-read.table("ann_col.txt",head=T,row.names=1,sep='\t')
ann_row<-read.table("ann_row.txt",head=T,row.names=1,sep='\t')


annotation_col = data.frame(
time=factor(ann_col$time),yx=factor(ann_col$property)
)

rownames(annotation_col)=rownames(ann_col)

annotation_row = data.frame(
gene_pathwat=factor(ann_row$pathway)
)

rownames(annotation_row) =rownames(ann_row)

pdf(file="test_final2.pdf",height=20,width=20)
pheatmap(data,cluster_row=TRUE,cluster_col=TRUE,show_rownames=F,show_colnames=F,color = colorRampPalette(c("navy", "white", "firebrick3"))(50),cellwidth=0.8,cellheight=0.4,fontsize_col=8,annotation_col = annotation_col,annotation_row = annotation_row)
dev.off()
q()



#作图
a<-read.table("lane6_plate1_20.txt",head=T,row.names=1,sep="\t")
pdf("bar_lane6_plate1_20.pdf",height=10,width=20)
barplot(a$diff_gene,col=as.factor(a$factor))
dev.off()
#将各组进行box plot的比较
a<-read.table("lane6_plate1_8.txt",head=T,row.names=1)
a<-read.table("lane6_plate1_8.txt",head=T,row.names=1)

b<-read.table("not_worked_sw_probe_list_tm.txt",head=T)
b<-read.table("not_worked_sw_probe_list_tm.txt",head=T)
b<-read.table("not_worked_sw_probe_list_tm.txt",head=T)
head(b)
pdf("Tm.pdf")
boxplot(a$Tm_a,a$Tm_d,b$Tm_a,b&Tm_d),col=c("red","blue"),ylab="The Tm Value of probes",las=1,font.lab=2))
boxplot(a$Tm_a,a$Tm_d,b$Tm_a,b&Tm_d,col=c("red","blue"),ylab="The Tm Value of probes",las=1,font.lab=2)
boxplot(a$Tm_a,a$Tm_d,b$Tm_a,b$Tm_d,col=c("red","blue"),ylab="The Tm Value of probes",las=1,font.lab=2)
dev.off()


a<-read.table("plot2.txt",head=T,sep="\t")
> pdf("scatter_plot7.pdf")
plot(a$Lane8_plate2_I1,a$Lane8_plate2_G24,col=as.factor(a$factor),lty=2,pch=20,cex=0.5,xlab="Argatroban",ylab="dmso")
NAME 替换成^t,
plot(a$Lane8_plate2_I1,a$Lane8_plate2_G24,col=as.factor(a$factor),lty=2,pch=20,cex=0.5,xlab="log2(hits of Argatroban)",ylab="log2(hits of dmso)")


a<-read.table("Lane7_plate1_N24_DAPT_3.txt",head=T,sep="\t")
pdf("Lane7_plate1_N24_DAPT_3.pdf")
plot(a$Lane7_plate1_N24_DAPT_3,a$Lane7_plate1_G23,col=as.factor(a$factor),lty=2,pch=20,cex=0.5,xlab="log2(Lane7_plate1_N24_DAPT_3)",ylab="log2(hits of dmso)")
dev.off
a<-read.table("Lane7_plate2_K24_dmso_11.txt",head=T,sep="\t")
pdf("Lane7_plate2_K24_dmso_11.pdf")
plot(a$Lane7_plate2_K24_dmso_11,a$Lane7_plate2_G24,col=as.factor(a$factor),lty=2,pch=20,cex=0.5,xlab="log2(Lane7_plate2_K24_dmso_11)",ylab="log2(hits of dmso)")
dev.off()

a<-read.table("Lane8_plate2_I1.txt",head=T,sep="\t")
pdf("Lane8_plate2_I1_3.pdf")
plot(a$Lane8_plate2_dmso_average,a$Lane8_plate2_I1,col=as.factor(a$factor),lty=2,pch=20,cex=0.5,xlab="log2(hits of dmso)",ylab="log2(Lane8_plate2_I1)")
abline(lm(a$Lane8_plate2_I1~a$Lane8_plate2_dmso_average))
dev.off()

利用r作正太分布曲线
a<-read.table("diff_gene.txt")
a
hist(a$V1,breaks=10,col="red")
hist(a$V1,breaks=5,col="red")
hist(a$V1,breaks=100,col="red")
hist(a$V1,breaks=20,col="red")
hist(a$V1,breaks=18,col="red")
hist(a$V1,breaks=36,col="red")
hist(a$V1,breaks=72,col="red")
420/72
hist(a$V1,breaks=36,col="red")
hist(a$V1,breaks=72,col="red")

a<-read.table("diff_gene7.txt",head=T,sep='\t')
密度的
hist(a$differential.expressed.genes,freq=FALSE,breaks=72,col="red",xlab="number of differential expressed genes",main="Histogram,density curve")
rug(jitter(a$differential.expressed.genes))
lines(density(a$differential.expressed.genes),col="blue",lwd=2)
pdf("hist_density.pdf")
hist(a$differential.expressed.genes,freq=FALSE,breaks=72,col="red",xlab="number of differential expressed genes",main="Histogram,density curve")
rug(jitter(a$differential.expressed.genes))
lines(density(a$differential.expressed.genes),col="blue",lwd=2)
dev.off()
频率的
pdf("hist_freq_try_two_factor_tmp_try2.pdf")
x<-a$DIFFERENTIAL.EXPRESSED.GENES
x
h<-hist(x,,breaks=72,col=as.factor(a$FACTOR),xlab="number of differential expressed genes",main="Histogram with normal curve")
xfit<-seq(min(x),max(x),length=40)
yfit<-dnorm(xfit,mean=mean(x),sd=sd(x))
yfit<-yfit*diff(h$mids[1:2])*length(x)
lines(xfit,yfit,col="blue",lwd=2)
box()
legend(300,100,c("dmso","others"),cex=1.5,fill=1:3)
dev.off()

#打分
up+(-down)（ctnnb1）
CTNNB1_UP替换成^t,
#作heatmap
pheatmap 位置
/Share/home/wangdong/lss/project/20150430_sw_probes_screen/plot
library(pheatmap)
pdf(file="capitalbio_bzk_dmso2.pdf")
data<-read.table("dmso_heatmap.txt",head=T,sep='\t',row.names=1)
data1<-log2(data)
data<-data.matrix(data1)
pheatmap(data,cluster_row=TRUE,clustering_method="average",cluster_col=TRUE,cellwidth=8,fontsize_row=2,fontsize_col=8,las=2，color = colorRampPalette(c("navy", "white", "firebrick3"))
dev.off()
q()

pheatmap(test, color = colorRampPalette(c("navy", "white", "firebrick3"))(50))

library(pheatmap)
pdf(file="LXH.pdf")
data<-read.table("brain_no_down_ranked.txt",head=T,sep='\t',row.names=1)
data1<-log2(data)
data<-data.matrix(data1)
pheatmap(data,show_rownames = F,las=2,color = colorRampPalette(c("navy", "white", "firebrick3"))(50))
dev.off()


gsea
java -Xmx512m xtools.gsea.GseaPreranked -gmx gseaftp.broadinstitute.org://pub/gsea/gene_sets/c2.cp.kegg.v5.0.symbols.gmt -collapse false -mode Max_probe -norm meandiv -nperm 1000 -rnk /Users/lishasha/Desktop/home/new_work/20150801_3000drugs/gsea/fc.rnk -scoring_scheme classic -rpt_label my_analysis -include_only_symbols true -make_sets true -plot_top_x 20 -rnd_seed timestamp -set_max 500 -set_min 15 -zip_report false -out /Users/lishasha/gsea_home/output/九月18 -gui false


或者利用一下的方法进行分份
get_the_sh.py
然后在split到100个文件里面，批量添加后缀.sh然后提交2563/50=51个sh,14:57开始,
split -l 50 try.sh gsea
用find和xargs添加后缀名
find . -type f |xargs -t -i mv {} {}.sh
find . -type f |xargs -i mv {} {}.txt

.  当前目录
type  查找某一类型的文件，诸如：
b - 块设备文件。
d - 目录。
c - 字符设备文件。
p - 管道文件。
l - 符号链接文件。
f - 普通文件。
＃
将上述列表作为参数进行传递，一次传递一个。xargs 命令允许你这样做。最后一部分，xargs ls -ltr，用于接收输出并对其执行 ls -ltr 命令，如下所示：
$ ls | xargs -t -i mv {} {}.bak
-i 选项告诉 xargs 用每项的名称替换 {}。-t 选项指示 xargs 先打印命令，然后再执行。


12将gsea数据提取出来
extract_gsea_result.py

NAME 替换成^t,
	
CTNNB1_UP替换成^t,

xargs
大 多数 Linux 命令都会产生输出：文件列表、字符串列表等。但如果要使用其他某个命令并将前一个命令的输出作为参数该怎么办？例如，file 命令显示文件类型（可执行文件、ascii 文本等）；你能处理输出，使其仅显示文件名，目前你希望将这些名称传递给 ls -l 命令以查看时间戳记。xargs 命令就是用来完成此项工作的。他允许你对输出执行其他某些命令。记住下面这个来自于第 1 部分中的语法：
例1：

file -Lz * | grep ASCII | cut -d":" -f1 | xargs ls -ltr
让我们来剖析这个命令字符串。第一个，file -Lz *，用于查找是符号链接或经过压缩的文件。他将输出传递给下一个命令 grep ASCII，该命令在其中搜索 "ASCII" 字符串并产生如下所示的输出：

alert_DBA102.log:        ASCII English text
alert_DBA102.log.Z:      ASCII text (compress’d data 16 bits)
dba102_asmb_12307.trc.Z: ASCII English text (compress’d data 16 bits)
dba102_asmb_20653.trc.Z: ASCII English text (compress’d data 16 bits)
由于我们只对文件名感兴趣，因此我们应用下一个命令 cut -d":" -f1，仅显示第一个字段：

alert_DBA102.log
alert_DBA102.log.Z
dba102_asmb_12307.trc.Z
dba102_asmb_20653.trc.Z
目前，我们希望使用 ls -l 命令，将上述列表作为参数进行传递，一次传递一个。xargs 命令允许你这样做。最后一部分，xargs ls -ltr，用于接收输出并对其执行 ls -ltr 命令，如下所示：
ls -ltr alert_DBA102.log
ls -ltr alert_DBA102.log.Z
ls -ltr dba102_asmb_12307.trc.Z
ls -ltr dba102_asmb_20653.trc.Z
因此，xargs 本身虽然没有多大用处，但在和其他命令相结合时，他的功能非常强大。
下面是另一个示例，我们希望计算这些文件中的行数：
例 2：

$ file * | grep ASCII | cut -d":" -f1 | xargs wc -l
47853 alert_DBA102.log
     19 dba102_cjq0_14493.trc
29053 dba102_mmnl_14497.trc
    154 dba102_reco_14491.trc
     43 dba102_rvwr_14518.trc
77122 total


panel.cor.scale <- function(x, y, digits=2, prefix="R2=", cex.cor) #在之前面加上描述的R2=
{
usr <- par("usr"); on.exit(par(usr))
par(usr = c(0, 1, 0, 1))
r = (cor(x, y,use="pairwise"))
txt <- format(c(r, 0.123456789), digits=digits)[1]
txt <- paste(prefix, txt, sep="")
if(missing(cex.cor)) cex <- 0.8/strwidth(txt)
text(0.5, 0.5, txt, cex = cex * abs(r))

 else {pairs(x,upper.panel=panel.cor,lower.panel=panel.smooth,gap=0,cex.labels=4) #改变中间字体的大小
pdf("well4_cex5_R.pdf",family="sans",height=10,width=10) ＃改变字体，此处为arial
family可以改变字典，在PDF后面，PDF较容易，参见book，p48，另外，先利用下面命令查一下有哪些官方的字体明
 names(pdfFonts())
 
[1] "serif"                "sans"                 "mono"
 [4] "AvantGarde"           "Bookman"              "Courier"
 [7] "Helvetica"            "Helvetica-Narrow"     "NewCenturySchoolbook"
[10] "Palatino"             "Times"                "URWGothic"
[13] "URWBookman"           "NimbusMon"            "NimbusSan"
[16] "URWHelvetica"         "NimbusSanCond"        "CenturySch"
[19] "URWPalladio"          "NimbusRom"            "URWTimes"
[22] "ArialMT"              "Japan1"               "Japan1HeiMin"
[25] "Japan1GothicBBB"      "Japan1Ryumin"         "Korea1"
[28] "Korea1deb"            "CNS1"                 "GB1"

/Work1/home/wangdong/lss/data/sw_20150110/SW_16_Samples_R
#改变字体成arial ,times
 pdf("font_times.pdf",family="serif",height=10,width=10)
Fonts
You can easily set font size and style, but font family is a bit more complicated.
option	description
font	Integer specifying font to use for text. 
1=plain, 2=bold, 3=italic, 4=bold italic, 5=symbol
font.axis	font for axis annotation
font.lab	font for x and y labels
font.main	font for titles
font.sub	font for subtitles
ps	font point size (roughly 1/72 inch)
text size=ps*cex
family	font family for drawing text. Standard values are "serif", "sans", "mono", "symbol". Mapping is device dependent.
In windows, mono is mapped to "TT Courier New", serif is mapped to"TT Times New Roman", sans is mapped to "TT Arial", mono is mapped to "TT Courier New", and symbol is mapped to "TT Symbol" (TT=True Type). You can add your own mappings
下面的想法挺好，但是不能用
C集群上有的字体
> names(pdfFonts())
 [1] "serif"                "sans"                 "mono"
 [4] "AvantGarde"           "Bookman"              "Courier"
 [7] "Helvetica"            "Helvetica-Narrow"     "NewCenturySchoolbook"
[10] "Palatino"             "Times"                "URWGothic"
[13] "URWBookman"           "NimbusMon"            "NimbusSan"
[16] "URWHelvetica"         "NimbusSanCond"        "CenturySch"
[19] "URWPalladio"          "NimbusRom"            "URWTimes"
[22] "ArialMT"              "Japan1"               "Japan1HeiMin"
[25] "Japan1GothicBBB"      "Japan1Ryumin"         "Korea1"
[28] "Korea1deb"            "CNS1"                 "GB1"

# Type family examples - creating new mappings 
plot(1:10,1:10,type="n")
windowsFonts(
 	A=windowsFont("Arial Black"),
  B=windowsFont("Bookman Old Style"),
  C=windowsFont("Comic Sans MS"),
  D=windowsFont("Symbol")
)
text(3,3,"Hello World Default")
text(4,4,family="A","Hello World from Arial Black")
text(5,5,family="B","Hello World from Bookman Old Style")
text(6,6,family="C","Hello World from Comic Sans MS")
text(7,7,family="D", "Hello World from Symbol")
聚类作图方法或软件有什么？类似于散点图成簇的图
楚亚男-宏基因  16:31:13
你指PCA？
袁丽娜-Pangenome  16:33:15
没有那么多成分，就一种数值
袁丽娜-Pangenome  16:37:35
散点 聚类，有吗？
袁丽娜-Pangenome  16:37:53
貌似是要做数值转化？
楚亚男-宏基因  16:39:13
不知了……
康禹-Pangenome  16:41:05
用similarity那个图的聚类方法行吗？
吴浩-细菌复制  16:44:32
PCoA~ R


fastqc -o /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/fastqc /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/barcode6/Project_C6F03ANXX/Sample_zlj_sh2/*.fastq



开始使用Screen
简单来说，Screen是一个可以在多个进程之间多路复用一个物理终端的窗口管理器。Screen中有会话的概念，用户可以在一个screen会话中创建多个screen窗口，在每一个screen窗口中就像操作一个真实的telnet/SSH连接窗口那样。在screen中创建一个新的窗口有这样几种方式：
1．直接在命令行键入screen命令
[root@tivf06 ~]# screen
Screen将创建一个执行shell的全屏窗口。你可以执行任意shell程序，就像在ssh窗口中那样
screen还有更高级的功能。你可以不中断screen窗口中程序的运行而暂时断开（detach）screen会话，并在随后时间重新连接（attach）该会话，重新控制各窗口中运行的程序。例如，我们打开一个screen窗口编辑/tmp/abc文件：
[root@tivf06 ~]# screen vi /tmp/abc
之后我们想暂时退出做点别的事情，比如出去散散步，那么在screen窗口键入C-a d，Screen会给出detached提示：
管理你的远程会话
先来看看如何使用screen解决SIGHUP问题，比如现在我们要ftp传输一个大文件。如果按老的办法，SSH登录到系统，直接ftp命令开始传输，之后。。如果网络速度还可以，恭喜你，不用等太长时间了；如果网络不好，老老实实等着吧，只能传输完毕再断开SSH连接了。让我们使用screen来试试。
SSH登录到系统，在命令行键入screen。
[root@tivf18 root]# screen
在screen shell窗口中输入ftp命令，登录，开始传输。不愿意等了？OK，在窗口中键入C-a d：
让screen来帮你“保存”吧，你只需要打开一个ssh窗口，创建需要的screen窗口，退出的时候C-a d“保存”你的工作，下次登录后直接screen -r <screen_pid>就可以了。
最好能给每个窗口起一个名字，这样好记些。使用C-a A给窗口起名字。使用C-a w可以看到这些窗口名字，可能名字出现的位置不同
进入srceen
运行后暂时退出 control a d 
运行完了后杀掉：
screen -ls  查看编号
screen -r 16582
看看出现什么了，太棒了，一切都在。继续干吧

C-a k	杀掉当前窗口


tophat -p 16 -G /data/reference/human/gencode_v20/gencode_v20_annotation.gtf -o /home/lss/project/zlj/mapping/zlj_NC2 -r 50 --library-type=fr-unstranded /data/reference/human/hg38 /home/lss/project/zlj/fastq/zlj_NC_R1_trime.fastq /home/lss/project/zlj/fastq/zlj_NC_R2_trime.fastq
tophat -p 16 -G /data/reference/human/gencode_v20/gencode_v20_annotation.gtf -o /home/lss/project/zlj/mapping/zlj_sh2 -r 50 --library-type=fr-unstranded /data/reference/human/hg38 /home/lss/project/zlj/fastq/zlj_sh2_R1_trime.fastq /home/lss/project/zlj/fastq/zlj_sh2_R2_trime.fastq
tophat -p 16 -G /data/reference/human/gencode_v20/gencode_v20_annotation.gtf -o /home/lss/project/zlj/mapping/zlj_sh3 -r 50 --library-type=fr-unstranded /data/reference/human/hg38 /home/lss/project/zlj/fastq/zlj_sh3_R1_trime.fastq /home/lss/project/zlj/fastq/zlj_sh3_R2_trime.fastq
tophat -p 16 -G /data/reference/human/gencode_v20/gencode_v20_annotation.gtf -o /home/lss/project/zlj/mapping/wqy_2M -r 50 --library-type=fr-unstranded /data/reference/human/hg38 /home/lss/project/zlj/fastq/wqy_2M_R1_trime.fastq /home/lss/project/zlj/fastq/wqy_2M_R2_trime.fastq



/Share/home/wangdong/local/bin/tophat -p 8 -G /Share/home/wangdong/data/human_annotation/gencode19.lite.gtf -o /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_sh2 -r 50 --library-type=fr-unstranded /Share/home/lulab1/users/liyang/project/Genome/human_hg19/bowtie2_index/human_genome_hg19 /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/barcode6/Project_C6F03ANXX/Sample_zlj_sh2/zlj_sh2_R1_trime.fastq /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/barcode6/Project_C6F03ANXX/Sample_zlj_sh2/zlj_sh2_R2_trime.fastq
/Share/home/wangdong/local/bin/tophat -p 8 -G /Share/home/wangdong/data/human_annotation/gencode19.lite.gtf -o /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_sh3 -r 50 --library-type=fr-unstranded /Share/home/lulab1/users/liyang/project/Genome/human_hg19/bowtie2_index/human_genome_hg19 /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/barcode6/Project_C6F03ANXX/Sample_zlj_sh3/zlj_sh3_R1_trime.fastq /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/barcode6/Project_C6F03ANXX/Sample_zlj_sh3/zlj_sh3_R2_trime.fastq
/Share/home/wangdong/local/bin/tophat -p 8 -G /Share/home/wangdong/data/human_annotation/gencode19.lite.gtf -o /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_NC -r 50 --library-type=fr-unstranded /Share/home/lulab1/users/liyang/project/Genome/human_hg19/bowtie2_index/human_genome_hg19 /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/barcode6/Project_C6F03ANXX/Sample_zlj_NC/zlj_NC_R1_trime.fastq /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/barcode6/Project_C6F03ANXX/Sample_zlj_NC/zlj_NC_R2_trime.fastq
/Share/home/wangdong/local/bin/tophat -p 8 -G /Share/home/wangdong/data/human_annotation/gencode19.lite.gtf -o /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/wqy_2M -r 50 --library-type=fr-unstranded /Share/home/lulab1/users/liyang/project/Genome/human_hg19/bowtie2_index/human_genome_hg19 /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/barcode6/Project_C6F03ANXX/Sample_wqy_2M/wqy_2M_R1_trime.fastq /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/barcode6/Project_C6F03ANXX/Sample_wqy_2M/wqy_2M_R2_trime.fastq



fastx_trimmer -f 16 -l 110 -Q 33 -i /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/barcode6/Project_C6F03ANXX/Sample_zlj_sh2/zlj_sh2_R2.fastq -o /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/barcode6/Project_C6F03ANXX/Sample_zlj_sh2/zlj_sh2_R2_trime.fastq
fastx_trimmer -f 16 -l 110 -Q 33 -i /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/barcode6/Project_C6F03ANXX/Sample_zlj_sh3/zlj_sh3_R1.fastq -o /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/barcode6/Project_C6F03ANXX/Sample_zlj_sh3/zlj_sh3_R1_trime.fastq
fastx_trimmer -f 16 -l 110 -Q 33 -i /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/barcode6/Project_C6F03ANXX/Sample_zlj_sh3/zlj_sh3_R2.fastq -o /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/barcode6/Project_C6F03ANXX/Sample_zlj_sh3/zlj_sh3_R2_trime.fastq
fastx_trimmer -f 16 -l 110 -Q 33 -i /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/barcode6/Project_C6F03ANXX/Sample_zlj_NC/zlj_NC_R1.fastq -o /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/barcode6/Project_C6F03ANXX/Sample_zlj_NC/zlj_NC_R1_trime.fastq
fastx_trimmer -f 16 -l 110 -Q 33 -i /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/barcode6/Project_C6F03ANXX/Sample_zlj_NC/zlj_NC_R2.fastq -o /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/barcode6/Project_C6F03ANXX/Sample_zlj_NC/zlj_NC_R2_trime.fastq
fastx_trimmer -f 16 -l 110 -Q 33 -i /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/barcode6/Project_C6F03ANXX/Sample_wqy_2M/wqy_2M_R1.fastq -o /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/barcode6/Project_C6F03ANXX/Sample_wqy_2M/wqy_2M_R1_trime.fastq
fastx_trimmer -f 16 -l 110 -Q 33 -i /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/barcode6/Project_C6F03ANXX/Sample_wqy_2M/wqy_2M_R2.fastq -o /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/barcode6/Project_C6F03ANXX/Sample_wqy_2M/wqy_2M_R2_trime.fastq

fastx_trimmer -f 1 -l 51 -Q 33 -i rasl_lab2_AGTGCAT_L005_R1_001.fastq -o rasl_lab2_AGTGCAT_L005_R1_51nb.fastq



/Share/home/wangdong/local/bin/tophat -p 8 -G /Share/home/wangdong/data/human_annotation/gencode19.lite.gtf -o /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_sh2 --library-type=fr-unstranded /Share/home/lulab1/users/liyang/project/Genome/human_hg19/bowtie2_index/human_genome_hg19 /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/barcode6/Project_C6F03ANXX/Sample_zlj_sh2/zlj_sh2_R1.fastq /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/barcode6/Project_C6F03ANXX/Sample_zlj_sh2/zlj_sh2_R2.fastq
/Share/home/wangdong/local/bin/tophat -p 8 -G /Share/home/wangdong/data/human_annotation/gencode19.lite.gtf -o /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_sh3 --library-type=fr-unstranded /Share/home/lulab1/users/liyang/project/Genome/human_hg19/bowtie2_index/human_genome_hg19 /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/barcode6/Project_C6F03ANXX/Sample_zlj_sh3/zlj_sh3_R1.fastq /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/barcode6/Project_C6F03ANXX/Sample_zlj_sh3/zlj_sh3_R2.fastq
/Share/home/wangdong/local/bin/tophat -p 8 -G /Share/home/wangdong/data/human_annotation/gencode19.lite.gtf -o /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_NC --library-type=fr-unstranded /Share/home/lulab1/users/liyang/project/Genome/human_hg19/bowtie2_index/human_genome_hg19 /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/barcode6/Project_C6F03ANXX/Sample_zlj_NC/zlj_NC_R1.fastq /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/barcode6/Project_C6F03ANXX/Sample_zlj_NC/zlj_NC_R2.fastq
/Share/home/wangdong/local/bin/tophat -p 8 -G /Share/home/wangdong/data/human_annotation/gencode19.lite.gtf -o /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/wqy_2M --library-type=fr-unstranded /Share/home/lulab1/users/liyang/project/Genome/human_hg19/bowtie2_index/human_genome_hg19 /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/barcode6/Project_C6F03ANXX/Sample_wqy_2M/wqy_2M_R1.fastq /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/barcode6/Project_C6F03ANXX/Sample_wqy_2M/wqy_2M_R2.fastq




fastqc 
fastqc -o /Share/home/wangdong/lss/project/tmp/test2/ /Share/home/wangdong/lss/project/tmp/Human_exon_1.0ST.fastq


.gzip 解压
unzip .zip -d 目标文件夹

设置bashrc后发现许多命令不能用了，如ls等等，原因如下 
解决办法：

先用：echo $PATH  
查看path是否含有17/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin

如果没有

先用临时环境变量（重启后消失）
#export PATH=$PATH:/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin

然后就可以用那些命令了，进去修改永久环境变量：

1。修改profile文件：（所有用户）

#vi /etc/profile
加入：export PATH=$PATH:/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin
保存退出。

2。修改.bashrc文件：（单独用户）

#vi /~/.bashrc （‘~’代表：$HOME,  .bashrc是每个用户家目录下都有的，ls -all）
加入：export PATH=$PATH:/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin

保存退出。

重启系统，搞定。

也可以不用重启，使用命令：#source .bashrc 
即可使刚刚修改的环境变量生效

自己mac上的解决办法就是加到
~/.bash_profile中

/Share/home/wangdong/local/app/bowtie-1.1.1/bowtie-build -f /Share/home/wangdong/lss/project/20150604_probe_final_test_1411/new_ref_1411_rename.fa /Share/home/wangdong/lss/project/20150604_probe_final_test_1411/new_ref_1411_rename

fastx_trimmer -f 1 -l 51 -Q 33 -i rasl_lab2_AGTGCAT_L005_R1_001.fastq -o rasl_lab2_AGTGCAT_L005_R1_51nb.fastq
fastx_trimmer -f 1 -l 51 -Q 33 -i rasl_lab1_GAATCAT_L005_R1_001.fastq -o rasl_lab1_GAATCAT_L005_R1_51nt.fastq
fastx_trimmer -f 1 -l 51 -Q 33 -i rasl_lab3_GTGGCAT_L005_R1_001.fastq -o rasl_lab3_GTGGCAT_L005_R1_51nt.fastq
bowtie -n 3 /Share/home/wangdong/lss/project/20150604_probe_final_test_1411/new_ref_1411_rename /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/barcode7/Project_C6F03ANXX/Sample_rasl_lab1/rasl_lab1_GAATCAT_L005_R1_51nt.fastq -S /Share/home/wangdong/lss/project/20150604_probe_final_test_1411/mapping_result/rasl_lab1.sam
bowtie -n 3 /Share/home/wangdong/lss/project/20150604_probe_final_test_1411/new_ref_1411_rename /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/barcode7/Project_C6F03ANXX/Sample_rasl_lab2/rasl_lab2_AGTGCAT_L005_R1_51nb.fastq -S /Share/home/wangdong/lss/project/20150604_probe_final_test_1411/mapping_result/rasl_lab2.sam
bowtie -n 3 /Share/home/wangdong/lss/project/20150604_probe_final_test_1411/new_ref_1411_rename /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/barcode7/Project_C6F03ANXX/Sample_rasl_lab3/rasl_lab3_GTGGCAT_L005_R1_51nt.fastq -S /Share/home/wangdong/lss/project/20150604_probe_final_test_1411/mapping_result/rasl_lab3_R1.sam
python /Share/home/wangdong/lss/project/20150430_sw_probes_screen/mapping/split_colume3.py /Share/home/wangdong/lss/project/20150604_probe_final_test_1411/mapping_result/rasl_lab1.sam
python /Share/home/wangdong/lss/project/20150430_sw_probes_screen/mapping/split_colume3.py /Share/home/wangdong/lss/project/20150604_probe_final_test_1411/mapping_result/rasl_lab2.sam
python /Share/home/wangdong/lss/project/20150430_sw_probes_screen/mapping/split_colume3.py /Share/home/wangdong/lss/project/20150604_probe_final_test_1411/mapping_result/rasl_lab3_R1.sam



configureBclToFastq.pl --input-dir /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/Data/Intensities/BaseCalls --output /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/barcode7_2 --sample-sheet /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/SampleSheet_barcode7.csv --no-eamss --use-bases-mask Y126,I7n,Y126
cd /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/barcode7_2
nohup make -j 8
6barcode
configureBclToFastq.pl --input-dir /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/Data/Intensities/BaseCalls --output /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/barcode6 --sample-sheet /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/SampleSheet_barcode6.csv --no-eamss --use-bases-mask Y126,I6n,Y126
cd /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/barcode6
nohup make -j 8

还可以通过下面方法输入文件夹，但是需要进行运行是加引号，运行募夹里的文17
也可以直接input=glob.glob(sys.argv[1])
下面是.py 
  1 import sys,os,glob
  2 inputdir = sys.argv[ 1 ]
  3 input = glob.glob( inputdir )#get the list
  4 for f in input:
  5     os.system( 'fastqc -o ./fastqc2/ %s'%f) #%s need format here %f:format like f after %
  6
运行过程如下
 
[wangdong@cluster ~/lss/project/tmp]$python fastqc2.py './test/*'
Started analysis of WHY_HCT116RNA_2.fastq
Approx 5% complete for WHY_HCT116RNA_2.fastq
Approx 10% complete for WHY_HCT116RNA_2.fastq
Approx 15% complete for WHY_HCT116RNA_2.fastq
Approx 20% complete for WHY_HCT116RNA_2.fastq
Approx 25% complete for WHY_HCT116RNA_2.fastq
Approx 30% complete for WHY_HCT116RNA_2.fastq
Approx 35% complete for WHY_HCT116RNA_2.fastq
Approx 40% complete for WHY_HCT116RNA_2.fastq
Approx 45% complete for WHY_HCT116RNA_2.fastq
Approx 50% complete for WHY_HCT116RNA_2.fastq
Approx 55% complete for WHY_HCT116RNA_2.fastq
Approx 60% complete for WHY_HCT116RNA_2.fastq
Approx 65% complete for WHY_HCT116RNA_2.fastq
Approx 70% complete for WHY_HCT116RNA_2.fastq
Approx 75% complete for WHY_HCT116RNA_2.fastq
Approx 80% complete for WHY_HCT116RNA_2.fastq
Approx 85% complete for WHY_HCT116RNA_2.fastq
Approx 90% complete for WHY_HCT116RNA_2.fastq
Approx 95% complete for WHY_HCT116RNA_2.fastq
Analysis complete for WHY_HCT116RNA_2.fastq
Started analysis of Human_exon_1.0ST.fastq
Approx 5% complete for Human_exon_1.0ST.fastq
Approx 10% complete for Human_exon_1.0ST.fastq
Approx 15% complete for Human_exon_1.0ST.fastq
Approx 20% complete for Human_exon_1.0ST.fastq
Approx 25% complete for Human_exon_1.0ST.fastq
Approx 30% complete for Human_exon_1.0ST.fastq
Approx 35% complete for Human_exon_1.0ST.fastq
Approx 40% complete for Human_exon_1.0ST.fastq
Approx 45% complete for Human_exon_1.0ST.fastq
Approx 50% complete for Human_exon_1.0ST.fastq
Approx 55% complete for Human_exon_1.0ST.fastq
Approx 60% complete for Human_exon_1.0ST.fastq
Approx 65% complete for Human_exon_1.0ST.fastq
Approx 70% complete for Human_exon_1.0ST.fastq
Approx 75% complete for Human_exon_1.0ST.fastq
Approx 80% complete for Human_exon_1.0ST.fastq
Approx 85% complete for Human_exon_1.0ST.fastq
Approx 90% complete for Human_exon_1.0ST.fastq
Approx 95% complete for Human_exon_1.0ST.fastq
Analysis complete for Human_exon_1.0ST.fastq

关于format进行字符串格式化 python
自python2.6开始，新增了一种格式化字符串的函数str.format()，可谓威力十足。那么，他跟之前的%型格式化字符串相比，有什么优越的存在呢？让我们来揭开它羞答答的面纱。

语法
它通过{}和:来代替%。

“映射”示例
通过位置

In [1]: '{0},{1}'.format('kzc',18)  
Out[1]: 'kzc,18'  
In [2]: '{},{}'.format('kzc',18)  
Out[2]: 'kzc,18'  
In [3]: '{1},{0},{1}'.format('kzc',18)  
Out[3]: '18,kzc,18'
字符串的format函数可以接受不限个参数，位置可以不按顺序，可以不用或者用多次，不过2.6不能为空{}，2.7才可以。
通过关键字参数

In [5]: '{name},{age}'.format(age=18,name='kzc')  
Out[5]: 'kzc,18'
通过对象属性

class Person:  
    def __init__(self,name,age):  
        self.name,self.age = name,age  
        def __str__(self):  
            return 'This guy is {self.name},is {self.age} old'.format(self=self)  
In [2]: str(Person('kzc',18))  
Out[2]: 'This guy is kzc,is 18 old'
通过下标

In [7]: p=['kzc',18]
In [8]: '{0[0]},{0[1]}'.format(p)
Out[8]: 'kzc,18'
有了这些便捷的“映射”方式，我们就有了偷懒利器。基本的python知识告诉我们，list和tuple可以通过“打散”成普通参数给函数，而dict可以打散成关键字参数给函数（通过和*）。所以可以轻松的传个list/tuple/dict给format函数。非常灵活。
:号后面带填充的字符，只能是一个字符，不指定的话默认是用空格填充
用，号还能用来做金额的千位分隔符。

In [47]: '{:,}'.format(1234567890)
Out[47]: '1,234,567,890'

文件处理
 1 import sys,os,glob
  2 input = glob.glob("/Share/home/wangdong/lss/project/tmp/test/*.fastq")
  3 for f in input:
  4     print(f)
  5     os.system( 'fastqc -o ./fastqc/ %s'%f) ＃提前建立fastqc这个文件夹
运行只需要python fastqc.py
就会输出
/Share/home/wangdong/lss/project/tmp/test/WHY_HCT116RNA_2.fastq
Started analysis of WHY_HCT116RNA_2.fastq
Approx 5% complete for WHY_HCT116RNA_2.fastq
Approx 10% complete for WHY_HCT116RNA_2.fastq
Approx 15% complete for WHY_HCT116RNA_2.fastq
Approx 20% complete for WHY_HCT116RNA_2.fastq
Approx 25% complete for WHY_HCT116RNA_2.fastq
Approx 30% complete for WHY_HCT116RNA_2.fastq
Approx 35% complete for WHY_HCT116RNA_2.fastq
Approx 40% complete for WHY_HCT116RNA_2.fastq
Approx 45% complete for WHY_HCT116RNA_2.fastq
Approx 50% complete for WHY_HCT116RNA_2.fastq
Approx 55% complete for WHY_HCT116RNA_2.fastq
Approx 60% complete for WHY_HCT116RNA_2.fastq
Approx 65% complete for WHY_HCT116RNA_2.fastq
Approx 70% complete for WHY_HCT116RNA_2.fastq
Approx 75% complete for WHY_HCT116RNA_2.fastq
Approx 80% complete for WHY_HCT116RNA_2.fastq
Approx 85% complete for WHY_HCT116RNA_2.fastq
Approx 90% complete for WHY_HCT116RNA_2.fastq
Approx 95% complete for WHY_HCT116RNA_2.fastq
Analysis complete for WHY_HCT116RNA_2.fastq
/Share/home/wangdong/lss/project/tmp/test/Human_exon_1.0ST.fastq
Started analysis of Human_exon_1.0ST.fastq
Approx 5% complete for Human_exon_1.0ST.fastq
Approx 10% complete for Human_exon_1.0ST.fastq
Approx 15% complete for Human_exon_1.0ST.fastq
Approx 20% complete for Human_exon_1.0ST.fastq
Approx 25% complete for Human_exon_1.0ST.fastq
Approx 30% complete for Human_exon_1.0ST.fastq
Approx 35% complete for Human_exon_1.0ST.fastq
Approx 40% complete for Human_exon_1.0ST.fastq
Approx 45% complete for Human_exon_1.0ST.fastq
Approx 50% complete for Human_exon_1.0ST.fastq
Approx 55% complete for Human_exon_1.0ST.fastq
Approx 60% complete for Human_exon_1.0ST.fastq
Approx 65% complete for Human_exon_1.0ST.fastq
Approx 70% complete for Human_exon_1.0ST.fastq
Approx 75% complete for Human_exon_1.0ST.fastq
Approx 80% complete for Human_exon_1.0ST.fastq
Approx 85% complete for Human_exon_1.0ST.fastq
Approx 90% complete for Human_exon_1.0ST.fastq
Approx 95% complete for Human_exon_1.0ST.fastq
Analysis complete for Human_exon_1.0ST.fastq

python glob model

说明：

1、glob是python自己带的一个文件操作相关模块，用它可以查找符合自己目的的文件，就类似于Windows下的文件搜索，支持通配符操作，*,?,[]这三个通配符，*代表0个或多个字符，?代表一个字符，[]匹配指定范围内的字符，如[0-9]匹配数字。

它的主要方法就是glob,该方法返回所有匹配的文件路径列表，该方法需要一个参数用来指定匹配的路径字符串（本字符串可以为绝对路径也可以为相对路径），其返回的文件名只包括当前目录里的文件名，不包括子文件夹里的文件。

比如：

glob.glob(r'c:\*.txt')

我这里就是获得C盘下的所有txt文件

glob.glob(r'E:\pic\*\*.jpg')

获得指定目录下的所有jpg文件

使用相对路径：

glob.glob(r'../*.py')

2、iglob方法：

获取一个可编历对象，使用它可以逐个获取匹配的文件路径名。与glob.glob()的区别是：glob.glob同时获取所有的匹配路径，而 glob.iglob一次只获取一个匹配路径。这有点类似于.NET中操作数据库用到的DataSet与DataReader。下面是一个简单的例子：
 
#父目录中的.py文件
f = glob.iglob(r'../*.py')

print f #<generator object iglob at 0x00B9FF80>

for py in f:
    print py

 

官方说明：
glob.glob(pathname)
Return a possibly-empty list of path names that match pathname, which must be a string containing a path specification. pathname can be either absolute (like /usr/src/Python-1.5/Makefile) or relative (like http://www.cnblogs.com/Tools/*/*.gif), and can contain shell-style wildcards. Broken symlinks are included in the results (as in the shell).
glob.iglob(pathname)
Return an iterator which yields the same values as glob() without actually storing them all simultaneously.

New in version 2.5.

For example, consider a directory containing only the following files: 1.gif, 2.txt, andcard.gif. glob() will produce the following results. Notice how any leading components of the path are preserved.

>>> import glob
>>> glob.glob('./[0-9].*')
['./1.gif', './2.txt']
>>> glob.glob('*.gif')
['1.gif', 'card.gif']
>>> glob.glob('?.gif')
['1.gif']


在许多编程语言中都包含有格式化字符串的功能，比如C和Fortran语言中的格式化输入输出。Python中内置有对字符串进行格式化的操作%。

 

模板

格式化字符串时，Python使用一个字符串作为模板。模板中有格式符，这些格式符为真实值预留位置，并说明真实数值应该呈现的格式。Python用一个tuple将多个值传递给模板，每个值对应一个格式符。

比如下面的例子：

print("I'm %s. I'm %d year old" % ('Vamei', 99))
上面的例子中，

"I'm %s. I'm %d year old" 为我们的模板。%s为第一个格式符，表示一个字符串。%d为第二个格式符，表示一个整数。('Vamei', 99)的两个元素'Vamei'和99为替换%s和%d的真实值。 
在模板和tuple之间，有一个%号分簦它代表了格式化操作17

整个"I'm %s. I'm %d year old" % ('Vamei', 99) 实际上构成一个字符串表达式。我们可以像一个正常的字符串那样，将它赋值给某个变量。比如:

a = "I'm %s. I'm %d year old" % ('Vamei', 99)
print(a)
 

我们还可以用词典来传递真实值。如下：

print("I'm %(name)s. I'm %(age)d year old" % {'name':'Vamei', 'age':99})
可以看到，我们对两个格式符进行了命名。命名使用()括起来。每个命名对应词典的一个key。

 

格式符

格式符为真实值预留位置，并控制显示的格式。格式符可以包含有一个类型码，用以控制显示的类型，如下:

%s    字符串 (采用str()的显示)

%r    字符串 (采用repr()的显示)

%c    单个字符

%b    二进制整数

%d    十进制整数

%i    十进制整数

%o    八进制整数

%x    十六进制整数

%e    指数 (基底写为e)

%E    指数 (基底写为E)

%f    浮点数

%F    浮点数，与上相同

%g    指数(e)或浮点数 (根据显示长度)

%G    指数(E)或浮点数 (根据显示长度)

 

%%    字符"%"

 返回所有匹配的文件路径列表。它只有一个参数pathname，定义了文件路径匹配规则，这里可以是绝对路径，也可以是相对路径。下面是使用glob.glob的例子：

import glob  
  
#获取指定目录下的所有图片  
print glob.glob(r"E:\Picture\*\*.jpg")  
  
#获取上级目录的所有.py文件  
print glob.glob(r'../*.py') #相对路径  

 print(glob.glob(r'/Users/lishasha/Desktop/home/script/*.r'))
['/Users/lishasha/Desktop/home/script/a_cor_cluster.r', '/Users/lishasha/Desktop/home/script/correlation.r', '/Users/lishasha/Desktop/home/script/correlation_again 2.r', '/Users/lishasha/Desktop/home/script/correlation_again.r', '/Users/lishasha/Desktop/home/script/correlation_dot_size.r', '/Users/lishasha/Desktop/home/script/correlation_text_modified 2.r', '/Users/lishasha/Desktop/home/script/correlation_text_modified.r', '/Users/lishasha/Desktop/home/script/corrrelation_modified_by_me.r', '/Users/lishasha/Desktop/home/script/heatmapV5.1.r', '/Users/lishasha/Desktop/home/script/net_work.r', '/Users/lishasha/Desktop/home/script/pheatmapV1 2.r', '/Users/lishasha/Desktop/home/script/pheatmapV1.r', '/Users/lishasha/Desktop/home/script/volcanoplot_corrected.r']


>> import glob
>>> f=glob.glob(r'*.py')
>>> print(f)
['probe_pick_for_well.py', 'test.py', 'test2.py']



亲，如何表示一个数可以被24整除？
逗你玩  20:40:26
if n/／24=？ is true
Nil  21:20:28
if n//24 == n/24:

1   2   3   4   5
6   7   8   9   10
11  12  13  14  15
program:

with open('in.txt') as f:
  lis = [x.split() for x in f]

for x in zip(*lis):
  for y in x:
    print(y+'\t', end='')
  print('\n')
output:

1   6   11  

2   7   12  

3   8   13  

4   9   14  

5   10  15

range的用法，python
>>> range(1,5) #代表从1到5(不包含5)
[1, 2, 3, 4]
>>> range(1,5,2) #代表从1到5，间隔2(不包含5)
[1, 3]
>>> range(5) #代表从0到5(不包含5)
[0, 1, 2, 3, 4]
再看看list的操作:

array = [1, 2, 5, 3, 6, 8, 4]
#其实这里的顺序标识是
[1, 2, 5, 3, 6, 8, 4]
(0，1，2，3，4，5，6)
(-7,-6,-5,-4,-3,-2,-1)
 
>>> array[0:] #列出0以后的
[1, 2, 5, 3, 6, 8, 4]
>>> array[1:] #列出1以后的
[2, 5, 3, 6, 8, 4]
>>> array[:-1] #列出-1之前的
[1, 2, 5, 3, 6, 8]
>>> array[3:-3] #列出3到-3之间的
[3]
 

那么两个[::]会是什么那？

>>> array[::2]
[1, 5, 6, 4]
>>> array[2::]
[5, 3, 6, 8, 4]
>>> array[::3]
[1, 3, 4]
>>> array[::4]
[1, 6] 
如果想让他们颠倒形成reverse函数的效果
>>> array[::-1]
[4, 8, 6, 3, 5, 2, 1]
>>> array[::-2]
[4, 6, 5, 1]
感觉自己懂了吧，那么来个冒泡吧：

array = [1, 2, 5, 3, 6, 8, 4]
for i in range(len(array) - 1, 0, -1):
    print i
    for j in range(0, i):
        print j
        if array[j] > array[j + 1]:
            array[j], array[j + 1] = array[j + 1], array[j]
print array
一行一行的来看：

line 1：array = [1, 2, 5, 3, 6, 8, 4]一个乱序的list没什么好解释的

line 2：for i in range(len(array) - 1, 0, -1):这就是上边给的例子的第二条，我们替换下就成为range(6,1,-1)，意思是从6到1间隔-1,也就是倒叙的range(2,7,1),随后把这些值循环赋给i，那么i的值将会是[6, 5, 4, 3, 2]

line 3：for j in range(0, i):这是一个循环赋值给j，j的值将会是[0, 1, 2, 3, 4, 5][0, 1, 2, 3, 4][0, 1, 2, 3][0, 1, 2][0, 1]
那么上边两个循环嵌套起来将会是

i------------6
j------------0j------------1j------------2j------------3j------------4j------------5

i------------5
j------------0j------------1j------------2j------------3j------------4
i------------4
j------------0j------------1j------------2j------------3
i------------3
j------------0j------------1j------------2
i------------2
j------------0j------------1

line 4：if array[j] > array[j + 1]:

>>> array = [1, 2, 5, 3, 6, 8, 4]
>>> array[0]
1
>>> array[1]
2
>>> array[2]
5
>>> array[3]
3
>>> array[4]
6
>>> array[5]
8
>>> array[6]
4
其实・就是使用这个把这个没有顺序的array = [1, 2, 5, 3, 6, 8, 4]排序

line 5：array[j], array[j + 1] = array[j + 1], array[j] 替换赋值

line 6：打印出来

其实要想省事儿，sort()函数一句就能搞定.......

File /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/Data/Intensities/BaseCalls/L005/C127.1/s_5_1101.bcl does not exist
实际上存在s_5_1101.bcl.gz，只是他自己没有解压，出问题了

针对6个barcode，
configureBclToFastq.pl --input-dir /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/Data/Intensities/BaseCalls --output /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/Data/Intensities/BaseCalls/barcode6 --sample-sheet /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/Data/Intensities/BaseCalls/SampleSheet_barcode6.csv --no-eamss --use-bases-mask Y126,I6n,Y126
对7个barcode
configureBclToFastq.pl --input-dir /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/Data/Intensities/BaseCalls --output /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/Data/Intensities/BaseCalls/barcode7 --sample-sheet /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/Data/Intensities/BaseCalls/SampleSheet_barcode7.csv --no-eamss --use-bases-mask Y126,I7n,Y126

实际上转化还需要其他许多文件
 INFO: Creating directory '/Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/Data/Intensities/BaseCalls/Unaligned'
[2015-06-09 10:38:52]   [configureBclToFastq.pl]        WARNING: Missing /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/Data/Intensities/BaseCalls/../RTAConfiguration.xml
[2015-06-09 10:38:53]   [configureBclToFastq.pl]        INFO: Basecalling software: RTA
[2015-06-09 10:38:53]   [configureBclToFastq.pl]        INFO:              version: 1.18 (build 64)
[2015-06-09 10:38:53]   [configureBclToFastq.pl]        WARNING: Missing /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/Data/Intensities/BaseCalls/../RTAConfiguration.xml
[2015-06-09 10:38:53]   [configureBclToFastq.pl]        WARNING: Missing /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/Data/Intensities/BaseCalls/../RTAConfiguration.xml
[2015-06-09 10:38:53]   [configureBclToFastq.pl]        WARNING: Couldn't find run info in /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/Data/Intensities/BaseCalls/../../../RunInfo.xml
[2015-06-09 10:38:53]   [configureBclToFastq.pl]        WARNING: Couldn't find RunInfo.xml for /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/Data/Intensities/BaseCalls
[2015-06-09 10:38:53]   [configureBclToFastq.pl]        INFO: Original use-bases mask: Y126,I7n,Y126
[2015-06-09 10:38:53]   [configureBclToFastq.pl]        INFO: Guessed use-bases mask: Y126,I7n,Y126
[2015-06-09 10:38:53]   [configureBclToFastq.pl]        ERROR: barcode CCGTCC for lane 5 has length 6: expected barcode lenth (including delimiters) is 1
 

grep -A 3 ':N:0:AGGTTA' B1_2.fastq | awk '$1 != "--"' >> /Work1/home/wangdong/lss/20141021/Undetermined_indices/test/B1_2_t.fastq
grep -A 3 ':N:0:GTCTTA' B2_2.fastq | awk '$1 != "--"' >> /Work1/home/wangdong/lss/20141021/Undetermined_indices/test/B2_2_t.fastq
grep -A 3 ':N:0:ACCTTA' B3_2.fastq | awk '$1 != "--"' >> /Work1/home/wangdong/lss/20141021/Undetermined_indices/test/B3_2_t.fastq
grep -A 3 ':N:0:GGATTA' B4_2.fastq | awk '$1 != "--"' >> /Work1/home/wangdong/lss/20141021/Undetermined_indices/test/B4_2_t.fastq
grep -A 3 ':N:0:AAATTA' B5_2.fastq | awk '$1 != "--"' >> /Work1/home/wangdong/lss/20141021/Undetermined_indices/test/B5_2_t.fastq
grep -A 3 ':N:0:GCTGTA' B6_2.fastq | awk '$1 != "--"' >> /Work1/home/wangdong/lss/20141021/Undetermined_indices/test/B6_2_t.fastq
grep -A 3 ':N:0:ATGGTA' B7_2.fastq | awk '$1 != "--"' >> /Work1/home/wangdong/lss/20141021/Undetermined_indices/test/B7_2_t.fastq
grep -A 3 ':N:0:GAGGTA' B8_2.fastq | awk '$1 != "--"' >> /Work1/home/wangdong/lss/20141021/Undetermined_indices/test/B8_2_t.fastq
grep -A 3 ':N:0:GCGCTA' C1_2.fastq | awk '$1 != "--"' >> /Work1/home/wangdong/lss/20141021/Undetermined_indices/test/C1_2_t.fastq
grep -A 3 ':N:0:ATCCTA' C2_2.fastq | awk '$1 != "--"' >> /Work1/home/wangdong/lss/20141021/Undetermined_indices/test/C2_2_t.fastq
grep -A 3 ':N:0:GACCTA' C3_2.fastq | awk '$1 != "--"' >> /Work1/home/wangdong/lss/20141021/Undetermined_indices/test/C3_2_t.fastq
grep -A 3 ':N:0:AGACTA' C4_2.fastq | awk '$1 != "--"' >> /Work1/home/wangdong/lss/20141021/Undetermined_indices/test/C4_2_t.fastq
grep -A 3 ':N:0:GTTATA' C5_2.fastq | awk '$1 != "--"' >> /Work1/home/wangdong/lss/20141021/Undetermined_indices/test/C5_2_t.fastq
grep -A 3 ':N:0:ACTATA' C6_2.fastq | awk '$1 != "--"' >> /Work1/home/wangdong/lss/20141021/Undetermined_indices/test/C6_2_t.fastq
grep -A 3 ':N:0:GGGCAC' SWS1_2.fastq | awk '$1 != "--"' >> /Work1/home/wangdong/lss/20141021/Undetermined_indices/test/SWS1_2_t.fastq
grep -A 3 ':N:0:ATCTCG' SWS2_2.fastq | awk '$1 != "--"' >> /Work1/home/wangdong/lss/20141021/Undetermined_indices/test/SWS2_2_t.fastq
grep -A 3 ':N:0:GAACAC' SWT4_2.fastq | awk '$1 != "--"' >> /Work1/home/wangdong/lss/20141021/Undetermined_indices/test/SWT4_2_t.fastq

sam种reads在第10个域，提取方法如下
cut -f 10 SW_Competition1_100_2.sam > test4.fa

怎么看C集群所有人的任务我也想知道怎么看所有用户。。现在就知道－u加指定用户，－f看所有节点状态。。
tiangeng-郎继东  10:29:28
qstat -f -u '*'

microarray数据包
combat做normalize，limma做差异表达
a<- read.table("plot_com1_10.csv",head=TRUE,sep=";")
pdf("COM100_2.pdf")
plot(a$old_value,a$Competition1_10_2,lty=2,pch=20,cex=0.5,col=a$clour,xlab="before competition(log10(hits+1))",ylab="1/10 competition(log10(hits+1))")
dev.off()

复制重复时要命名为不一样的

02Unix系统里，每行结尾只有“<换行>”，即“\n”；Windows系统里面，每行结尾是“<换行><回车>”，即“\n\r”；Mac系统里，每行结尾是“<回车>”。一个直接后果是，Unix/Mac系统下的文件在Windows里打开的话，所有文字会变成一行；而Windows里的文件在Unix/Mac下打开的话，在每行的结尾可能会多出一个^M符号。

生成反向互补序列的诸多方法python
lt='CATGCATCGT'
def func1(liststr):
	t=list(liststr);d=[]
	dct={'A':'T','T':'A','C':'G','G':'C'}
	for x in range(len(t)):
	    d.append(dct[t.pop()])
	return d 
其他的都简单，1.直接字符串反向处理，再逐一翻译；2.用正负数来处理，这个对于大量的任务可以提高效率；3.两遍处理，True、False开关；4.列表内替换，然后反向；5.成对换位，不过效率低下； 6.还有就是直接的字符串替换，然后一个切片s[::-1]就OK了 ； 
lt='CATGCATCGT'
lt=lt.replace('A','{A}').replace('T','{T}').replace('C','{C}').replace('G','{G}')
result=lt.format(A='T',T='A',C='G',G='C')[::-1]

在探针用 Tm值进行过滤时，由于minor会出现长度不一的现象，所以，会报错, 可能原因还是均要转化为小写，因为苹果里面python需要区分大小写
另外探针最后一条少1bp的原因是由于没有去掉每行的换行符，所以占用了一个字符each=each.strip('\n')
Traceback (most recent call last):
  File "/Users/lishasha/Desktop/home/old_computer/E/lss/work/oligo_design/1/chr/extracted/all_minor/oligo-filtered_by_Tm.py", line 27, in <module>
    Tm_r=4*(nc+ng)+2*(na+nt)
NameError: name 'nc' is not defined


快捷键 Shift+Command+G就可以去自己想去的文件夹，mac下
基因集富集分析 (Gene Set Enrichment Analysis, GSEA) 的基本思想是使用预定义的基因集，通常来自功能注释或先前实验的结果，将基因按照在两类样本中的差异表达程度排序，然后检验预先设定的基因集合是否在这个排序表的顶端或者底端富集。基因集合富集分析检测基因集合而不是单个基因的表达变化，因此可以包含这些细微的表达变化，预期得到更为理想的结果。


configureBclToFastq.pl --input-dir /Share/home/wangdong/data/rawdata/150520_C00126_0190_AHFNKYADXX/Data/Intensities/BaseCalls --output /Share/home/wangdong/data/rawdata/150520_C00126_0190_AHFNKYADXX/Unaligned --sample-sheet /Share/home/wangdong/data/rawdata/150520_C00126_0190_AHFNKYADXX/SampleSheet_undetermined.csv --no-eamss
cd /Share/home/wangdong/data/rawdata/150520_C00126_0190_AHFNKYADXX/Unaligned
nohup make -j 8
Permutation test 置换检验是Fisher于20世纪30年代提出的一种基于大量计算（computationally intensive），利用样本数据的全（或随机）排列，进行统计推断的方法，
因其对总体分布自由，应用较为广泛，特别适糜谧芴宸植嘉粗的小样本资料，以及某些难以用常规方法分析资料的假设检验问题。在具体使用上它和Bootstrap Methods类似，
通过对样本进行顺序上的置换，重新计算统计检验量，构造经验分布，然后在此基础上求出P-value进行推断。

plot（）不同点不同颜色，更多参考寝室实验记录
a<- read.table("icg001_10um_dmso.txt",head=TRUE,sep="\t")
pdf("icg001_dmso3.pdf")
plot(a$RNA_12h_icg001_10um_clc,a$RNA_12h_dmso_clc,lty=2,pch=20,cex=0.5,col=a$clour,main="expression difference between icg001 and dmso",xlab="hits(icg001)",ylab="hots(dmso)")
dev.off()

改变dot的大小
自己是这样解决的
 panel.points<-function(x,y)
 42 {
 43   points(x,y,cex=0.5)
 44 }
 pairs(x,lower.panel=panel.points,upper.panel=panel.cor)
 
I managed to make a plot with the r function "pairs" (the value of the Kendall tau's in the lower panel and pairs-plots in upper panel). Still i have one problem. How do I change the size of the dots in my upper panel? This is my code.

panel.Kendall <- function(x,y,digits=2, prefix="", cex.cor)
{
usr <- par("usr"); on.exit(par(usr))
par(usr = c(0, 1, 0, 1))
r <- Kendall(x, y)$tau
txt <- format(c(r, 0.123456789), digits=digits)[1]
txt <- paste(prefix, txt, sep="")
if(missing(cex.cor)) cex <- 0.8/strwidth(txt)
text(0.5, 0.5, txt, cex = cex * 0.5)
}
png("Kendall1.jpg",width=600,height=600,res=100)
pairs(all[c(2,6,8,9,10,11,14,15)],lower.panel=panel.Kendall)
dev.off() 
You can define own function also for the upper.panel= (for example, panel.points) where you set points size with cex=

panel.points<-function(x,y)
{
  points(x,y,cex=3)
}

pairs(iris[,1:4],lower.panel=panel.Kendall,upper.panel=panel.points)

> pairs(iris[1:4], main = "Anderson's Iris Data -- 3 species", pch = 21, bg = c("red", "green3", "blue")[unclass(iris$Species)], lower.panel=NULL, labels=c("SL","SW","PL","PW"), font.labels=2, cex.labels=4.5) 
#可改变中间txt的笮17

pairs {graphics}	R Documentation
Scatterplot Matrices

Description

A matrix of scatterplots is produced.

Usage

pairs(x, ...)

## S3 method for class 'formula'
pairs(formula, data = NULL, ..., subset,
      na.action = stats::na.pass)

## Default S3 method:
pairs(x, labels, panel = points, ...,
      horInd = 1:nc, verInd = 1:nc,
      lower.panel = panel, upper.panel = panel,
      diag.panel = NULL, text.panel = textPanel,
      label.pos = 0.5 + has.diag/3, line.main = 3,
      cex.labels = NULL, font.labels = 1,
      row1attop = TRUE, gap = 1, log = "")
Arguments

x	
the coordinates of points given as numeric columns of a matrix or data frame. Logical and factor columns are converted to numeric in the same way that data.matrix does.

formula	
a formula, such as ~ x + y + z. Each term will give a separate variable in the pairs plot, so terms should be numeric vectors. (A response will be interpreted as another variable, but not treated specially, so it is confusing to use one.)

data	
a data.frame (or list) from which the variables in formula should be taken.

subset	
an optional vector specifying a subset of observations to be used for plotting.

na.action	
a function which indicates what should happen when the data contain NAs. The default is to pass missing values on to the panel functions, but na.action = na.omit will cause cases with missing values in any of the variables to be omitted entirely.

labels	
the names of the variables.

panel	
function(x, y, ...) which is used to plot the contents of each panel of the display.

...	
arguments to be passed to or from methods.

Also, graphical parameters can be given as can arguments to plot such as main. par("oma") will be set appropriately unless specified.

horInd, verInd	
The (numerical) indices of the variables to be plotted on the horizontal and vertical axes respectively.

lower.panel, upper.panel	
separate panel functions (or NULL) to be used below and above the diagonal respectively.

diag.panel	
optional function(x, ...) to be applied on the diagonals.

text.panel	
optional function(x, y, labels, cex, font, ...) to be applied on the diagonals.

label.pos	
y position of labels in the text panel.

line.main	
if main is specified, line.main gives the line argument to mtext() which draws the title. You may want to specify oma when changing line.main.

cex.labels, font.labels	
graphics parameters for the text panel.

row1attop	
logical. Should the layout be matrix-like with row 1 at the top, or graph-like with row 1 at the bottom?

gap	
distance between subplots, in margin lines.

log	
a character string indicating if logarithmic axes are to be used: see plot.default. log = "xy" specifies logarithmic axes for all variables.

Details

The ijth scatterplot contains x[,i] plotted against x[,j]. The scatterplot can be customised by setting panel functions to appear as something completely different. The off-diagonal panel functions are passed the appropriate columns of x as x and y: the diagonal panel function (if any) is passed a single column, and the text.panel function is passed a single (x, y) location and the column name. Setting some of these panel functions to NULL is equivalent to not drawing anything there.

The graphical parameters pch and col can be used to specify a vector of plotting symbols and colors to be used in the plots.

The graphical parameter oma will be set by pairs.default unless supplied as an argument.

A panel function should not attempt to start a new plot, but just plot within a given coordinate system: thus plot and boxplot are not panel functions.

By default, missing values are passed to the panel functions and will often be ignored within a panel. However, for the formula method and na.action = na.omit, all cases which contain a missing values for any of the variables are omitted completely (including when the scales are selected).

Arguments horInd and verInd were introduced in R 3.2.0. If given the same value they can be used to select or re-order variables: with different ranges of consecutive values they can be used to plot rectangular windows of a full pairs plot; in the latter case ‘diagonal’ refers to the diagonal of the full plot.

Author(s)

Enhancements for R 1.0.0 contributed by Dr. Jens Oehlschlaegel-Akiyoshi and R-core members.

References

Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988) The New S Language. Wadsworth & Brooks/Cole.

Examples

pairs(iris[1:4], main = "Anderson's Iris Data -- 3 species",
      pch = 21, bg = c("red", "green3", "blue")[unclass(iris$Species)])  # 可给散点图加颜色/Work1/home/wangdong/lss/data/sw_20150110/SW_16_Samples_R／Rplots.pdf，空心散点，实际上那个correlation.r就是用pairs来做图的,按照species里面的几个因素

## formula method
pairs(~ Fertility + Education + Catholic, data = swiss,
      subset = Education < 20, main = "Swiss data, Education < 20") 

pairs(USJudgeRatings)
## show only lower triangle (and suppress labeling for whatever reason):
pairs(USJudgeRatings, text.panel = NULL, upper.panel = NULL)

## put histograms on the diagonal
panel.hist <- function(x, ...)
{
    usr <- par("usr"); on.exit(par(usr))
    par(usr = c(usr[1:2], 0, 1.5) )
    h <- hist(x, plot = FALSE)
    breaks <- h$breaks; nB <- length(breaks)
    y <- h$counts; y <- y/max(y)
    rect(breaks[-nB], 0, breaks[-1], y, col = "cyan", ...)
}
pairs(USJudgeRatings[1:5], panel = panel.smooth,	
      cex = 1.5, pch = 24, bg = "light blue",
      diag.panel = panel.hist, cex.labels = 2, font.labels = 2)  ＃这儿可以改变字体，颜色大小等等
## put (absolute) correlations on the upper panels,
## with size proportional to the correlations.
panel.cor <- function(x, y, digits = 2, prefix = "", cex.cor, ...)
{
    usr <- par("usr"); on.exit(par(usr))
    par(usr = c(0, 1, 0, 1))
    r <- abs(cor(x, y))
    txt <- format(c(r, 0.123456789), digits = digits)[1]
    txt <- paste0(prefix, txt)
    if(missing(cex.cor)) cex.cor <- 0.8/strwidth(txt)
    text(0.5, 0.5, txt, cex = cex.cor * r)
}
pairs(USJudgeRatings, lower.panel = panel.smooth, upper.panel = panel.cor)

pairs(iris[-5], log = "xy") # plot all variables on log scale
pairs(iris, log = 1:4, # log the first four
      main = "Lengths and Widths in [log]", line.main=1.5, oma=c(2,2,3,2))


结束

      


pairs.panels {psych}

SPLOM, histograms and correlations for a data matrix
Package:  psych
Version:  1.4.2.3
Description
Adapted from the help page for pairs, pairs.panels shows a scatter plot of matrices (SPLOM), with bivariate scatter plots below the diagonal, histograms on the diagonal, and the Pearson correlation above the diagonal. Useful for descriptive statistics of small data sets. If lm=TRUE, linear regression fits are shown for both y by x and x by y. Correlation ellipses are also shown. Points may be given different colors depending upon some grouping variable.

Usage
 
## S3 method for class 'panels':
pairs((x, smooth = TRUE, scale = FALSE, density=TRUE,ellipses=TRUE,
     digits = 2,method="pearson", pch = 20,lm=FALSE, 
     cor=TRUE,jiggle=FALSE,factor=2,hist.col="cyan",show.points=TRUE,rug=TRUE, ...))

Arguments
x
a data.frame or matrix
smooth
TRUE draws loess smooths
scale
TRUE scales the correlation font by the size of the absolute correlation.
density
TRUE shows the density plots as well as histograms
ellipses
TRUE draws correlation ellipses
lm
Plot the linear fit rather than the LOESS smoothed fits.
digits
the number of digits to show
method
method parameter for the correlation ("pearson","spearman","kendall")
pch
The plot character (defaults to 20 which is a '.').
cor
If plotting regressions, should correlations be reported?
jiggle
Should the points be jittered before plotting?
factor
factor for jittering (1-5)
hist.col
What color should the histogram on the diagonal be?
show.points
If FALSE, do not show the data points, just the data ellipses and smoothed functions
rug
if TRUE (default) draw a rug under the histogram, if FALSE, don't draw the rug
...
other options for pairs
Details
Shamelessly adapted from the pairs help page. Uses panel.cor, panel.cor.scale, and panel.hist, all taken from the help pages for pairs. Also adapts the ellipse function from John Fox's car package.

pairs.panels is most useful when the number of variables to plot is less than about 6-10. It is particularly useful for an initial overview of the data.

To show different groups with different colors, use a plot character (pch) between 21 and 25 and then set the background color to vary by group. (See the second example).

When plotting more than about 10 variables, it is useful to set the gap parameter to something less than 1 (e.g., 0). Alternatively, consider using cor.plot

In addition, when plotting more than about 100-200 cases, it is useful to set the plotting character to be a point. (pch=".")

Sometimes it useful to draw the correlation ellipses and best fitting lowess without the points. (points.false=TRUE).

Values
A scatter plot matrix (SPLOM) is drawn in the graphic window. The lower off diagonal draws scatter plots, the diagonal histograms, the upper off diagonal reports the Pearson correlation (with pairwise deletion).

If lm=TRUE, then the scatter plots are drawn above and below the diagonal, each with a linear regression fit. Useful to show the difference between regression lines.

See Also
pairs which is the base from which pairs.panels is derived, cor.plot to do a heat map of correlations, and scatter.hist to draw a single correlation plot with histograms and best fitted lines.

Examples
pairs.panels(attitude)   #see the graphics window
data(iris)
pairs.panels(iris[1:4],bg=c("red","yellow","blue")[iris$Species],
        pch=21,main="Fisher Iris data by Species") #to show color grouping
 
pairs.panels(iris[1:4],bg=c("red","yellow","blue")[iris$Species],
   pch=21,main="Fisher Iris data by Species",hist.col="red") 
   #to show changing the diagonal
 
#demonstrate not showing the data points
data(sat.act)
pairs.panels(sat.act,show.points=FALSE)
#better yet is to show the points as a period
pairs.panels(sat.act,pch=".")
#show many variables with 0 gap between scatterplots
# data(bfi)
# pairs.panels(bfi,show.points=FALSE,gap=0)

sort above_six_A.fa | uniq -u >> above_6A_unique.fa

sw相关图所在路径（ibm-a）
/Work1/home/wangdong/lss/data/sw_20150110/SW_16_Samples_R

Error bars are a graphical representation of the variability of data and are used on graphs to indicate the error, or uncertainty in a reported measurement. They give a general idea of how precise a measurement is, or conversely, how far from the reported value the true (error free) value might be. Error bars often represent one standard deviation of uncertainty, one standard error, or a certain confidence interval (e.g., a 95% interval). These quantities are not the same and so the measure selected should be stated explicitly in the graph or supporting text.

Error bars can be used to compare visually two quantities if various other conditions hold. This can determine whether differences are statistically significant. Error bars can also suggest goodness of fit of a given function, i.e., how well the function describes the data

标准分数（Standard Score，又称z-score，中文称为Z-分数或标准化值）在统计学中是一种无因次值，是借由从单一（原始）分数中减去母体的平均值，再依照母体（母集合）的标准差分割成不同的差距。

目录  [隐藏] 
1 概念
2 数理统计学中的标准化
3 应用
4 外部链接
5 参见
概念[编辑]
标准分数与使用在高速筛选分析中的“Z-因数”（z-factor）不同，甚至有时两者会互相混淆。

其约化过程被称为“标准化”（standardizing）。

标准分数可借由以下公式求出：

 z = {x - \mu \over \sigma}
其中 \sigma \ne 0。

其中

x\, 是需要被标准化的原始分数
\mu\, 是母体的平均值
\sigma\, 是母体的标准差
Z值的量代表着原始分数和母体平均值之间的距离，是以标准差为单位计算。在原始分数低于平均值时Z则为负数，反之则为正数。

关键点是，计算Z值时需要“母体”的平均值和标准差，而不是“样本”的平均值和标准差。因此需要了解母体的统计数据资料。

但是要确实了解母体真正的标准差往往是不切实际的，除非是在“标准化测验”（Standardized testing）之类的情形中，整个母体都是经过测量的。在其他情况中，几乎不可能测量母体的每一个组成单位，因此通常会使用随机的样本来评估标准差。例如：“有吸烟习惯的总人数”就不是经过一个一个测量而得出的。

当母体为正态分布时，其百分位数可能是由标准分数和普通表格所决定的。

数理统计学中的标准化[编辑]
在数理统计学中，随机变数“X”是使用理论（母体）的平均值和标准差所标准化的结果：

Z = {X - \mu \over \sigma}
其中 μ = E(X) 为平均值、σ05 = Var(X) X的概率分布之方差

若随机变数无法确定时，则为算术平均数：

\bar{X}={1 \over n} \sum_{i=1}^n X_i
因此经过标准化的结果为：

Z={\bar{X}-\mu\over\sigma/\sqrt{n}}.
应用[编辑]

 reads processed: 2021918
# reads with at least one reported alignment: 336945 (16.66%)
# reads that failed to align: 1684973 (83.34%)
Reported 336945 alignments to 1 output stream(s)


1.Linux下

增加一下三行：

给你的ls一点颜色

[html] view plaincopy

    alias ls='/bin/ls --color=auto'  


给你的grep一点颜色

[html] view plaincopy

    alias grep='grep --color'  


给你的vi一点颜色

[html] view plaincopy

    alias vi='vim'  


2.Mac下

增加一下三行：

给你的ls一点颜色

[html] view plaincopy

    alias ls='/bin/ls -G'  


给你的grep一点颜色

[html] view plaincopy

    alias grep='grep --color'  


给你的vi一点颜色

[html] view plaincopy

    alias vi='vim'  


3.修改颜色

修改/etc/profile
加入

[html] view plaincopy

    export CLICOLOR=1  
      
    export LSCOLORS=gxfxaxdxcxegedabagacad  

CLICOLOR是用来设置是否进行颜色的显示。CLI是Command Line Interface的缩写。

LSCOLORS是用来设置当CLICOLOR被启用后，各种文件类型的颜色。LSCOLORS的值中每两个字母为一组，分别设置某个文件类型的文字颜色和背景颜色。LSCOLORS中一共11组颜色设置，按照先后顺序，分别对以下的文件类型进行设置：

    [html] view plaincopy
        directory  
        symbolic link  
        socket  
        pipe  
        executable  
        block special  
        character special  
        executable with setuid bit set  
        executable with setgid bit set  
        directory writable to others, with sticky bit  
        directory writable to others, without sticky bit  

LSCOLORS中，字母代表的颜色如下：

    [html] view plaincopy
        a 黑色  
        b 红色  
        c 绿色  
        d 棕色  
        e 蓝色  
        f 洋红色  
        g 青色  
        h 浅灰色  
        A 黑色粗体  
        B 红色粗体  
        C 绿色粗体  
        D 棕色粗体  
        E 蓝色粗体  
        F 洋红色粗体  
        G 青色粗体  
        H 浅灰色粗体  
        

Vim

Vim 的配色最好和终端的配色保持一致，不然在 Terminal/iTerm2 里使用命令行 Vim 会很别扭：

$ cd solarized
$ cd vim-colors-solarized/colors
$ mkdir -p ~/.vim/colors
$ cp solarized.vim ~/.vim/colors/

$ vi ~/.vimrc
syntax enable
set background=dark
colorscheme solarized
ls

vimrc是vim的配置文件，是启动vim的时候自动读取的，不需要也不能够在终端里面运行或者像bashrc那样导入，他们不是一样的东西。vimrc是不能source的，source是对bash的相关设置用的，对vim不会起作用。

Mac OS X 是基于 FreeBSD 的，所以一些工具 ls, top 等都是 BSD 那一套，ls 不是 GNU ls，所以即使 Terminal/iTerm2 配置了颜色，但是在 Mac 上敲入 ls 命令也不会显示高亮，可以通过安装 coreutils 来解决（brew install coreutils），不过如果对 ls 颜色不挑剔的话有个简单办法就是在 .bash_profile 里输出 CLICOLOR=1：

$ vi ~/.bash_profile
export CLICOLOR=1



mkdir的-p选项允许你一次性创建多层次的目录，而不是一次只创建单独的目录。例如，我们要在当前目录创建目录Projects/a/src，使用命令

mkdir -p Project/a/src

git clone git://github.com/altercation/solarized.git /Users/lishasha/Desktop/home/tools
fatal: destination path '/Users/lishasha/Desktop/home/tools' already exists and is not an empty directory.
git clone  可以指定下载所在文件夹，但是要是一个空的文件夹

bowtie -n 3 /Share/home/wangdong/lss/project/20150430_capitalbio_AR/misligation/capital_bio_misligation /Share/home/wangdong/lss/project/20150430_capitalbio_AR/misligation/unmatched_reads2.fastq -S /Share/home/wangdong/lss/project/20150430_capitalbio_AR/misligation/unmatched_reads.sam
bowtie -n 3 /Share/home/wangdong/lss/project/20150430_sw_probes_screen/ref_1411 /Share/home/wangdong/data/rawdata/150429_C00126_0187_AHFMG2ADXX/Project_HFMG2ADXX/fastq/Sample_SW_our_lab_probe.fastq -S /Share/home/wangdong/lss/project/20150430_sw_probes_screen/mapping/Sample_SW_our_lab_probe_tmp.sam

/Share/home/wangdong/local/app/bowtie-1.1.1/bowtie-build -f /Share/home/wangdong/lss/project/20150430_capitalbio_AR/misligation/capital_bio_misligation.fa /Share/home/wangdong/lss/project/20150430_capitalbio_AR/misligation/capital_bio_misligation
/Share/home/wangdong/local/app/bowtie-1.1.1/bowtie-build -f /Share/home/wangdong/lss/project/20150430_capitalbio_AR/misligation/capital_bio_misligation.fa /Share/home/wangdong/lss/project/20150430_capitalbio_AR/misligation/capital_bio_misligation2
VIM如何将空格替换为换行，比如有时需要把很长的一行按空格分为多行，这时就可以用如下命令实现:

:%s/ +/\r/gc

参数解释：

    %s ：在整个文件范围查找替换
    / ： 分隔符
    + ：匹配空格，其中“ ”表示空格，+表示重复1次或多次，加在一起表示一个或多个空格。
    \r ：换行符
    g ：全局替换
    c ：替换前确认
    
touch unmatched_reads.fastq # --un参数输出没有match的reads,需要建一个空文件unmatched_reads.fastq 
bowtie -n 3 --un /Share/home/wangdong/lss/project/20150430_capitalbio_AR/mapping/unmatched_reads2.fastq /Share/home/wangdong/lss/project/20150430_capitalbio_AR/ref_2920_biocapital /Share/home/wangdong/data/rawdata/150429_C00126_0187_AHFMG2ADXX/Project_HFMG2ADXX/fastq/Sample_SW_captialbio_probe.fastq -S /Share/home/wangdong/lss/project/20150430_capitalbio_AR/mapping/captialbio_TEST.sam

pheatmap
/Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/plot
pheatmap	R Documentation
A function to draw clustered heatmaps.
Description
A function to draw clustered heatmaps where one has better control over some graphical parameters such as cell size, etc.
Usage
  pheatmap(mat,
    color = colorRampPalette(rev(c("#D73027", "#FC8D59", "#FEE090", "#FFFFBF", "#E0F3F8", "#91BFDB", "#4575B4")))(100),
    kmeans_k = NA, breaks = NA, border_color = "grey60",
    cellwidth = NA, cellheight = NA, scale = "none",
    cluster_rows = TRUE, cluster_cols = TRUE,
    clustering_distance_rows = "euclidean",
    clustering_distance_cols = "euclidean",
    clustering_method = "complete",
    treeheight_row = ifelse(cluster_rows, 50, 0),
    treeheight_col = ifelse(cluster_cols, 50, 0),
    legend = TRUE, legend_breaks = NA, legend_labels = NA,
    annotation = NA, annotation_colors = NA,
    annotation_legend = TRUE, drop_levels = TRUE,
    show_rownames = T, show_colnames = T, main = NA,
    fontsize = 10, fontsize_row = fontsize,
    fontsize_col = fontsize, display_numbers = F,
    number_format = "%.2f",
    fontsize_number = 0.8 * fontsize, filename = NA,
    width = NA, height = NA, ...)
Arguments
de>matde>	
numeric matrix of the values to be plotted.
de>colorde>	
vector of colors used in heatmap.
de>kmeans_kde>	
the number of kmeans clusters to make, if we want to agggregate the rows before drawing heatmap. If NA then the rows are not aggregated.
de>breaksde>	
a sequence of numbers that covers the range of values in mat and is one element longer than color vector. Used for mapping values to colors. Useful, if needed to map certain values to certain colors, to certain values. If value is NA then the breaks are calculated automatically.
de>border_colorde>	
color of cell borders on heatmap, use NA if no border should be drawn.
de>cellwidthde>	
inpidual cell width in points. If left as NA, then the values depend on the size of plotting window.
de>cellheightde>	
inpidual cell height in points. If left as NA, then the values depend on the size of plotting window.
de>scalede>	
character indicating if the values should be centered and scaled in either the row direction or the column direction, or none. Corresponding values are de>"row"de>, de>"column"de> and de>"none"de>
de>cluster_rowsde>	
boolean values determining if rows should be clustered,
de>cluster_colsde>	
boolean values determining if columns should be clustered.
de>clustering_distance_rowsde>	
distance measure used in clustering rows. Possible values are de>"correlation"de> for Pearson correlation and all the distances supported by de>distde>, such as de>"euclidean"de>, etc. If the value is none of the above it is assumed that a distance matrix is provided.
de>clustering_distance_colsde>	
distance measure used in clustering columns. Possible values the same as for clustering_distance_rows.
de>clustering_methodde>	
clustering method used. Accepts the same values as de>hclustde>.
de>treeheight_rowde>	
the height of a tree for rows, if these are clustered. Default value 50 points.
de>treeheight_colde>	
the height of a tree for columns, if these are clustered. Default value 50 points.
de>legendde>	
logical to determine if legend should be drawn or not.
de>legend_breaksde>	
vector of breakpoints for the legend.
de>legend_labelsde>	
vector of labels for the de>legend_breaksde>.
de>annotationde>	
data frame that specifies the annotations shown on top of the columns. Each row defines the features for a specific column. The columns in the data and rows in the annotation are matched using corresponding row and column names. Note that color schemes takes into account if variable is continuous or discrete.
de>annotation_colorsde>	
list for specifying annotation track colors manually. It is possible to define the colors for only some of the features. Check examples for details.
de>annotation_legendde>	
boolean value showing if the legend for annotation tracks should be drawn.
de>drop_levelsde>	
logical to determine if unused levels are also shown in the legend
de>show_rownamesde>	
boolean specifying if column names are be shown.
de>show_colnamesde>	
boolean specifying if column names are be shown.
de>mainde>	
the title of the plot
de>fontsizede>	
base fontsize for the plot
de>fontsize_rowde>	
fontsize for rownames (Default: fontsize)
de>fontsize_colde>	
fontsize for colnames (Default: fontsize)
de>display_numbersde>	
logical determining if the numeric values are also printed to the cells.
de>number_formatde>	
format strings (C printf style) of the numbers shown in cells. For example "de>%.2fde>" shows 2 decimal places and "de>%.1ede>" shows exponential notation (see more in de>sprintfde>).
de>fontsize_numberde>	
fontsize of the numbers displayed in cells
de>filenamede>	
file path where to save the picture. Filetype is decided by the extension in the path. Currently following formats are supported: png, pdf, tiff, bmp, jpeg. Even if the plot does not fit into the plotting window, the file size is calculated so that the plot would fit there, unless specified otherwise.
de>widthde>	
manual option for determining the output file width in
de>heightde>	
manual option for determining the output file height in inches.
de>...de>	
graphical parameters for the text used in plot. Parameters passed to de>grid.textde>, see de>gparde>.
Details
The function also allows to aggregate the rows using kmeans clustering. This is advisable if number of rows is so big that R cannot handle their hierarchical clustering anymore, roughly more than 1000. Instead of showing all the rows separately one can cluster the rows in advance and show only the cluster centers. The number of clusters can be tuned with parameter kmeans_k.
Value
Invisibly a list of components
de>tree_rowde> the clustering of rows as de>hclustde> object
de>tree_colde> the clustering of columns as de>hclustde> object
de>kmeansde> the kmeans clustering of rows if parameter de>kmeans_kde> was specified
Author(s)
Raivo Kolde <rkolde@gmail.com>
Examples
# Generate some data
test = matrix(rnorm(200), 20, 10)
test[1:10, seq(1, 10, 2)] = test[1:10, seq(1, 10, 2)] + 3
test[11:20, seq(2, 10, 2)] = test[11:20, seq(2, 10, 2)] + 2
test[15:20, seq(2, 10, 2)] = test[15:20, seq(2, 10, 2)] + 4
colnames(test) = paste("Test", 1:10, sep = "")
rownames(test) = paste("Gene", 1:20, sep = "")

# Draw heatmaps
pheatmap(test)
pheatmap(test, kmeans_k = 2)
pheatmap(test, scale = "row", clustering_distance_rows = "correlation")
pheatmap(test, color = colorRampPalette(c("navy", "white", "firebrick3"))(50))
pheatmap(test, cluster_row = FALSE)
pheatmap(test, legend = FALSE)
pheatmap(test, display_numbers = TRUE)
pheatmap(test, display_numbers = TRUE, number_format = "%.1e")
pheatmap(test, cluster_row = FALSE, legend_breaks = -1:4, legend_labels = c("0",
"1e-4", "1e-3", "1e-2", "1e-1", "1"))
pheatmap(test, cellwidth = 15, cellheight = 12, main = "Example heatmap")
pheatmap(test, cellwidth = 15, cellheight = 12, fontsize = 8, filename = "test.pdf")


# Generate column annotations
annotation = data.frame(Var1 = factor(1:10 %% 2 == 0, labels = c("Class1", "Class2")), Var2 = 1:10)
annotation$Var1 = factor(annotation$Var1, levels = c("Class1", "Class2", "Class3"))
rownames(annotation) = paste("Test", 1:10, sep = "")

pheatmap(test, annotation = annotation)
pheatmap(test, annotation = annotation, annotation_legend = FALSE)
pheatmap(test, annotation = annotation, annotation_legend = FALSE, drop_levels = FALSE)

# Specify colors
Var1 = c("navy", "darkgreen")
names(Var1) = c("Class1", "Class2")
Var2 = c("lightgreen", "navy")

ann_colors = list(Var1 = Var1, Var2 = Var2)

pheatmap(test, annotation = annotation, annotation_colors = ann_colors, main = "Example with all the features")

# Specifying clustering from distance matrix
drows = dist(test, method = "minkowski")
dcols = dist(t(test), method = "minkowski")
pheatmap(test, clustering_distance_rows = drows, clustering_distance_cols = dcols)


R Graphical Manual

Source Code Search
Blog (test)
Help (test)
Browse All
Last data update: 2013.05.20
Data Source
R Release (2.15.3) 
CranContrib 
BioConductor 
All 
Data Type
Packages 
Functions 
Images 
Data set 
pheatmap	R Documentation
A function to draw clustered heatmaps.
Description
A function to draw clustered heatmaps where one has better control over some graphical parameters such as cell size, etc.
Usage
  pheatmap(mat,
    color = colorRampPalette(rev(c("#D73027", "#FC8D59", "#FEE090", "#FFFFBF", "#E0F3F8", "#91BFDB", "#4575B4")))(100),
    kmeans_k = NA, breaks = NA, border_color = "grey60",
    cellwidth = NA, cellheight = NA, scale = "none",
    cluster_rows = TRUE, cluster_cols = TRUE,
    clustering_distance_rows = "euclidean",
    clustering_distance_cols = "euclidean",
    clustering_method = "complete",
    treeheight_row = ifelse(cluster_rows, 50, 0),
    treeheight_col = ifelse(cluster_cols, 50, 0),
    legend = TRUE, legend_breaks = NA, legend_labels = NA,
    annotation = NA, annotation_colors = NA,
    annotation_legend = TRUE, drop_levels = TRUE,
    show_rownames = T, show_colnames = T, main = NA,
    fontsize = 10, fontsize_row = fontsize,
    fontsize_col = fontsize, display_numbers = F,
    number_format = "%.2f",
    fontsize_number = 0.8 * fontsize, filename = NA,
    width = NA, height = NA, ...)
Arguments
de>matde>	
numeric matrix of the values to be plotted.
de>colorde>	
vector of colors used in heatmap.
de>kmeans_kde>	
the number of kmeans clusters to make, if we want to agggregate the rows before drawing heatmap. If NA then the rows are not aggregated.
de>breaksde>	
a sequence of numbers that covers the range of values in mat and is one element longer than color vector. Used for mapping values to colors. Useful, if needed to map certain values to certain colors, to certain values. If value is NA then the breaks are calculated automatically.
de>border_colorde>	
color of cell borders on heatmap, use NA if no border should be drawn.
de>cellwidthde>	
inpidual cell width in points. If left as NA, then the values depend on the size of plotting window.
de>cellheightde>	
inpidual cell height in points. If left as NA, then the values depend on the size of plotting window.
de>scalede>	
character indicating if the values should be centered and scaled in either the row direction or the column direction, or none. Corresponding values are de>"row"de>, de>"column"de> and de>"none"de>
de>cluster_rowsde>	
boolean values determining if rows should be clustered,
de>cluster_colsde>	
boolean values determining if columns should be clustered.
de>clustering_distance_rowsde>	
distance measure used in clustering rows. Possible values are de>"correlation"de> for Pearson correlation and all the distances supported by de>distde>, such as de>"euclidean"de>, etc. If the value is none of the above it is assumed that a distance matrix is provided.
de>clustering_distance_colsde>	
distance measure used in clustering columns. Possible values the same as for clustering_distance_rows.
de>clustering_methodde>	
clustering method used. Accepts the same values as de>hclustde>.
de>treeheight_rowde>	
the height of a tree for rows, if these are clustered. Default value 50 points.
de>treeheight_colde>	
the height of a tree for columns, if these are clustered. Default value 50 points.
de>legendde>	
logical to determine if legend should be drawn or not.
de>legend_breaksde>	
vector of breakpoints for the legend.
de>legend_labelsde>	
vector of labels for the de>legend_breaksde>.
de>annotationde>	
data frame that specifies the annotations shown on top of the columns. Each row defines the features for a specific column. The columns in the data and rows in the annotation are matched using corresponding row and column names. Note that color schemes takes into account if variable is continuous or discrete.
de>annotation_colorsde>	
list for specifying annotation track colors manually. It is possible to define the colors for only some of the features. Check examples for details.
de>annotation_legendde>	
boolean value showing if the legend for annotation tracks should be drawn.
de>drop_levelsde>	
logical to determine if unused levels are also shown in the legend
de>show_rownamesde>	
boolean specifying if column names are be shown.
de>show_colnamesde>	
boolean specifying if column names are be shown.
de>mainde>	
the title of the plot
de>fontsizede>	
base fontsize for the plot
de>fontsize_rowde>	
fontsize for rownames (Default: fontsize)
de>fontsize_colde>	
fontsize for colnames (Default: fontsize)
de>display_numbersde>	
logical determining if the numeric values are also printed to the cells.
de>number_formatde>	
format strings (C printf style) of the numbers shown in cells. For example "de>%.2fde>" shows 2 decimal places and "de>%.1ede>" shows exponential notation (see more in de>sprintfde>).
de>fontsize_numberde>	
fontsize of the numbers displayed in cells
de>filenamede>	
file path where to save the picture. Filetype is decided by the extension in the path. Currently following formats are supported: png, pdf, tiff, bmp, jpeg. Even if the plot does not fit into the plotting window, the file size is calculated so that the plot would fit there, unless specified otherwise.
de>widthde>	
manual option for determining the output file width in
de>heightde>	
manual option for determining the output file height in inches.
de>...de>	
graphical parameters for the text used in plot. Parameters passed to de>grid.textde>, see de>gparde>.
Details
The function also allows to aggregate the rows using kmeans clustering. This is advisable if number of rows is so big that R cannot handle their hierarchical clustering anymore, roughly more than 1000. Instead of showing all the rows separately one can cluster the rows in advance and show only the cluster centers. The number of clusters can be tuned with parameter kmeans_k.
Value
Invisibly a list of components
de>tree_rowde> the clustering of rows as de>hclustde> object
de>tree_colde> the clustering of columns as de>hclustde> object
de>kmeansde> the kmeans clustering of rows if parameter de>kmeans_kde> was specified
Author(s)
Raivo Kolde <rkolde@gmail.com>
Examples
# Generate some data
test = matrix(rnorm(200), 20, 10)
test[1:10, seq(1, 10, 2)] = test[1:10, seq(1, 10, 2)] + 3
test[11:20, seq(2, 10, 2)] = test[11:20, seq(2, 10, 2)] + 2
test[15:20, seq(2, 10, 2)] = test[15:20, seq(2, 10, 2)] + 4
colnames(test) = paste("Test", 1:10, sep = "")
rownames(test) = paste("Gene", 1:20, sep = "")

# Draw heatmaps
pheatmap(test)
pheatmap(test, kmeans_k = 2)
pheatmap(test, scale = "row", clustering_distance_rows = "correlation")
pheatmap(test, color = colorRampPalette(c("navy", "white", "firebrick3"))(50))
pheatmap(test, cluster_row = FALSE)
pheatmap(test, legend = FALSE)
pheatmap(test, display_numbers = TRUE)
pheatmap(test, display_numbers = TRUE, number_format = "%.1e")
pheatmap(test, cluster_row = FALSE, legend_breaks = -1:4, legend_labels = c("0",
"1e-4", "1e-3", "1e-2", "1e-1", "1"))
pheatmap(test, cellwidth = 15, cellheight = 12, main = "Example heatmap")
pheatmap(test, cellwidth = 15, cellheight = 12, fontsize = 8, filename = "test.pdf")


# Generate column annotations
annotation = data.frame(Var1 = factor(1:10 %% 2 == 0, labels = c("Class1", "Class2")), Var2 = 1:10)
annotation$Var1 = factor(annotation$Var1, levels = c("Class1", "Class2", "Class3"))
rownames(annotation) = paste("Test", 1:10, sep = "")

pheatmap(test, annotation = annotation)
pheatmap(test, annotation = annotation, annotation_legend = FALSE)
pheatmap(test, annotation = annotation, annotation_legend = FALSE, drop_levels = FALSE)

# Specify colors
Var1 = c("navy", "darkgreen")
names(Var1) = c("Class1", "Class2")
Var2 = c("lightgreen", "navy")

ann_colors = list(Var1 = Var1, Var2 = Var2)

pheatmap(test, annotation = annotation, annotation_colors = ann_colors, main = "Example with all the features")

# Specifying clustering from distance matrix
drows = dist(test, method = "minkowski")
dcols = dist(t(test), method = "minkowski")
pheatmap(test, clustering_distance_rows = drows, clustering_distance_cols = dcols)
Results

draw clustered heatmaps_pheatmap(热图) - 云之南 - 云之南 draw clustered heatmaps_pheatmap(热图) - 云之南 - 云之南 draw clustered heatmaps_pheatmap(热图) - 云之南 - 云之南 draw clustered heatmaps_pheatmap(热图) - 云之南 - 云之南 draw clustered heatmaps_pheatmap(热图) - 云之南 - 云之南 draw clustered heatmaps_pheatmap(热图) - 云之南 - 云之南 draw clustered heatmaps_pheatmap(韧17) - 云之南 - 云之南 draw clustered heatmaps_pheatmap(热图) - 云之南 - 云之南 draw clustered heatmaps_pheatmap(热图) - 云之南 - 云之南 draw clustered heatmaps_pheatmap(热图) - 云之南 - 云之南 draw clustered heatmaps_pheatmap(热图) - 云之南 - 云之南 draw clustered heatmaps_pheatmap(热图) - 云之南 - 云之南 draw clustered heatmaps_pheatmap(热图) - 云之南 - 云之南 draw clustered heatmaps_pheatmap(热图) - 云之南 - 云之南 draw clustered heatmaps_pheatmap(热图) - 云之南 - 云之南 draw clustered heatmaps_pheatmap(热图) - 云之南 - 云之南

Created & Maintained by Osamu Ogasawara (osamu.ogasawara@gmail.com)

library("pheatmap")
pheatmap(c,
border_color="#FFFAEA",
color=colorRampPalette(rev(c("red","gray90","green")))(102),
cellwidth=8,cellheight=8,
fontsize=7,
cluster_rows=TRUE,cluster_cols=FALSE,
breaks=c(seq(0,0.9,length.out=51),seq(1.1,6,length.out=51))

)
library(grid)
library(VennDiagram)
mydata1<-read.table("/Users/lishasha/Desktop/home/new_work/biocapital_2700probe/expression_data/value_four_cellline_3000genes/PLOT/MDA231.txt",head=T)
mydata2<-read.table("/Users/lishasha/Desktop/home/new_work/biocapital_2700probe/expression_data/value_four_cellline_3000genes/PLOT/SW620.txt",head=T)
mydata3<-read.table("/Users/lishasha/Desktop/home/new_work/biocapital_2700probe/expression_data/value_four_cellline_3000genes/PLOT/THP1.txt",head=T)
mydata4<-read.table("/Users/lishasha/Desktop/home/new_work/biocapital_2700probe/expression_data/value_four_cellline_3000genes/PLOT/hepg2.txt",head=T)
mydata5<-read.table("/Users/lishasha/Desktop/home/new_work/biocapital_2700probe/expression_data/value_four_cellline_3000genes/PLOT/hp7702.txt",head=T)
venn.diagram(list("MDA231"=mydata1$MDA231,"SW620"=mydata2$SW620,"PMA_THP1"=mydata3$PMA_THP1,"HEPG2"=mydata4$HEPG2,"HP_7702"=mydata5$HP_7702),fill=c("red","blue","yellow","green","pink"),"/Users/lishasha/Desktop/home/new_work/biocapital_2700probe/expression_data/value_four_cellline_3000genes/PLOT/O6_cex_lables.png",main="overlap in 5 cell lines",main.col = "black",main.cex =1.5,cex.labels=0.6, font.labels=4)


library(grid)
library(VennDiagram)
mydata1<-read.table("/Users/lishasha/Desktop/home/new_work/biocapital_2700probe/expression_data/value_four_cellline_3000genes/PLOT/MDA231.txt",head=T)
mydata2<-read.table("/Users/lishasha/Desktop/home/new_work/biocapital_2700probe/expression_data/value_four_cellline_3000genes/PLOT/SW620.txt",head=T)
mydata3<-read.table("/Users/lishasha/Desktop/home/new_work/biocapital_2700probe/expression_data/value_four_cellline_3000genes/PLOT/THP1.txt",head=T)
mydata4<-read.table("/Users/lishasha/Desktop/home/new_work/biocapital_2700probe/expression_data/value_four_cellline_3000genes/PLOT/hepg2.txt",head=T)
mydata5<-read.table("/Users/lishasha/Desktop/home/new_work/biocapital_2700probe/expression_data/value_four_cellline_3000genes/PLOT/hp7702.txt",head=T)
venn.diagram(list("MDA231"=mydata1$MDA231,"SW620"=mydata2$SW620,"PMA_THP1"=mydata3$PMA_THP1,"HEPG2"=mydata4$HEPG2,"HP_7702"=mydata5$HP_7702), height = 3500, width = 3500,resolution = 500,fill=c("red","blue","yellow","green","pink"),"/Users/lishasha/Desktop/home/new_work/biocapital_2700probe/expression_data/value_four_cellline_3000genes/PLOT/O6_height_res.png",main="overlap in 5 cell lines",main.col = "black",main.cex =1.5)

Make a Venn Diagram

Description:

     This function takes a list and creates a publication-quality TIFF
     Venn Diagram

Usage:

     venn.diagram(x, filename, height = 3000, width = 3000, resolution = 500, 
     imagetype = "tiff", units = "px", compression = "lzw", na = "stop", main = "", sub = "", 
     main.pos = c(0.5, 1.05), main.fontface = "plain", main.fontfamily = "serif", 
     main.col = "black", main.cex = 1, main.just = c(0.5, 1), sub.pos = c(0.5, 1.05), 
     sub.fontface = "plain", sub.fontfamily = "serif", sub.col = "black", sub.cex = 1, 
     sub.just = c(0.5, 1), category.names = names(x), force.unique = TRUE, ...);
     



http://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE5230

python /Share/home/wangdong/lss/project/20150430_capitalbio_AR/split_colume3.py /Share/home/wangdong/lss/project/20150430_capitalbio_AR/mapping/captialbio.sam
python /Share/home/wangdong/lss/project/20150430_capitalbio_AR/split_colume3.py /Share/home/wangdong/lss/project/20150430_capitalbio_AR/mapping/AR.sam

bowtie -n 3 /Share/home/wangdong/lss/project/20150430_capitalbio_AR/ref_2920_biocapital /Share/home/wangdong/data/rawdata/150429_C00126_0187_AHFMG2ADXX/Project_HFMG2ADXX/fastq/Sample_SW_captialbio_probe.fastq -S /Share/home/wangdong/lss/project/20150430_capitalbio_AR/mapping/captialbio.sam 
bowtie -n 3 /Share/home/wangdong/lss/project/20150430_capitalbio_AR/ref_89 /Share/home/wangdong/data/rawdata/150429_C00126_0187_AHFMG2ADXX/Project_HFMG2ADXX/fastq/Sample_SW_AR_probe.fastq -S /Share/home/wangdong/lss/project/20150430_capitalbio_AR/mapping/AR.sam

/Share/home/wangdong/local/app/bowtie-1.1.1/bowtie-build -f ref_2920_biocapital.fa ref_2920_biocapital
/Share/home/wangdong/local/app/bowtie-1.1.1/bowtie-build -f ref_89.fa ref_89

Normalizing microarray 3data in R
1) Place the .CEL file(s) in your working directory. By default, the working directory is the R folder; if you want to use any other directory, do so as follows:
a. >setwd(“c:\\My Documents\MyCelFiles”)
2) Open R.
3) > library(affy)
a. This opens the “affy” library in Bioconductor, which enables you to process Affymetrix data.
4) >memory.limit(size=3000)
a. You want to set the memory as high as possible; 3000 Mb (3 Gb) is the most Windows is capable of allocating to R. It is possible that it will not like that, and you will have to do “size=2000”. R will probably respond with the word “NULL”. (The newest build of R seems to have gotten around some of the Windows size problems.)
5) >rawdata <- ReadAffy()
a. You are choosing a name for an object (“rawdata” is sensible, but you could call it anything you like), and defining “rawdata” as the temporary file that holds the output of the “ReadAffy()” command. “ReadAffy()” reads all of the .CEL files in the R folder into “rawdata”; if you only wanted to work with, say, 24 hour replicates, you would say
i. >rawdata <- ReadAffy(“24H_rep1.CEL”, “24H_rep2.CEL”, “24H_rep3.CEL”)
6) >eset <- rma(rawdata)
a. You are creating an object (named “eset” by convention; again, it could be whatever you like it to be), and defining it as the outcome of running “rma” on your previously-named “rawdata”. “rma” does three things: “convolution background correction, quantile normalization, and a summarization based on a multi-array model fit robustly using the median polish algorithm” (BM Bolstad, RA Irizarry, L Gautier & Z Wu (2005) “Preprocessing high-density oligonucleotide arrays”, pp. 13-32 in Bioinformatics and Computational Biology Solutions Using R and Bioconductor, Gentleman, et al., eds. New York, Springer.)
7) >write.exprs(eset, file=”NormalizedDate.txt”)
a. You are exporting your normalized data, stored in “eset”, to a text file with the name of your choosing. I recommend “NormalizedJuly2006.txt”, or some other such thing to help identify precisely what generation it is.
8) Open Excel. Open your new normalized file and go through the steps to turn a text file into a proper Excel file.
9) Insert one cell in the upper left-hand corner of the Excel file, as all of the column headings are shifted one to the left of where they should be. Note the order in which R arranges your probe sets; it’s generally in alphabetical order, but can vary a bit between versions of R/Bioconductor. You may or may not be given a column with the numbers 1-x, x being the total number of probe sets on your GeneChip. If you do not have such a column, make one; your first probe (as ordered in the normalized file) is number 1 (and corresponds with the 2nd row on
your Excel file); your xth probe is number x (and corresponds with row x+1). This is important, because this is how R thinks of your probe sets; probe names (fg00505_at) mean nothing to R, and cannot be used in doing more sophisticated analyses.
10) Quit R: >q() 

将microarray数据.cel使用起来的pipeline
> source("http://www.bioconductor.org/biocLite.R")
> biocLite("affy")
> biocLite("affycoretools")
> library(affy)
> library(affycoretools)
> mydata <- ReadAffy("sw620_GSM1312999_CTS_69_SW620_0h.CEL")
> mydata
trying URL 'http://bioconductor.org/packages/3.0/data/annotation/src/contrib/hugene10stv1cdf_2.15.0.tar.gz'
Content type 'application/x-gzip' length 3010436 bytes (2.9 Mb)
opened URL
==================================================
downloaded 2.9 Mb

* installing *source* package 'hugene10stv1cdf' ...
** R
** data
** preparing package for lazy loading
** help
*** installing help indices
** building package indices
** testing if installed package can be loaded
* DONE (hugene10stv1cdf)

The downloaded source packages are in
        '/tmp/RtmppE94Ng/downloaded_packages'
Updating HTML index of packages in '.Library'
Making 'packages.html' ... donemakeTagDirectorymakeTagDirectory


AffyBatch object
size of arrays=1050x1050 features (17 kb)
cdf=HuGene-1_0-st-v1 (32321 affyids)
number of samples=1
number of genes=32321
annotation=hugene10stv1
notes=
> pdf("620hist_2.pdf")
> hist(mydata)
> pdf("620boxplot.pdf")
> boxplot(mydata)
> pdf("620image.pdf")
> image(mydata[,1])
#In affy package there are several methodologies available to correct background, normalize and summarize expression values per each probe set of the dataset.
#Here we will use the RMA and MAS5 methods that are implemented into ready made functions.
#In order to apply the RMA method, we can type:
>eset <- rma(mydata) #(normalization)
Background correcting
Normalizing
Calculating Expression 
> eset
ExpressionSet (storageMode: lockedEnvironment)
assayData: 32321 features, 1 samples 
  element names: exprs
protocolData
  sampleNames: sw620_GSM1312999_CTS_69_SW620_0h.CEL
  varLabels: ScanDate
  varMetadata: labelDescription
phenoData
  sampleNames: sw620_GSM1312999_CTS_69_SW620_0h.CEL
  varLabels: sample
  varMetadata: labelDescription
featureData: none
experimentData: use 'experimentData(object)'
Annotation: hugene10stv1
> eset1 <- mas5(mydata)
background correction: mas
PM/MM correction : mas
expression values: mas
background correcting...done.
32321 ids to be processed
|                    |
|####################|
> eset1
ExpressionSet (storageMode: lockedEnvironment)
assayData: 32321 features, 1 samples
  element names: exprs, se.exprs
protocolData
  sampleNames: sw620_GSM1312999_CTS_69_SW620_0h.CEL
  varLabels: ScanDate
  varMetadata: labelDescription
phenoData
  sampleNames: sw620_GSM1312999_CTS_69_SW620_0h.CEL
  varLabels: sample
  varMetadata: labelDescription
featureData: none
experimentData: use 'experimentData(object)'
Annotation: hugene10stv1
>pdf("exploratory_boxplot.pdf")
>boxplot(exprs(eset))
>dev.off()
>source("http://www.bioconductor.org/biocLite.R")
>biocLite("hgu133a.db")
>biocLite("annotate")
> biocLite("R2HTML")
> library(hgu133a.db)
> library(annotate)
Loading required package: XML

Attaching package: 'annotate'

The following object is masked from 'package:GenomeInfoDb':

    organism
> library(R2HTML)
We are now going to extract the feature names from eset2 that contains the selected genes of interest:
> ID <- featureNames(eset2)
And look up the Gene Symbol, Name, and Ensembl Gene ID for each of those IDs:
> Symbol <- getSYMBOL(ID, "hgu133a.db")
> Name <- as.character(lookUp(ID, "hgu133a.db", "GENENAME"))
> Ensembl <- as.character(lookUp(ID, "hgu133a.db", "ENSEMBL"))
For each Ensembl ID (if we have it), we will now create a hyperlink that goes to the Ensembl genome browser:
> Ensembl <- ifelse(Ensembl=="NA", NA,
paste("<a href='http://useast.ensembl.org/Homo_sapiens/Gene/Summary?g=",
Ensembl, "'>", Ensembl, "</a>", sep=""))
And make a temporary data frame with all those identifiers:
> tmp <- data.frame(ID=ID, Symbol=Symbol, Name=Name, Ensembl=Ensembl, stringsAsFactors=F) > tmp[tmp=="NA"] <- NA # The stringsAsFactors makes "NA" characters. This fixes that problem.
We are now going to write out an HTML file with clickable links to the Ensembl Genome Browser, and .txt file with gene list:
> HTML(tmp, "out.html", append=F)
> write.table(tmp ,file="target.txt",row.names=F, sep="\t")


 


boxplot等将坐标标签斜起来
correlation中间text用这两个参数即可
cex.labels=0.8, font.labels=4,可以直接用
 #panel.hist <- function(x, ...)
 36 #{
 37 #usr <- par("usr"); on.exit(par(usr))
 38 #par(usr = c(usr[1:2], 0, 4) )
 39 #h <- hist(x, plot = FALSE)
 40 #breaks <- h$breaks; nB <- length(breaks)
 41 #y <- h$counts; y <- y/max(y)
 42 #rect(breaks[-nB], 0, breaks[-1], y, col="cyan", ...)
 43 #}
 
 panel.boxplot        More univariate panel plots可以直接加入在pairs()里面来画 boxplot 
panel.boxplot        More univariate panel plots
panel.cor        More panel plots
panel.density        More univariate panel plots
panel.ellipse        More panel plots
panel.hist        More univariate panel plots
panel.qqnorm        More univariate panel plots
下面这个图是拿来作直方图的
    panel.hist <- function(x, ...)
    {
        usr <- par("usr"); on.exit(par(usr))
        par(usr = c(usr[1:2], 0, 1.5) )
        h <- hist(x, plot = FALSE)
        breaks <- h$breaks; nB <- length(breaks)
        y <- h$counts; y <- y/max(y)
        rect(breaks[-nB], 0, breaks[-1], y, col="cyan", ...)
    }
    pairs(USJudgeRatings[1:5], panel=panel.smooth,
          cex = 1.5, pch = 24, bg="light blue",
          diag.panel=panel.hist, cex.labels = 2, font.labels=2)
          
还是得这样才能改掉想线图的颜色
airs(iris[,1:4], upper.panel = panel.cor, pch = 21, bg = c("lightblue"))

 
text(1:6, par("usr")[3]-0.25, srt =30, adj = 1,labels = labels, xpd = TRUE)
王会丽cor invivo in vitro normalization(10000000*hits/total of reads)

fastqc -o /Share/home/wangdong/lss/project/20150402rasl_sw_again_why_hepg2/fastqc/ R3_2015_03_25.fastq R3-mops_AGTGCCC.fastq mops_R1.fastq mops_C3.fastq rasl_cell.fastq rasl_ha.fastq rasl_hg2_15.fastq rasl_nc_negative.fastq rasl_nc_positive.fastq rasl_rna.fastq rasl_shrna2.fastq rasl_shrna3.fastq

> mydata  <-read.table("cor_whl.txt",head=T)
> cor(mydata)
              C1         C2         C3          C4         C5       EXP1
C1   1.000000000 0.03754071 0.01653239 0.038140525 0.03209038 0.03957553
C2   0.037540715 1.00000000 0.03471711 0.012206128 0.02818689 0.05267082
C3   0.016532385 0.03471711 1.00000000 0.009407780 0.01378644 0.06162066
C4   0.038140525 0.01220613 0.00940778 1.000000000 0.35795586 0.01885744
C5   0.032090377 0.02818689 0.01378644 0.357955856 1.00000000 0.03392869
EXP1 0.039575535 0.05267082 0.06162066 0.018857443 0.03392869 1.00000000
EXP2 0.021015294 0.03108098 0.04494751 0.018359859 0.02638896 0.04890143
EXP3 0.098674955 0.13395507 0.03435392 0.015572696 0.02308882 0.06746699
EXP4 0.050923726 0.05901322 0.03716548 0.018256494 0.01982727 0.06727874
EXP5 0.006787069 0.01166773 0.00752868 0.003704685 0.01292783 0.08213378
BI   0.013649467 0.02008112 0.01501878 0.009698009 0.00888524 0.03777409
           EXP2        EXP3       EXP4        EXP5          BI
C1   0.02101529 0.098674955 0.05092373 0.006787069 0.013649467
C2   0.03108098 0.133955066 0.05901322 0.011667734 0.020081123
C3   0.04494751 0.034353924 0.03716548 0.007528680 0.015018776
C4   0.01835986 0.015572696 0.01825649 0.003704685 0.009698009
C5   0.02638896 0.023088824 0.01982727 0.012927833 0.008885240
EXP1 0.04890143 0.067466987 0.06727874 0.082133776 0.037774089
EXP2 1.00000000 0.149879010 0.04363418 0.038647002 0.019827310
EXP3 0.14987901 1.000000000 0.06573363 0.009921881 0.016585631
EXP4 0.04363418 0.065733633 1.00000000 0.051256056 0.026198685
EXP5 0.03864700 0.009921881 0.05125606 1.000000000 0.010872146
BI   0.01982731 0.016585631 0.02619869 0.010872146 1.000000000


Ultraedit mac版本破解方法
下载软件地址
http://www.bubuko.com/infodetail-581168.html
1. 首先下载安装文件：下载，提取码：b003

2. 网盘中两个文件下载完以后，安装.dmg文件，安装完后先别打开软件，用PATCH那个压缩包解压出来的文件替换掉 /Applications/UltraEdit.app/Contents/MacOS下同名文件 即可


 a=read.csv("above10.csv",head=T,sep=";")
> b=read.csv("below10.csv",head=T,sep=";")
> pdf("boxplot_probe_above10.pdf")
> boxplot(a$array,b$array,col=c("pink"),ylab="The micrparray indensity of genes",las=1,font.lab=2)
>stripchart()#增加上面的点，另外读入数据
stripchart(B,vertical=TRUE,method="jitter",pch=20,cex=0.01,col=c("grey"),bg="bisque",add=TRUE)

 dev.off()
null device
          1
> a=read.csv("above10_log.csv",head=T,sep=";")
> b=read.csv("below10_log.csv",head=T,sep=";")
> pdf("boxplot_probe_above10_log10.pdf")
> boxplot(a$array,b$array,col=c("pink"),ylab="The micrparray indensity of genes",las=1,font.lab=2)
> dev.off()
null device
          1
> pdf("boxplot_probe_above10_log10_below.pdf")
> boxplot(b$array,col=c("pink"),ylab="The micrparray indensity of genes",las=1,font.lab=2)
> dev.off()
null device
          1
> pdf("boxplot_probe_above10_log10_above.pdf")
> boxplot(a$array,col=c("pink"),ylab="The micrparray indensity of genes",las=1,font.lab=2)
Error in boxplot.default(a$array, col = c("pink"), ylab = "The micrparray indensity of genes",  :
  adding class "factor" to an invalid object
> q()
a=read.csv("25minus.csv",head=T,sep=";")
b=read.csv("25_10.csv",head=T,sep=";")
c=read.csv("below10minus.csv",head=T,sep=";")
 #抬头开始不能有数字
finaldata<-data.frame(a$above_25_a,a$above_25_d,b$from_10_25_a,b$from_10t_25_d,c$below_10_a,c$below_10_d)
 pdf("boxplot_probe_minus3.pdf")
 boxplot(a$Tm_acceptor_minus_Tm_donor,b$Tm_acceptor_minus_Tm_donor,c$Tm_acceptor_minus_Tm_donor,col=c("pink"),ylab="The Dvalue of Acceptor Tm and donor Tm",las=1,font.lab=2)
 boxplot(b$from_10_25_a,b$from_10t_25_d,col=c("pink"),ylab="normalized Indensity Values",las=1,font.lab=2)
 pdf("boxplot_probe_below10.pdf")
  boxplot(c$below_10_a,c$below_10_d,col=c("yellow"),ylab="normalized Indensity Values",las=1,font.lab=2)
above_25_a	above_25_d	10to25_a	10to25_d	below_10_a	below_10_d

#制作comvert.sh，qsub提交即可
configureBclToFastq.pl --input-dir /Share/home/wangdong/data/rawdata/150401_C00126_0182_AHGT55ADXX/Data/Intensities/BaseCalls --output ./Unaligned --sample-sheet /Share/home/wangdong/data/rawdata/150401_C00126_0182_AHGT55ADXX/SampleSheet_undetermined.csv --no-eamss
cd /Share/home/wangdong/data/rawdata/150401_C00126_0182_AHGT55ADXX/Unaligned
nohup make -j 8


corRaw <- cor(Raw)
library(spatstat) # "im" function
plot(im(corRaw[nrow(corRaw):1,]), main="Correlation Matrix Map")
#Here's the R code that creates a distance matrix from 1-Correlation as the dissimilarity index:
#Several methods of "dissimilarity" were explored to see if one method is "better" in some way over the other methods:

    Dissimilarity = 1 - Correlation
    Dissimilarity = (1 - Correlation)/2
    Dissimilarity = 1 - Abs(Correlation)
    Dissimilarity = Sqrt(1 - Correlation2)

dissimilarity <- 1 - cor(Raw)
distance <- as.dist(dissimilarity)
#Note the as.dist function is used here to assign the correlation values to be "distances".  (In some cases, you may want to use the dist function to compute distances using a variety of distance metrics instead.) 

#The hierarchical clustering function, hclust, expects a dissimilarity matrix.  The plot function knows how to plot a dendrogram from hclust's result

    plot(hclust(distance),
         main="Dissimilarity = 1 - Correlation", xlab="")


转换成dendrogram可以有更多选项
    > hc = hclust(dist(mtcars))
    > hcd = as.dendrogram(hc)
    > plot(hcd)
    > plot(cut(hcd, h=75)$lower[ [2] ])
    
    
    
    
corrgram

mydata  <-read.table("test.txt",head=T)
cor(mydata)
library(corrgram)
> mydata
     a    b    c    d     e
a 1.00 0.85 0.85 0.70 0.681
b 0.85 1.00 0.90 0.83 0.700
c 0.85 0.90 1.00 0.79 0.710
d 0.78 0.83 0.79 1.00 0.449
e 0.68 0.70 0.71 0.45 1.000

> corrgram(mydata, order=TRUE, lower.panel=panel.shade,upper.panel=NULL)
> dev.off()
null device
          1
> pdf("test.pdf")
> corrgram(mydata, order=TRUE, lower.panel=panel.shade,up
update                update.formula        update.packageStatus  upper.tri
update.default        update.packages       upgrade               upper.panel=
> corrgram(mydata, order=TRUE, lower.panel=panel.shade,upper.panel=p
Display all 184 possibilities? (y or n)
> corrgram(mydata, order=TRUE, lower.panel=panel.shade,upper.panel=panel.pie)
> dev.off()
null device
          1
> pdf("test2.pdf")
> corrgram(mydata, order=TRUE, lower.panel=panel.shade,upper.panel=panel.shade)
> dev.off()
null device
          1
> pdf("test3.pdf")
> corrgram(mydata, order=TRUE, lower.panel=panel.shade,upper.panel=panel.pie,text.panel=NULL)
> dev.off()
null device

# Changing Colors in a Correlogram
library(corrgram)
col.corrgram <- function(ncol){   
  colorRampPalette(c("darkgoldenrod4", "burlywood1",
  "darkkhaki", "darkgreen"))(ncol)}
corrgram(mtcars, order=TRUE, lower.panel=panel.shade,
   upper.panel=panel.pie, text.panel=panel.txt,
   main="Correlogram of Car Mileage Data (PC2/PC1 Order)")
   

 #R#barplot
分类： R 2013-01-30 09:30 490人阅读 评论(0) 收藏 举报
 （1）建一个测试的DATASET：
> a<-seq(2,10,2)
> b<-seq(10,100,20)
> c<-as.matrix(a,b)
> c
     [,1]
[1,]    2
[2,]    4
[3,]    6
[4,]    8
[5,]   10
> c<-data.frame(a,b)
> c<-as.matrix(c)
> c
      a  b
[1,]  2 10
[2,]  4 30
[3,]  6 50
[4,]  8 70
[5,] 10 90

（2）根据测试DATASET，来画图
>barplot(c,main="Test",xlab="Product",ylab="xiaohao",col=c("red","blue","yellow","black","green"),legend=rownames(c))



韦恩图自己成功运行
install.packages("VennDiagram")
library(VennDiagram)
library(grid)
mydata1<-read.table("/Users/lishasha/Desktop/exp1_2oligos.txt",head=T)
mydata2<-read.table("/Users/lishasha/Desktop/exp2_2oligos.txt",head=T)
mydata3<-read.table("/Users/lishasha/Desktop/exp3_2oligos.txt",head=T)
mydata4<-read.table("/Users/lishasha/Desktop/exp4_2oligos.txt",head=T)
mydata5<-read.table("/Users/lishasha/Desktop/exp5_2oligos.txt",head=T)
venn.diagram(list("exp1"=mydata1$exp1,"exp2"=mydata2$exp2,"exp3"=mydata3$exp3,"exp4"=mydata4$exp4,"exp5"=mydata5$exp5),fill=c("red","blue","yellow","green","orange"),"/Users/lishasha/Desktop/OVERLAP3.png",main="overlap gene in 5 experiment group",main.col = "black", main.cex =1)


临客圈传脚本


library(VennDiagram)
A<-c("a","b")
B<-c("b","c")
C<-c("c","d")
D<-c("d","e")
E<-c("e","a")
venn.diagram(list("lncapablonly"=A,"lncaponly"=B,"C"=C,"D"=D),fill=c("red","blue","yellow","green"),"G:/test.png",main="H3K27Ac Peak Numbers",main.col = "black", main.cex =3)




网上下载一些其他相关韦恩图
R中提供了多个可用于绘制韦恩图的软件包，本文主要是介绍的是VennDiagram包。
安装VennDiagram包：
install.packages("VennDiagram")
首先加载相应的软件包：
library(VennDiagram)
生成几个集合并计算各个集合及其相互交集的大小：
A = 1:150
B = c(121:170,300:320)
C = c(20:40,141:200)
Length_A<-length(A)
Length_B<-length(B)
Length_C<-length(C)
Length_AB<-length(intersect(A,B))
Length_BC<-length(intersect(B,C))
Length_AC<-length(intersect(A,C))
Length_ABC<-length(intersect(intersect(A,B),C))
利用通用函数venn.diagram绘制两个集合的韦恩图：
T<-venn.diagram(list(A=A,B=B),filename=NULL
,lwd=1,lty=2
,col=c("red","green"),fill=c("red","green")
,cat.col=c("red","green")
,rotation.degree=90)
grid.draw(T)
其中，参数filename指定用于保存图形文件的文件名，如果希望在当前的图形窗口中看到绘制的韦恩图，则filename必须为空；若希望将绘制的图 形直接保存为某文件，则直接使用venn.diagram(...,filename="*")即可完成。参数fill表示各个集合对应的圆的填充颜 色,col表示对应的圆周的颜色，而cat.col则表示集合名称的显示颜色。lwd用于设定圆弧的宽度，lty用于设定圆弧的线型。参数 rotation.degree则可用于调整图形的旋转角度。
利用函数venn.diagram绘制三个集合的韦恩图：
T<-venn.diagram(list(A=A,B=B,C=C),filename=NULL
,lwd=1,lty=2,col=c("red","green","blue")
,fill=c("red","green","blue")
,cat.col=c("red","green","blue")
,reverse=TRUE)
grid.draw(T)
从上面的两个例子可以看出函数venn.diagram是利用集合作为参数绘制韦恩图的，但是有时候我们并不知道各个集合都包含什么元素，而只知道集合及 相互之间交集的大小，这个时候如何绘制韦恩图呢？包VennDiagram还给我们提供了另外几个函数：绘制两个集合的韦恩图的 draw.pairwise.venn，三个集合的draw.triple.venn，四个、五个集合的draw.quad.venn、 draw.quintuple.venn。我们此处只介绍前两个函数的用法。
利用函数draw.pairwise.venn绘制两个集合的韦恩图：
draw.pairwise.venn(area1=Length_A,area2=Length_B,cross.area=Length_AB
,category=c("A","B"),lwd=rep(1,1),lty=rep(2,2)
,col=c("red","green"),fill=c("red","green")
,cat.col=c("red","green")
,rotation.degree=90)
其中area1指第一个集合的大小，area2指第二个集合的大小，而cross.area则指交集的大小。参数category用于指定集合名称。其余参数与venn.diagram相同。
利用函数draw.triple.venn绘制三个集合的韦恩图：
draw.triple.venn(area1=Length_A, area2=Length_B, area3=Length_C
,n12=Length_AB, n23=Length_BC, n13=Length_AC, n123=Length_ABC
,category = c("A","B","C")
,col=c("red","green","blue"),fill=c("red","green","blue")
,cat.col=c("red","green","blue")
,reverse = FALSE)
同draw.pairwise.venn类似，area1、area2、area3分别指第一个、第二个、第三个集合的大小。n12表示第一个与第二个集合的交集大小，n23、n13也是类似，n123指三个集合的交集大小。reverse则指是否对图形进行反转。


重新生成将rpkm根据不同转录本合成一个基因
analyzeRepeats.pl rna hg19 -strand both -condenseGenes -count exons -d /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/c2_2/ /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/c3_2/ /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/nc_2/ -rpkm > condensed_rpkm.txt
[wangdong@cluster ~/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/mktags]$analyzeRepeats.pl rna hg19 -strand both -condenseGenes -count exons -d /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/c2_2/ /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/c3_2/ /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/nc_2/ -rpkm > condensed_rpkm.txt
        Tag Directories:
                /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/c2_2/
                /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/c3_2/
                /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/nc_2/
        Input file format: homerRmsk
        Filtering based on repeat parameters: kept 51794 of 51794
        Calculating read coverage for /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/c2_2/
        Calculating read coverage for /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/c3_2/
        Calculating read coverage for /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/nc_2/
                Normalizing...
                Normalizing...
                Normalizing...
        Printing output
                Printed 25690 of 48118 repeats (expression >= -1e+20)
                
google 小丽给网址


http://173.194.121.48/

1#trim_reads to 22bp of antisense

fastx_trimmer -f 25 -l 46 -Q 33 -i before_injection.fastq -o bi_antisense.fastq
fastx_trimmer -f 25 -l 46 -Q 33 -i c1.fastq -o c1_antisense.fastq
fastx_trimmer -f 25 -l 46 -Q 33 -i c2.fastq -o c2_antisense.fastq
fastx_trimmer -f 25 -l 46 -Q 33 -i c3.fastq -o c3_antisense.fastq
fastx_trimmer -f 25 -l 46 -Q 33 -i c4.fastq -o c4_antisense.fastq
fastx_trimmer -f 25 -l 46 -Q 33 -i c5.fastq -o c5_antisense.fastq
fastx_trimmer -f 25 -l 46 -Q 33 -i d0.fastq -o d0_antisense.fastq
fastx_trimmer -f 25 -l 46 -Q 33 -i d10_negative.fastq -o d10plus_antisense.fastq
fastx_trimmer -f 25 -l 46 -Q 33 -i d10_positive.fastq -o d10minor_antisense.fastq
fastx_trimmer -f 25 -l 46 -Q 33 -i d2_negative.fastq -o d2plus_antisense.fastq
fastx_trimmer -f 25 -l 46 -Q 33 -i d2_positive.fastq -o d2minor_antisense.fastq
fastx_trimmer -f 25 -l 46 -Q 33 -i Exp1.fastq -o Exp1_antisense.fastq
fastx_trimmer -f 25 -l 46 -Q 33 -i Exp2.fastq -o Exp2_antisense.fastq
fastx_trimmer -f 25 -l 46 -Q 33 -i Exp3.fastq -o Exp3_antisense.fastq
fastx_trimmer -f 25 -l 46 -Q 33 -i Exp4.fastq -o Exp4_antisense.fastq
fastx_trimmer -f 25 -l 46 -Q 33 -i Exp5.fastq -o Exp5_antisense.fastq

2#prepare ref sequence big pool
bowtie2-build -f ref_22_antisense.fasta ref_22_antisense
ref_51.fasta
bowtie2-build -f ref_51.fasta ref_51_antisense
3#mapping(bowtie2),count(python)
bowtie2 -x /Share/home/wangdong/lss/project/whl_invivo_screen_big_pool/ref_51/ref_51_antisense /Share/home/wangdong/lss/project/whl_invivo_screen_big_pool/bi_antisense.fastq -S /Share/home/wangdong/lss/project/whl_invivo_screen_big_pool/ref_51/bi_51.sam
f

bowtie2-build -f ref.fasta ref_whl
bowtie2 -N 3 -x ref_whl WHL_C1_R1_antisense.fastq -S C1_3.sam
fastx_trimmer -f 25 -l 46 -Q 33 -i WHL_C1_R1.fastq -o WHL_C1_R1_antisense.fastq
bedtools安装




pheatmap用于作heatmap用
~/lkq/projects/cluster_for_whl/pheatmap

boxplot

a=read.csv("probe_plot.csv",head=T,sep=";")
finaldata<-data.frame(a$X1N,a$X2N,a$X3N,a$X4N,a$X5N,a$X8N,a$X9N,a$X10N,a$X11N,a$X12N,a$X13N,a$X15N,a$X16N,a$X17N,a$X1T,a$X2T,a$X3T,a$X4T,a$X5T,a$X8T,a$X9T,a$X10T,a$X11T,a$X12T,a$X13T,a$X15T,a$X16T,a$X17T)
 pdf("boxplot_mRNA_28.pdf")
 boxplot(finaldata,col=c("mediumturquoise"),ylab="normalized Indensity Values",las=1,font.lab=2)
 


maping 用的～／lkq/script/produce.pls
bin/bash\n
/Share/home/wangdong/local/bin/tophat -p 8 -G /Share/home/wangdong/data/human_annotation/gencode19.lite.gtf -o $out/$sample --library-type=fr-unstranded /Share/home/lulab1/users/liyang/project/Genome/human_hg19/bowtie2_index/human_genome_hg19 $_

bamToBed -i accepted_hits.bam >> c2.bed
bamToBed -i /Share/home/wangdong/lss/sw_ctnnb_RNASeq_reanalysis/mapping/SWC3_1_t/accepted_hits.bam >> /Share/home/wangdong/lss/sw_ctnnb_RNASeq_reanalysis/mapping/SWC3_1_t/c3.bed
bamToBed -i /Share/home/wangdong/lss/sw_ctnnb_RNASeq_reanalysis/mapping/SWNC_1_t/accepted_hits.bam >> /Share/home/wangdong/lss/sw_ctnnb_RNASeq_reanalysis/mapping/SWNC_1_t/NC.bed


makeTagDirectory /Share/home/wangdong/lss/sw_ctnnb_RNASeq_reanalysis/maketags/c2_2/ /Share/home/wangdong/lss/sw_ctnnb_RNASeq_reanalysis/mapping/SWC2_1_t/c2.bed -genome /Share/home/lulab1/users/liyang/project/Genome/human_hg19/bowtie2_index/human_genome_hg19.fa -sspe
makeTagDirectory /Share/home/wangdong/lss/sw_ctnnb_RNASeq_reanalysis/maketags/c3_2/ /Share/home/wangdong/lss/sw_ctnnb_RNASeq_reanalysis/mapping/SWC3_1_t/c3.bed -genome /Share/home/lulab1/users/liyang/project/Genome/human_hg19/bowtie2_index/human_genome_hg19.fa -sspe
makeTagDirectory /Share/home/wangdong/lss/sw_ctnnb_RNASeq_reanalysis/maketags/nc_2/ /Share/home/wangdong/lss/sw_ctnnb_RNASeq_reanalysis/mapping/SWNC_1_t/NC.bed -genome /Share/home/lulab1/users/liyang/project/Genome/human_hg19/bowtie2_index/human_genome_hg19.fa -sspe

removeOutOfBoundsReads.pl ~/lss/sw_ctnnb_RNASeq_reanalysis/maketags/c2/ /Share/home/lulab1/users/liyang/project/Genome/human_hg19/bowtie2_index/human_genome_hg19.fa -chromSizes /Share/home/wangdong/data/human/hg19.chrom.sizes
removeOutOfBoundsReads.pl ~/lss/sw_ctnnb_RNASeq_reanalysis/maketags/c3/ /Share/home/lulab1/users/liyang/project/Genome/human_hg19/bowtie2_index/human_genome_hg19.fa -chromSizes /Share/home/wangdong/data/human/hg19.chrom.sizes
removeOutOfBoundsReads.pl ~/lss/sw_ctnnb_RNASeq_reanalysis/maketags/nc/ /Share/home/lulab1/users/liyang/project/Genome/human_hg19/bowtie2_index/human_genome_hg19.fa -chromSizes /Share/home/wangdong/data/human/hg19.chrom.sizes


makeUCSCfile /Work1/home/wangdong/lss/data/GQ_CHIP/maketags_H3K4ME1/ -o auto -bigWig /Work1/home/wangdong/ensemble/hg19.chrom.sizes -norm 1e7 -strand both -fsize 1e20 > /Work1/home/wangdong/lss/data/GQ_CHIP/maketags_H3K4ME1/tags.trackInfo2.txt
makeUCSCfile /Work1/home/wangdong/lss/data/GQ_CHIP/maketags_h3k4me2/ -o auto -bigWig /Work1/home/wangdong/ensemble/hg19.chrom.sizes -norm 1e7 -strand both -fsize 1e20 > /Work1/home/wangdong/lss/data/GQ_CHIP/maketags_h3k4me2/tags.trackInfo2.txt
makeUCSCfile /Work1/home/wangdong/lss/data/GQ_CHIP/maketags_H3K4ME1/ -o auto -bigWig /Work1/home/wangdong/ensemble/hg19.chrom.sizes -norm 1e7 -strand both -fsize 1e20 > /Work1/home/wangdong/lss/data/GQ_CHIP/maketags_H3K4ME1/tags.trackInfo2.txt

analyzeRepeats.pl rna /Share/home/lulab1/users/liyang/project/Genome/human_hg19/bowtie2_index/human_genome_hg19.fa -strand both -condenseGenes -d ./c2 ./c3 ./nc -rpkm > ./shRNA1_shRNA2_nc_rpkm.txt

报错
makeUCSCfile /Share/home/wangdong/lss/sw_ctnnb_RNASeq_reanalysis/maketags/c2_2/ -fragLength given -o auto
makeUCSCfile /Share/home/wangdong/lss/sw_ctnnb_RNASeq_reanalysis/maketags/c3_2/ -fragLength given -o auto
makeUCSCfile /Share/home/wangdong/lss/sw_ctnnb_RNASeq_reanalysis/maketags/nc_2/ -fragLength given -o auto     
      
analyzeRepeats.pl rna hg19 -strand both -count exons -d /Share/home/wangdong/lss/sw_ctnnb_RNASeq_reanalysis/maketags/c2_2/ /Share/home/wangdong/lss/sw_ctnnb_RNASeq_reanalysis/maketags/c3_2/ /Share/home/wangdong/lss/sw_ctnnb_RNASeq_reanalysis/maketags/nc_2/ -rpkm > rpkm.txt

analyzeRepeats.pl rna hg19 -strand both -count exons -d /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/c2_2/ /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/c3_2/ /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/nc_2/ -rpkm > rpkm.txt
新文件夹中再运行
[wangdong@cluster ~/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/mktags]$analyzeRepeats.pl rna hg19 -strand both -count exons -d /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/c2_2/ /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/c3_2/ /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/nc_2/ -rpkm > rpkm.txt
        Tag Directories:
                /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/c2_2/
                /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/c3_2/
                /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/nc_2/
        Input file format: homerRmsk
        Filtering based on repeat parameters: kept 51794 of 51794
        Calculating read coverage for /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/c2_2/
        Calculating read coverage for /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/c3_2/
        Calculating read coverage for /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/nc_2/
                Normalizing...
                Normalizing...
                Normalizing...
        Printing output
                Printed 48118 of 48118 repeats (expression >= -1e+20)

analyzeRepeats.pl rna hg19 -strand both -count exons -d /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/c2_2/ /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/nc_2/ -rpkm > shRNA1_NC_rpkm.txt
[wangdong@cluster ~/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/mktags]$analyzeRepeats.pl rna hg19 -strand both -count exons -d /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/c2_2/ /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/nc_2/ -rpkm > shRNA1_NC_rpkm.txt
        Tag Directories:
                /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/c2_2/
                /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/nc_2/
        Input file format: homerRmsk
        Filtering based on repeat parameters: kept 51794 of 51794
        Calculating read coverage for /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/c2_2/
        Calculating read coverage for /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/nc_2/
                Normalizing...
                Normalizing...
        Printing output
                Printed 48118 of 48118 repeats (expression >= -1e+20)
分开做结果和三个合在一起一样
analyzeRepeats.pl rna hg19 -strand both -count exons -d /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/c3_2/ /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/nc_2/ -rpkm > shRNA2_NC_rpkm.txt
[wangdong@cluster ~/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/mktags]$analyzeRepeats.pl rna hg19 -strand both -count exons -d /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/c3_2/ /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/nc_2/ -rpkm > shRNA2_NC_rpkm.txt
        Tag Directories:
                /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/c3_2/
                /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/nc_2/
        Input file format: homerRmsk
        Filtering based on repeat parameters: kept 51794 of 51794
        Calculating read coverage for /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/c3_2/
        Calculating read coverage for /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/nc_2/
                Normalizing...
                Normalizing...
        Printing output
                Printed 48118 of 48118 repeats (expression >= -1e+20)

 Tag Directories:
                /Share/home/wangdong/lss/sw_ctnnb_RNASeq_reanalysis/maketags/c2_2/
                /Share/home/wangdong/lss/sw_ctnnb_RNASeq_reanalysis/maketags/c3_2/
                /Share/home/wangdong/lss/sw_ctnnb_RNASeq_reanalysis/maketags/nc_2/
        Input file format: homerRmsk
        Filtering based on repeat parameters: kept 51794 of 51794
        Calculating read coverage for /Share/home/wangdong/lss/sw_ctnnb_RNASeq_reanalysis/maketags/c2_2/
        Calculating read coverage for /Share/home/wangdong/lss/sw_ctnnb_RNASeq_reanalysis/maketags/c3_2/
        Calculating read coverage for /Share/home/wangdong/lss/sw_ctnnb_RNASeq_reanalysis/maketags/nc_2/
                Normalizing...
                Normalizing...
                Normalizing...
        Printing output
                Printed 48118 of 48118 repeats (expression >= -1e+20)
                
                
analyzeRepeats.pl rna hg19 -strand both -count exons -d Exp1r1/ Exp1r2 Exp2r1/ Exp2r2/ -rpkm > rpkm.txt
analyzeRepeats.pl rna hg19 -strand both -count exons -d Exp1r1/ Exp1r2 Exp2r1/ Exp2r2/ -rpkm > rpkm.txt      

4. Quantify gene expression as integer counts for differential expression (-noadj)
# May also wish to use "-condenseGenes" if you don't want multiple isoforms per gene
analyzeRepeats.pl rna hg19 -strand both -count exons -condenseGenes -d /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/c2_2/ /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/c3_2/ /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/nc_2/ -noadj > shRNA1_shRNA2_NC_raw.txt
[wangdong@cluster ~/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/mktags]$analyzeRepeats.pl rna hg19 -strand both -count exons -condenseGenes -d /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/c2_2/ /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/c3_2/ /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/nc_2/ -noadj > shRNA1_shRNA2_NC_raw.txt
        Tag Directories:
                /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/c2_2/
                /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/c3_2/
                /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/nc_2/
        Input file format: homerRmsk
        Filtering based on repeat parameters: kept 51794 of 51794
        Calculating read coverage for /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/c2_2/
        Calculating read coverage for /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/c3_2/
        Calculating read coverage for /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/nc_2/
        Printing output
                Printed 25690 of 48118 repeats (expression >= -1e+20)  #由于加了-condenseGenes，所以只输出25690
                
analyzeRepeats.pl rna hg19 -strand both -count exons -condenseGenes -d /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/c2_2/ /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/nc_2/ -noadj > shRNA1_NC_raw.txt


Calculate differential expression (requires R/Bioconductor and the package edgeR to be installed!!)
# Name the samples in the same order as they were used in the analyzeRepeats.pl command.
# Replicates should have the same name (i.e. cond1)
getDiffExpression.pl raw.txt cond1 cond1 cond2 cond2 > diffExp.output.txt

getDiffExpression.pl shRNA1_shRNA2_NC_raw.txt c2_2 c3_2 nc_2 > shRNA1_shRNA2_NC_diffExp.txt
getDiffExpression.pl shRNA1_NC_raw.txt c2_2 nc_2 > shRNA1_NC_diffExp.txt

getDiffExpression.pl shRNA2_NC_raw.txt c3_2 nc_2 > shRNA2_NC_diffExp.txt

getDiffExpression.pl shRNA1_shRNA2_NC_raw.txt shRNA shRNA NC -repeats > shRNA1_shRNA2_NC_diffOutput.txt



# Check out the results in Excel - there will be 4 columns per comparison: log2 fold, log2 intensity cpm, p-value, and FDR/q-value.


下面是临客圈脚本里关于这几步，主要是用里面的参考序列和注释文件
maketags
Share/home/wangdong/packages/homer/bin/makeTagDirectory $out/$sample $_ -unique -genome /Share/home/lulab1/users/liyang/project/Genome/human_hg19/bowtie2_index/human_genome_hg19.fa -checkGC
makeucscfiles
/Share/home/wangdong/packages/homer/bin/removeOutOfBoundsReads.pl $_ /Share/home/wangdong/data/human/human_genome_hg19.fa -chromSizes /Share/home/wangdong/data/human/hg19.chrom.sizes &&\n/Share/home/wangdong/packages/homer/bin/makeUCSCfile $_ -o auto -bigWig /Share/home/wangdong/data/human/hg19.chrom.sizes -strand both -fsize 1e20 >$out/$sample/$sample.trackInfo.txt

孟凡琳的homer rna-seq脚本
1、Make tag directories for each experiment
cd /Share/home/wangdong/mfl/result/OvCa_result/2HOMER
makeTagDirectory /Share/home/wangdong/mfl/result/OvCa_result/2HOMER/OvCa_1T /Share/home/wangdong/mfl/result/OvCa_result/2HOMER/Tophat_accepted_hits/Ovarian_1T.bam -sspe
makeTagDirectory /Share/home/wangdong/mfl/result/OvCa_result/2HOMER/OvCa_1M /Share/home/wangdong/mfl/result/OvCa_result/2HOMER/Tophat_accepted_hits/Ovarian_1M.bam -sspe

makeTagDirectory /Share/home/wangdong/mfl/result/OvCa_result/2HOMER/OvCa_3T /Share/home/wangdong/mfl/result/OvCa_result/2HOMER/Tophat_accepted_hits/Ovarian_3T.bam -sspe
makeTagDirectory /Share/home/wangdong/mfl/result/OvCa_result/2HOMER/OvCa_3M /Share/home/wangdong/mfl/result/OvCa_result/2HOMER/Tophat_accepted_hits/Ovarian_3M.bam -sspe

makeTagDirectory /Share/home/wangdong/mfl/result/OvCa_result/2HOMER/OvCa_4T /Share/home/wangdong/mfl/result/OvCa_result/2HOMER/Tophat_accepted_hits/Ovarian_4T.bam -sspe
makeTagDirectory /Share/home/wangdong/mfl/result/OvCa_result/2HOMER/OvCa_4M /Share/home/wangdong/mfl/result/OvCa_result/2HOMER/Tophat_accepted_hits/Ovarian_4M.bam -sspe

2、Make bedGraph visualization files for each tag directory

4、Quantify gene expression as integer counts for differential expression (-noadj)
analyzeRepeats.pl rna hg19 -count exons -d ./OvCa_1T ./OvCa_1M -noadj > OvCa_1T_1M_count.txt
analyzeRepeats.pl rna hg19 -count exons -d ./OvCa_3T ./OvCa_3M -noadj > OvCa_3T_3M_count.txt
analyzeRepeats.pl rna hg19 -count exons -d ./OvCa_4T ./OvCa_4M -noadj > OvCa_4T_4M_count.txt


5、Calculate differential expression (requires R/Bioconductor and the package edgeR to be installed!!)
getDiffExpression.pl OvCa_1T_1M_count.txt tumor metastasis > OvCa_1T_1M.diffExp_count.txt
getDiffExpression.pl OvCa_3T_3M_count.txt tumor metastasis > OvCa_3T_3M.diffExp_count.txt
getDiffExpression.pl OvCa_4T_4M_count.txt tumor metastasis > OvCa_4T_4M.diffExp_count.txt



configureBclToFastq.pl --input-dir /Share/home/wangdong/data/rawdata/150315_C00126_0172_AHFNJ5ADXX/Data/Intensities/BaseCalls --output ./Unaligned2 --sample-sheet ./SampleSheet_undetermined.csv --no-eamss


我觉得可能可以了，你试一下，把这个命令写在一个bash脚本里，使用绝对路径，然后qsub这个脚本
qsub -V foo.sh -o foo.log
LKQ  服务器所在位置
/Share/home/wangdong/lkq/Rscipt/htmap/heatmapV5.1.r

configureBclToFastq.pl --input-dir /Share/home/wangdong/data/rawdata/150311_C00126_0168_AHFNLMADXX/Data/Intensities/BaseCalls --output ./Unaligned2 --sample-sheet ./SampleSheet_undetermined.csv --no-eamss
换成苹果得到的sample sheet由于分隔符不同而报错

configureBclToFastq.pl --input-dir /Share/home/wangdong/data/rawdata/150210_C00126_0166_AC6EYWANXX/Data/Intensities/BaseCalls --output ./Unaligned3_undetermined --sample-sheet ./SampleSheet_undetermined.csv --no-eamss

 最终调节版，数据自己变过后，里面就不用再取了
library(ggplot2)
tiff("new5.tiff")
a<- read.csv("mRNA_right.csv",head=TRUE,sep=";")
P.Value <- c(a$PValue)
FC <- c(a$FC)
df <- data.frame(P.Value, FC)
df$threshold = as.factor(((df$FC > 1) |(df$FC < -1) ) & df$P.Value > 1.30103)
g = ggplot(data=df, aes(x=FC, y=P.Value, colour=threshold)) +
 geom_point(alpha=0.4, size=1.75) +
  theme(legend.position = "none") +  
  xlim(c(-5, 5)) + ylim(c(0, 5)) +
  xlab("log2 fold change") + ylab("-log10 p-value")
g
dev.off()

library(ggplot2)
tiff("new2.tiff")
a<- read.csv("mRNA2.csv",head=TRUE,sep=";")
P.Value <- c(a$PValue)
FC <- c(a$FC)
df <- data.frame(P.Value, FC)
df$threshold = as.factor(abs(df$FC) > 1 & df$P.Value < 0.05) 这几部数据必须行数一致，检查一下空格，以及是不是P.Value，FC均有，不能在抬头里面有P_Value 中文下划线
 
##Construct the plot object
g = ggplot(data=df, aes(x=FC, y=-log10(P.Value), colour=threshold)) +
 geom_point(alpha=0.4, size=1.75) +
  theme(legend.position = "none") +        #错误: Use 'theme' instead. (Defunct; last used in version 0.9.1), 将原来的opts换成theme
  xlim(c(-5, 5)) + ylim(c(0, 5)) +
  xlab("log2 fold change") + ylab("-log10 p-value")
g
dev.off()

 Mac其实所有移动硬盘都在/Volumes/目录中
$cd /
$cd /Volumes

 R画volcano plot from baidu
分类： R 2013-03-22 23:10 1461人阅读 评论(0) 收藏 举报
require(ggplot2)
png("")
##Highlight genes that have an absolute fold change > 2 and a p-value < Bonferroni cut-off
a <- read.table("",header=TRUE,sep="\t",)
P.Value <- c(a$R画volcano plot)
FC <- c(a$FC)
df <- data.frame(P.Value, FC)
df$threshold = as.factor(abs(df$FC) > 1 & df$P.Value < 0.05)
 
##Construct the plot object
g = ggplot(data=df, aes(x=FC, y=-log10(P.Value), colour=threshold)) +
 geom_point(alpha=0.4, size=1.75) +
  opts(legend.position = "none") +
  xlim(c(-5, 5)) + ylim(c(0, 5)) +
  xlab("log2 fold change") + ylab("-log10 p-value")
g
dev.off()



homer李洋在A集群上安装位置
/Share/home/lulab1/users/liyang/apps/homer/bin

自己摸索chip分析流程（homer几步）
1，mapping,bowtie2
bowtie2 -q -N 1 -x /Work1/home/wangdong/ensemble/genome -1 /Work1/home/wangdong/lss/data/GQ_CHIP/GQ-1_R1.fastq -2 /Work1/home/wangdong/lss/data/GQ_CHIP/GQ-1_R2.fastq -S /Work1/home/wangdong/lss/data/GQ_CHIP/mapping/GQ_H3K4ME.sam
bowtie2 -q -N 1 -x /Work1/home/wangdong/ensemble/genome -1 /Work1/home/wangdong/lss/data/GQ_CHIP/GQ-2_R1.fastq -2 /Work1/home/wangdong/lss/data/GQ_CHIP/GQ-2_R2.fastq -S /Work1/home/wangdong/lss/data/GQ_CHIP/mapping/GQ_H3K4ME2.sam

###选用的mapping为bowtie2，homer上推荐用bowtie，所以bowtie2参数说明如下
bowtie2 [options]* -x <bt2-idx> {-1 <m1> -2 <m2> | -U <r>} -S [<hit>]

必须参数：

    -x <bt2-idx> 由bowtie2-build所生成的索引文件的前缀。首先 在当前目录搜寻，然后
    在环境变量 BOWTIE2_INDEXES 中制定的文件夹中搜寻。
    -1 <m1> 双末端测寻对应的文件1。可以为多个文件，并用逗号分开；多个文件必须和 -2 
    <m2> 中制定的文件一一对应。比如:"-1 flyA_1.fq,flyB_1.fq -2 flyA_2.fq,flyB
    _2.fq". 测序文件中的reads的长度可以不一样。
    -2 <m2> 双末端测寻对应的文件2.
    -U <r> 非双末端测寻对应的文件。可以为多个文件，并用逗号分开。测序文件中的reads的
    长度可以不一样。
    -S <hit> 所生成的SAM格式的文件前缀。默认是输入到标准输出。


2提交任务
perl bsub.pl mapping_gq_chip_bowtie2.sh 1 TINY ./source.txt
samtobam
samtools view -bS /Work1/home/wangdong/lss/data/GQ_CHIP/mapping/GQ_H3K4ME.sam > /Work1/home/wangdong/lss/data/GQ_CHIP/mapping/GQ_H3K4ME1.bam

提交任务
GQ_samtobam1.sh
samtobam.sh
perl bsub.pl samtobam.sh 1 TINY ./source.txt

bamToBed -i /Work1/home/wangdong/lss/data/GQ_CHIP/mapping/GQ_H3K4ME1.bam | awk 'BEGIN{FS="\t";OFS="\t"}{if($1=="MT"){print "chrM",$2,$3,$4,$5,$6}else{print "chr"$1,$2,$3,$4,$5,$6}}' > /Work1/home/wangdong/lss/data/GQ_CHIP/mapping/GQ_H3K4ME1.bed
bamToBed -i /Work1/home/wangdong/lss/data/GQ_CHIP/mapping/GQ_H3K4ME2.bam | awk 'BEGIN{FS="\t";OFS="\t"}{if($1=="MT"){print "chrM",$2,$3,$4,$5,$6}else{prznt "chr"$1,$2,$3,$4,$5,$6}}' > /Work1/home/wangdong/lss/data/GQ_CHIP/mapping/GQ_H3K4ME2.bed

bamtobed.sh
perl bsub.pl bamtobed.sh 1 TINY ./source.txt

makeTagDirectory /Work1/home/wangdong/lss/data/GQ_CHIP/maketags_H3K4ME1/ /Work1/home/wangdong/lss/data/GQ_CHIP/mapping/GQ_H3K4ME1.bed -genome /Work1/home/wangdong/ensemble/genome.fa -sspe
makeTagDirectory /Work1/home/wangdong/lss/data/GQ_CHIP/maketags_h3k4me2/ /Work1/home/wangdong/lss/data/GQ_CHIP/mapping/GQ_H3K4ME2.bed -genome /Work1/home/wangdong/ensemble/genome.fa -sspe

maketagdirectory.sh
perl bsub.pl maketagdirectory.sh 1 TINY ./source.txt

removeOutOfBoundsReads.pl /Work1/home/wangdong/lss/data/GQ_CHIP/maketags_H3K4ME1/ /Work1/home/wangdong/ensemble/genome.fa -chromSizes /Work1/home/wangdong/ensemble/hg19.chrom.sizes
removeOutOfBoundsReads.pl /Work1/home/wangdong/lss/data/GQ_CHIP/maketags_h3k4me2/ /Work1/home/wangdong/ensemble/genome.fa -chromSizes /Work1/home/wangdong/ensemble/hg19.chrom.sizes

makeUCSCfile /Work1/home/wangdong/lss/data/GQ_CHIP/maketags_H3K4ME1/ -o auto -bigWig /Work1/home/wangdong/ensemble/hg19.chrom.sizes -norm 1e7 -strand both -fsize 1e20 > /Work1/home/wangdong/lss/data/GQ_CHIP/maketags_H3K4ME1/tags.trackInfo2.txt
makeUCSCfile /Work1/home/wangdong/lss/data/GQ_CHIP/maketags_h3k4me2/ -o auto -bigWig /Work1/home/wangdong/ensemble/hg19.chrom.sizes -norm 1e7 -strand both -fsize 1e20 > /Work1/home/wangdong/lss/data/GQ_CHIP/maketags_h3k4me2/tags.trackInfo2.txt

perl bsub.pl makeUCSCfile.sh 1 TINY ./source.txt

findPeaks <tag directory> -style <factor|histone|groseq> -o auto -i <control tag directory>

findPeaks /Work1/home/wangdong/lss/data/GQ_CHIP/maketags_H3K4ME1/ -style histone -o auto
findPeaks /Work1/home/wangdong/lss/data/GQ_CHIP/maketags_h3k4me2/ -style histone -o auto

perl bsub.pl findpeak.sh 1 TINY ./source.txt

pos2bed.pl regions.txt > H3K4ME2.regions.bed
$pos2bed.pl regions.txt > H3K4ME1.regions.bed

        Converted 107445 peaks total
pos2bed.pl regions.txt > H3K4ME2.regions.bed

        Converted 89935 peaks total


#李洋方法
findPeaks $path0/2.tags/$i -i $input480 -o auto -style histone -F 4 -L 4 -size 1000 -fdr 0.001 -nfr
pos2bed.pl $path0/2.tags/$i/regions.txt > $path0/2.tags/$i/$i.regions.bed

chip李洋所在位置
/Share/home/lulab1/users/liyang/project/wanglab/bin


chip分析小玄推荐
先maketag
findPeaks <tag directory> -style <factor|histone|groseq> -o auto -i <control tag directory>

李洋推荐
没有一个工具是可以从头到尾的，可以查查MACS和MEME
现在的结果是homer
我用过macs，meme，感觉他们的结果会更好一些

尝试利用-fragLength given(?)去将依旧还是有一些峰的intron隐藏，似乎不行？
bamToBed -i accepted_hits.bam | awk 'BEGIN{FS="\t";OFS="\t"}{if($1=="MT"){print "chrM",$2,$3,$4,$5,$6}else{print "chr"$1,$2,$3,$4,$5,$6}}' > accepted_hits.bed
makeTagDirectory tags2/ accepted_hits.bed -genome /Work1/home/wangdong/ensemble/genome.fa -sspe
removeOutOfBoundsReads.pl tags/ /Work1/home/wangdong/ensemble/genome.fa -chromSizes /Work1/home/wangdong/ensemble/hg19.chrom.sizes     
makeUCSCfile tags2 -o auto -bigWig 	 -norm 1e7 -strand both -fsize 1e20 -fragLength given > tags2/tags.trackInfo2.txt

可视化流程过程(swc2_swc3_swnc_swm1_swm2全按下面流程，在A集群上)
bamToBed -i accepted_hits.bam | awk 'BEGIN{FS="\t";OFS="\t"}{if($1=="MT"){print "chrM",$2,$3,$4,$5,$6}else{print "chr"$1,$2,$3,$4,$5,$6}}' > accepted_hits.bed
makeTagDirectory tags/ accepted_hits.bed -genome /Work1/home/wangdong/ensemble/genome.fa -sspe
removeOutOfBoundsReads.pl tags/ /Work1/home/wangdong/ensemble/genome.fa -chromSizes /Work1/home/wangdong/ensemble/hg19.chrom.sizes     
makeUCSCfile tags -o auto -bigWig /Work1/home/wangdong/ensemble/hg19.chrom.sizes -norm 1e7 -strand both -fsize 1e20 > tags/tags.trackInfo.txt      也调用了bedGraphToBigWig，所以这个一样要首先安装好
参考序列所在目录
/Work1/home/wangdong/ensemble/genome.fa

/Work1/home/wangdong/lss/20141021/Undetermined_indices/sw4/RNA_seq/SWC2_1/accepted_hits.bed
configureBclToFastq.pl --input-dir /Share/home/wangdong/data/rawdata/150210_C00126_0166_AC6EYWANXX/Data/Intensities/BaseCalls --output ./Unaligned3_undetermined --sample-sheet ./SampleSheet_undetermined.csv --no-eamss

configureBclToFastq.pl --input-dir /Share/home/wangdong/data/rawdata/150210_C00126_0166_AC6EYWANXX/Data/Intensities/BaseCalls --output ./Unaligned2_undetermined --sample-sheet ./SampleSheet_undetermined.csv --no-eamss
configureBclToFastq.pl --input-dir /Share/home/wangdong/data/rawdata/150210_C00126_0166_AC6EYWANXX/Data/Intensities/BaseCalls --output ./Unaligned --sample-sheet ./SampleSheet.csv --no-eamss --use-bases-mask Y126,I6n,Y126


/Share/home/lulab1/users/liyang/project/wanglab/tmp

homer将mapping文件进行可视化流程，李洋pipeline
#!/usr/bin/bash
## add "chr" to chromsome id; change MT to chrM
bamToBed -i accepted_hits.bam | awk 'BEGIN{FS="\t";OFS="\t"}{if($1=="MT"){print "chrM",$2,$3,$4,$5,$6}else{print "chr"$1,$2,$3,$4,$5,$6}}' > accepted_hits.bed
## create tag index
makeTagDirectory tags/ accepted_hits.bed -genome hg19 -sspe
## create UCSC bigwig file for visualization
removeOutOfBoundsReads.pl tags/ hg19 -chromSizes ./hg19.chrom.sizes
makeUCSCfile tags -o auto -bigWig ./hg19.chrom.sizes -norm 1e7 -strand both -fsize 1e20 > tags/tags.trackInfo.txt

##关于链特异性，来自RNA-seq数据Interpreting the correct strand                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       
    By default, HOMER will interpret each read as it is presented in the alignment file.  This can differ from the expected results if you have paired-end sequencing from a strand-specific RNA-Seq experiment.  It can also result in reads aligning to the opposite strand if a 'first-strand' library synthesis protocol is used, such as methods that use the deoxy-UTP incorporation.  To control how HOMER interprets the strands you have two choices: Either use "-strand -" when running programs like annotatePeaks.pl when you would normally use "-strand +", or you can flip the strand during the makeTagDirectory step: 
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        makeTagDirectory $path0/Tags/$i $path0/Raw/$i/bowtie/$i.bam -unique -genome hg19 -checkGC
        -flip : (will flip the strands of each read in the alignment file)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               948  makeTagDirectory test accepted_hits.bam -unique -genome hg19 -checkGC
        -sspe : (for Strand Specific Paired End sequencing - this will flip the 2nd read of the mate-pair to match the first read).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      949  which makeTagDirectory 
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         950  /Share/home/lulab1/users/liyang/apps/homer/bin/makeTagDirectory tagsDir accepted_hits.bam -unique -genome hg19 -checkGC
    For example, if you're analyzing data from Illumina's newer strand specific RNA-Seq kits, you'll probably want to use "-flip" and also "-sspe" if you have a paired-end experiment                                                                                                                                                                                                                                                                                                                                                                                                                                                   951  ll
  952  cd tagsDir/
  953  ls
  954  ll
  955  less 10.tags.tsv 
  956  ls
  957  ll
  958  cd ../
  959  vi /Share/home/lulab1/users/liyang/project/Wanglab/ChIP/bin/6.makeUCSCfile.sh 
  960  ll
  961  makeUCSCfile $path1/$i -o auto -bigWig $path0/hg19.chrom.sizes -fsize 1e20 > $path1/$i/$i.trackInfo.txt
  962  makeUCSCfile tagsDir/ -o auto -bigWig /Share/home/lulab1/users/liyang/project/Wanglab/ChIP/hg19.chrom.sizes -fsize 1e20 > tagsDir/test.trackInfo.txt
  
cat tags.bed | awk 'BEGIN{FS="\t";OFS="\t"}{print "chr"$1,$2,$3,$4,$5,$6}'

> mx <- matrix(randu())
randu
> mx <- matrix(1:10,ncol=2)
> head(mx)
     [,1] [,2]
[1,]    1    6
[2,]    2    7
[3,]    3    8
[4,]    4    9
[5,]    5   10
> mx2 <- matrix(10:20,ncol=2)
Warning message:
In matrix(10:20, ncol = 2) :
ggplot 
 
  data length [11] is not a sub-multiple or multiple of the number of rows [6]
> mx2 <- matrix(10:21,ncol=2)
> head(mx2)
     [,1] [,2]
[1,]   10   16
[2,]   11   17
[3,]   12   18
[4,]   13   19
[5,]   14   20
[6,]   15   21
> mx  <- cbind(mx,"Red")
> mx2 <- cbind(mx2,"Blue")
> tmp <- rbind(mx,mx2)
> head(tmp)
     [,1] [,2] [,3]  
[1,] "1"  "6"  "Red" 
[2,] "2"  "7"  "Red" 
[3,] "3"  "8"  "Red" 
[4,] "4"  "9"  "Red" 
[5,] "5"  "10" "Red" 
[6,] "10" "16" "Blue"
> tmp <- data.frame(tmp)
> head(tmp)
  X1 X2   X3
1  1  6  Red
2  2  7  Red
3  3  8  Red
4  4  9  Red
5  5 10  Red
6 10 16 Blue
> library(ggplot2)
Error in library(ggplot2) : there is no package called ggplot2?
> library("ggplot2")
Error in library("ggplot2") : there is no package called ggplot2?
> ggplot2(tmp,aes(x=tmp$X1,y=tmp$X2)) + geom_point(aes(colour=tmp$X3))


  git clone 命令参数：
复制代码

usage: git clone [options] [--] <repo> [<dir>]
7.git clone: 
这是较为简单的一种初始化方式，当你已经有一个远程的Git版本库，只需要在本地克隆一份，例如'git clone git://github.com/someone/some_project.git some_project'命令就是将'git://github.com/someone/some_project.git'这个URL地址的远程版 本库完全克隆到本地some_project目录下面

 参数挺多，但常用的就几个：

1. 最简单直接的命令

git clone xxx.git

2. 如果想clone到指定目录

git clone xxx.git "指定目录"

3. clone时创建新的分支替代默认Origin HEAD（master）

git clone -b [new_branch_name]  xxx.git

4. clone 远程分支

　　git clone 命令默认的只会建立master分支，如果你想clone指定的某一远程分支(如：dev)的话，可以如下：

　　A. 查看所有分支(包括隐藏的)  git branch -a 显示所有分支，如：　　　　

* master
  remotes/origin/HEAD -> origin/master
  remotes/origin/dev
  remotes/origin/master

　　B.  在本地新建同名的("dev")分支，并切换到该分支

git checkout -t origin/dev 该命令等同于：
git checkout -b dev origin/dev



correlation作图
m=read.table("ratio100_sw.txt",head=T)  直接读取txt
for (i in 2:17){
    m[,i]=log(m[,i]+1e-3)
}
colnames(m)=c("genes","SW1","SW2","SW3","SW4","SW5","SW6","SW7","SW8","SW9","SW10","SW11","SW12","SW13","SW14","SW15","SW16")


## functions, scatter plot
panel.cor.scale <- function(x, y, digits=2, prefix="", cex.cor)
{
usr <- par("usr"); on.exit(par(usr))
par(usr = c(0, 1, 0, 1))
r = (cor(x, y,use="pairwise"))
txt <- format(c(r, 0.123456789), digits=digits)[1]
txt <- paste(prefix, txt, sep="")
if(missing(cex.cor)) cex <- 0.8/strwidth(txt)
text(0.5, 0.5, txt, cex = cex * abs(r))
}

## function, correlation coefficient
panel.cor <- function(x, y, digits=2, prefix="", cex.cor)
{
usr <- par("usr"); on.exit(par(usr))
par(usr = c(0, 1, 0, 1))
r = (cor(x, y,use="pairwise"))
txt <- format(c(r, 0.123456789), digits=digits)[1]
txt <- paste(prefix, txt, sep="")
if(missing(cex.cor)) cex <- 0.8/strwidth(txt)
text(0.5, 0.5, txt, cex = cex )


}


#panel.hist <- function(x, ...)
#{
#usr <- par("usr"); on.exit(par(usr))
#par(usr = c(usr[1:2], 0, 1.5) )
#h <- hist(x, plot = FALSE)
#breaks <- h$breaks; nB <- length(breaks)
#y <- h$counts; y <- y/max(y)
#rect(breaks[-nB], 0, breaks[-1], y, col="cyan", ...)
#}
pairs.panels <- function (x,y,smooth=FALSE,scale=FALSE)  #为了不要线，此处将smooth设定平滑拟合曲线为FALSE即可以关掉了
{if (smooth){
        if (scale) {
                pairs(x,upper.panel=panel.cor.scale,lower.panel=panel.smooth)
        }
        else {pairs(x,upper.panel=panel.cor,lower.panel=panel.smooth,gap=0)
        } #else {pairs(x,upper.panel=panel.cor,lower.panel=panel.smooth)
}
else #smooth is not true
        { if (scale) {pairs(x,diag.panel=panel.hist,upper.panel=panel.cor.scale)
          } else {pairs(x,upper.panel=panel.cor) }
        } #end of else (smooth)
} #end of function

pdf("cor_sw_again.pdf",height=10,width=10)
pairs.panels(m[,-1])

dev.off()


> mydata <- read.csv("c2_c3.csv")
> tiff("c2_c3_edit.tiff")
> plot(mydata$ctnnb1_shRNA1_rpkm,mydata$ctnnb1_shRNA2_rpkm,lty=2,pch=20,cex=0.5,main="expression difference between ctnnb1_shRNA1 and ctnnb1_shRNA2",xlab="log10(ctnnb1_shRNA1_rpkm)",ylab="log10(ctnnb1_shRNA2_rpkm)")
> dev.off()


王会丽new，分析大库中样品的差异
> library("limma")
> library("edgeR")
> targets = readTargets("F:/lss/work/20150113/WHL/whl/final_result/edger/targets.txt")
Warning message:
In read.table(file, header = TRUE, stringsAsFactors = FALSE, sep = sep,  :
  incomplete final line found by readTableHeader on 'F:/lss/work/20150113/WHL/whl/final_result/edger/targets.txt'
在文件的最后一行加个回车
原来最后一行的结尾不是EOL（End of line）字符，所以R提醒你数据是不是不完全。
targets = readTargets("F:/lss/work/20150113/WHL/whl/final_result/edger/targets.txt")  ##sample information 读取数据的方法不局限，只需要将计数过得reads数据以矩阵形式读入即可

d = readDGE(targets$file,group = targets$group)
colnames(d) = targets$description

keep = rowSums(cpm(d) > 1) >= 2   #edgeR::cpm 		Counts per Million or Reads per Kilobase per Million,rowSums:Give Column Sums of a Matrix or Data Frame
d = d[keep,]
## remove tags with low abundance

d$samples$lib.size = colSums(d$counts)

d = calcNormFactors(d) #默认为TMM标准化

tiff("F:/lss/work/20150113/WHL/whl/final_result/edger/d2_d5_Information.par.tiff")

plotMDS(d) #Multidimensional scaling plot of digital gene expression profiles

d = estimateCommonDisp(d) #Maximizes the negative binomial conditional common likelihood to give the estimate of the common dispersion across all tags
d = estimateTagwiseDisp(d) #Estimates tagwise dispersion values by an empirical Bayes method based on weighted conditional maximum likelihood. 
tiff("F:/lss/work/20150113/WHL/whl/final_result/edger/d2_d5_MeanVar.tiff")
plotMeanVar(d, show.tagwise.vars = TRUE, NBline = TRUE) # edgeR::binMeanVar 		Explore the mean-variance relationship for DGE data
tiff("F:/lss/work/20150113/WHL/whl/final_result/edger/d2_d5_BCV.tiff")
plotBCV(d) #edgeR::plotBCV 		Plot Biological Coefficient of Variation

de = exactTest(d, pair = c("TREAT","CONTROL")) #Exact Tests for Differences between Two Groups of Negative-Binomial Counts，Compute genewise exact tests for differences in the means between two groups of negative-binomially distributed counts
### differential expreesed gene
top = topTags(de)

deinfo = de$table
result = cbind(cpm(d)[rownames(de),],deinfo[rownames(de),])# ，cbind() 把矩阵横向合并成一个大矩阵（列方式），而rbind()是纵向合并（行方式）。在命令中> X <- cbind(arg 1 , arg 2 , arg 3 , ...) cbind() 的参数要么是任何长度的向量，要么是列长度一致的的矩阵（即行数一样）。结果将是一个合并arg1 , arg2 , . . . 的列形成的矩阵。
write.table(result,"F:/lss/work/20150113/WHL/whl/final_result/edger/d2_d5_DEG.txt",quote = F, col.names = T, row.names = T, sep = "\t")

summary(de.sig <- decideTestsDGE(de)) #multiple Testing Across Genes and Contrasts，Classify a series of related differential expression statistics as up, down or not significant. A number of different multiple testing schemes are offered which adjust for multiple testing down the genes as well as across contrasts for each gene. 
> summary(de.sig <- decideTestsDGE(de))
   [,1]
-1 4067
0  1909
1  4108
   [,1]
-1 4181
0  1599
1  4256


detags <- rownames(d)[as.logical(de.sig)]
result.sig = result[detags,]
write.table(result.sig,"F:/lss/work/20150113/WHL/whl/final_result/edger/d2_d5__p.05.txt",quote = F, col.names = T, row.names = T, sep = "\t")

tiff("F:/lss/work/20150113/WHL/whl/final_result/edger/d2_d5_Smear.tiff")
plotSmear(de,de.tags = detags)
abline(h =c(-2,2), col = "blue")

dev.off()


c2_c3_nc过滤
library(limma)
library("edgeR")
targets = readTargets("F:/lss/work/20150113/WHL/whl/final_result/edger/targets.txt")  ##sample information 读取数据的方法不局限，只需要将计数过得reads数据以矩阵形式读入即可

d = readDGE(targets$file,group = targets$group)
colnames(d) = targets$description

s= 2   #edgeR::cpm 		Counts per Million or Reads per Kilobase per Million,rowSums:Give Column Sums of a Matrix or Data Frame
d = d[keep,]
## remove tags with low abundance

d$samples$lib.size = colSums(d$counts)

d = calcNormFactors(d) #默认为TMM标准化

tiff("D:/htseq_result/c2_c3_nc/check_filtered/c_f_Information.par.tiff")

plotMDS(d) #Multidimensional scaling plot of digital gene expression profiles

d = estimateCommonDisp(d)
d = estimateTagwiseDisp(d)
tiff("D:/htseq_result/c2_c3_nc/check_filtered/c_f_MeanVar.tiff")
plotMeanVar(d, show.tagwise.vars = TRUE, NBline = TRUE)
tiff("D:/htseq_result/c2_c3_nc/check_filtered/c_f_BCV.tiff")
plotBCV(d)

de = exactTest(d, pair = c("TREAT","CONTROL"))
### differential expreesed gene
top = topTags(de)

deinfo = de$table
result = cbind(cpm(d)[rownames(de),],deinfo[rownames(de),])# ，cbind() 把矩阵横向合并成一个大矩阵（列方式），而rbind()是纵向合并（行方式）。在命令中> X <- cbind(arg 1 , arg 2 , arg 3 , ...) cbind() 的参数要么是任何长度的向量，要么是列长度一致的的矩阵（即行数一样）。结果将是一个合并arg1 , arg2 , . . . 的列形成的矩阵。
write.table(result,"D:/htseq_result/c2_c3_nc/check_filtered/c_f_DEG.txt",quote = F, col.names = T, row.names = T, sep = "\t")

summary(de.sig <- decideTestsDGE(de))
  [,1] 
-1  2091
0  11821
1   2231


detags <- rownames(d)[as.logical(de.sig)]
result.sig = result[detags,]
write.table(result.sig,"D:/htseq_result/c2_c3_nc/check_filtered/c_f_p.05.txt",quote = F, col.names = T, row.names = T, sep = "\t")

tiff("D:/htseq_result/c2_c3_nc/check_filtered/c_f_Smear.tiff")
plotSmear(de,de.tags = detags)
abline(h =c(-1,1), col = "blue")

dev.off()

m1_m2_nc不过滤
library(limma)
library("edgeR")
targets = readTargets("D:/htseq_result/m1_m2_nc/targets.txt")  ##sample information 读取数据的方法不局限，只需要将计数过得reads数据以矩阵形式读入即可

d = readDGE(targets$file,group = targets$group)s
colnames(d) = targets$description

d$samples$lib.size = colSums(d$counts)

d = calcNormFactors(d)

tiff("D:/htseq_result/m1_m2_nc/m1_m2_nc_no_filtered/Information.par.tiff")

plotMDS(d) #Multidimensional scaling plot of digital gene expression profiles

d = estimateCommonDisp(d)
d = estimateTagwiseDisp(d)
tiff("D:/htseq_result/m1_m2_nc/m1_m2_nc_no_filtered/MeanVar.tiff")
plotMeanVar(d, show.tagwise.vars = TRUE, NBline = TRUE)
tiff("D:/htseq_result/m1_m2_nc/m1_m2_nc_no_filtered/BCV.tiff")
plotBCV(d)

de = exactTest(d, pair = c("TREAT","CONTROL"))
### differential expreesed gene
top = topTags(de)

deinfo = de$table
result = cbind(cpm(d)[rownames(de),],deinfo[rownames(de),])# ，cbind() 把矩阵横向合并成一个大矩阵（列方式），而rbind()是纵向合并（行方式）。在命令中> X <- cbind(arg 1 , arg 2 , arg 3 , ...) cbind() 的参数要么是任何长度的向量，要么是列长度一致的的矩阵（即行数一样）。结果将是一个合并arg1 , arg2 , . . . 的列形成的矩阵。
write.table(result,"D:/htseq_result/m1_m2_nc/m1_m2_nc_no_filtered/DEG.txt",quote = F, col.names = T, row.names = T, sep = "\t")

summary(de.sig <- decideTestsDGE(de))
   [,1] 
-1   117
0  61614
1    337

detags <- rownames(d)[as.logical(de.sig)]
result.sig = result[detags,]
write.table(result.sig,"D:/htseq_result/m1_m2_nc/m1_m2_nc_no_filtered/p.05.txt",quote = F, col.names = T, row.names = T, sep = "\t")

tiff("D:/htseq_result/m1_m2_nc/m1_m2_nc_no_filtered/Smear.tiff")
plotSmear(de,de.tags = detags)
abline(h =c(-1,1), col = "blue")

dev.off()

m1_m2_nc过滤
library(limma)
library("edgeR")
targets = readTargets("D:/htseq_result/m1_m2_nc/targets.txt")  ##sample information 读取数据的方法不局限，只需要将计数过得reads数据以矩阵形式读入即可

d = readDGE(targets$file,group = targets$group)
colnames(d) = targets$description

keep = rowSums(cpm(d) > 1) >= 2   #edgeR::cpm 		Counts per Million or Reads per Kilobase per Million,rowSums:Give Column Sums of a Matrix or Data Frame
d = d[keep,]
## remove tags with low abundance

d$samples$lib.size = colSums(d$counts)

d = calcNormFactors(d)

tiff("D:/htseq_result/m1_m2_nc/m1_m2_nc_filter/Information.par.tiff")

plotMDS(d) #Multidimensional scaling plot of digital gene expression profiles

d = estimateCommonDisp(d)
d = estimateTagwiseDisp(d)
tiff("D:/htseq_result/m1_m2_nc/m1_m2_nc_filter/MeanVar.tiff")
plotMeanVar(d, show.tagwise.vars = TRUE, NBline = TRUE)
tiff("D:/htseq_result/m1_m2_nc/m1_m2_nc_filter/BCV.tiff")
plotBCV(d)

de = exactTest(d, pair = c("TREAT","CONTROL"))
### differential expreesed gene
top = topTags(de)

deinfo = de$table
result = cbind(cpm(d)[rownames(de),],deinfo[rownames(de),])# ，cbind() 把矩阵横向合并成一个大矩阵（列方式），而rbind()是纵向合并（行方式）。在命令中> X <- cbind(arg 1 , arg 2 , arg 3 , ...) cbind() 的参数要么是任何长度的向量，要么是列长度一致的的矩阵（即行数一样）。结果将是一个合并arg1 , arg2 , . . . 的列形成的矩阵。
write.table(result,"D:/htseq_result/m1_m2_nc/m1_m2_nc_filter/DEG.txt",quote = F, col.names = T, row.names = T, sep = "\t")

summary(de.sig <- decideTestsDGE(de))
 [,1] 
-1   313
0  14525
1    545

detags <- rownames(d)[as.logical(de.sig)]
result.sig = result[detags,]
write.table(result.sig,"D:/htseq_result/m1_m2_nc/m1_m2_nc_filter/p.05.txt",quote = F, col.names = T, row.names = T, sep = "\t")

tiff("D:/htseq_result/m1_m2_nc/m1_m2_nc_filter/Smear.tiff")
plotSmear(de,de.tags = detags)
abline(h =c(-1,1), col = "blue")

dev.off()

c2_c3_nc过滤一下

library(limma)
library("edgeR")
targets = readTargets("D:/htseq_result/targets.txt")  ##sample information 读取数据的方法不局限，只需要将计数过得reads数据以矩阵形式读入即可

d = readDGE(targets$file,group = targets$group)
colnames(d) = targets$description

keep = rowSums(cpm(d) > 1) >= 2   #edgeR::cpm 		Counts per Million or Reads per Kilobase per Million,rowSums:Give Column Sums of a Matrix or Data Frame
d = d[keep,]
## remove tags with low abundance

d$samples$lib.size = colSums(d$counts)

d = calcNormFactors(d)

tiff("D:/htseq_result/c2_c3_nc_filtered/Information.par.tiff")

plotMDS(d) #Multidimensional scaling plot of digital gene expression profiles

d = estimateCommonDisp(d)
d = estimateTagwiseDisp(d)
tiff("D:/htseq_result/c2_c3_nc_filtered/MeanVar.tiff")
plotMeanVar(d, show.tagwise.vars = TRUE, NBline = TRUE)
tiff("D:/htseq_result/c2_c3_nc_filtered/BCV.tiff")
plotBCV(d)

de = exactTest(d, pair = c("TREAT","CONTROL"))
### differential expreesed gene
top = topTags(de)

deinfo = de$table
result = cbind(cpm(d)[rownames(de),],deinfo[rownames(de),])# ，cbind() 把矩阵横向合并成一个大矩阵（列方式），而rbind()是纵向合并（行方式）。在命令中> X <- cbind(arg 1 , arg 2 , arg 3 , ...) cbind() 的参数要么是任何长度的向量，要么是列长度一致的的矩阵（即行数一样）。结果将是一个合并arg1 , arg2 , . . . 的列形成的矩阵。
write.table(result,"D:/htseq_result/c2_c3_nc_filtered/DEG.txt",quote = F, col.names = T, row.names = T, sep = "\t")

summary(de.sig <- decideTestsDGE(de))

detags <- rownames(d)[as.logical(de.sig)]
result.sig = result[detags,]
write.table(result.sig,"D:/htseq_result/c2_c3_nc_filtered/p.05.txt",quote = F, col.names = T, row.names = T, sep = "\t")

tiff("D:/htseq_result/c2_c3_nc_filtered/Smear.tiff")
plotSmear(de,de.tags = detags)
abline(h =c(-1,1), col = "blue")

dev.off()

最终结果还是要c2_c3一起，过滤,不然差异表达的可达到4000多个，显然不是想要的，如果想要不过滤的也可以，另外，该软件许多过程评估都需要replicate的，所以讲c2,c3一起作用
library(limma)
library("edgeR")
targets = readTargets("D:/htseq_result/targets.txt")  ##sample information 读取数据的方法不局限，只需要将计数过得reads数据以矩阵形式读入即可

d = readDGE(targets$file,group = targets$group)
colnames(d) = targets$description

d$samples$lib.size = colSums(d$counts)

d = calcNormFactors(d)

tiff("D:/htseq_result/final_c2_c3_nc/Information.par.tiff")

plotMDS(d) #Multidimensional scaling plot of digital gene expression profiles

d = estimateCommonDisp(d)
d = estimateTagwiseDisp(d)
tiff("D:/htseq_result/final_c2_c3_nc/MeanVar.tiff")
plotMeanVar(d, show.tagwise.vars = TRUE, NBline = TRUE)
tiff("D:/htseq_result/final_c2_c3_nc/BCV.tiff")
plotBCV(d)

de = exactTest(d, pair = c("TREAT","CONTROL"))
### differential expreesed gene
top = topTags(de)

deinfo = de$table
result = cbind(cpm(d)[rownames(de),],deinfo[rownames(de),])# ，cbind() 把矩阵横向合并成一个大矩阵（列方式），而rbind()是纵向合并（行方式）。在命令中> X <- cbind(arg 1 , arg 2 , arg 3 , ...) cbind() 的参数要么是任何长度的向量，要么是列长度一致的的矩阵（即行数一样）。结果将是一个合并arg1 , arg2 , . . . 的列形成的矩阵。
write.table(result,"D:/htseq_result/final_c2_c3_nc/DEG.txt",quote = F, col.names = T, row.names = T, sep = "\t")

summary(de.sig <- decideTestsDGE(de))

detags <- rownames(d)[as.logical(de.sig)]
result.sig = result[detags,]
write.table(result.sig,"D:/htseq_result/final_c2_c3_nc/p.05.txt",quote = F, col.names = T, row.names = T, sep = "\t")

tiff("D:/htseq_result/final_c2_c3_nc/Smear.tiff")
plotSmear(de,de.tags = detags)
abline(h =c(-1,1), col = "blue")

dev.off()

重新利用c2_nc,c3_nc,c2_c3做一遍，只需要改变target即可，不进行过滤的
要想得到那几个图，之前就要先定义好tiff等，然后开始plot,不进行过滤的
library(limma)
library("edgeR")
targets = readTargets("D:/htseq_result/c2_nc/targets.txt")  ##sample information 读取数据的方法不局限，只需要将计数过得reads数据以矩阵形式读入即可

d = readDGE(targets$file,group = targets$group)
colnames(d) = targets$description

d$samples$lib.size = colSums(d$counts)

d = calcNormFactors(d)

pdf("D:/htseq_result/c2_nc/Information.par.pdf")

plotMDS(d) #Multidimensional scaling plot of digital gene expression profiles

d = estimateCommonDisp(d)
d = estimateTagwiseDisp(d)
pdf("D:/htseq_result/c2_nc/MeanVar.pdf")
plotMeanVar(d, show.tagwise.vars = TRUE, NBline = TRUE)
pdf("D:/htseq_result/c2_nc/BCV.pdf")
plotBCV(d)

de = exactTest(d, pair = c("TREAT","CONTROL"))
### differential expreesed gene
top = topTags(de)

deinfo = de$table
result = cbind(cpm(d)[rownames(de),],deinfo[rownames(de),])# ，cbind() 把矩阵横向合并成一个大矩阵（列方式），而rbind()是纵向合并（行方式）。在命令中> X <- cbind(arg 1 , arg 2 , arg 3 , ...) cbind() 的参数要么是任何长度的向量，要么是列长度一致的的矩阵（即行数一样）。结果将是一个合并arg1 , arg2 , . . . 的列形成的矩阵。
write.table(result,"D:/htseq_result/c2_nc/DEG.txt",quote = F, col.names = T, row.names = T, sep = "\t")

summary(de.sig <- decideTestsDGE(de))

detags <- rownames(d)[as.logical(de.sig)]
result.sig = result[detags,]
write.table(result.sig,"D:/htseq_result/c2_nc/p.05.txt",quote = F, col.names = T, row.names = T, sep = "\t")

tiff("D:/htseq_result/c2_nc/Smear.tiff")
plotSmear(de,de.tags = detags)
abline(h =c(-1,1), col = "blue")

dev.off()

点保存工作空间就可以得到历史
在自己电脑上R运行的一些历史
source('http://www.bioconductor.org/biocLite.R')
biocLite('cummeRbund')
library('cummeRbund')
cuff_data <- readCufflinks('E:\??\sw_RNASWQ\SWC2')
csDensity(genes(cuff_data))
cuff_data <- readCufflinks('E:\??\sw_RNASWQ\SWC2')
cuff_data <- readCufflinks('E:\work\sw_RNASWQ\SWC2')
cuff_data <- readCufflinks('E:/work/sw_RNASWQ/SWC2')
csDensity(genes(cuff_data))
csScatter(genes(cuff_data),'C2','NC')
csScatter(genes(cuff_data))
?csScatter
?csScatter
csScatter(genes(cuff_data),'SWC2','SWNC')
cuff_data <- readCufflinks('E:/work/sw_RNASWQ/SWC2')
csScatter(genes(cuff_data),'SWC2','SWNC')
q()
library(cummeRbund)
cuff_data <- readCufflinks('E:/work/sw_RNASWQ/SWC2')
mygene <- getGene(cuff_data,'CTNNB1')
expressionBarplot(mygene)
expressionBarplot(mygene)
mygene <- getGene(cuff_data,'CTNNB1')
expressionBarplot(mygene)
expressionBarplot(mygene)
q()
library(cummRbund)
library(cummeRbund)
cuff_data <- readCufflinks('E:/work/sw_RNASWQ/SWC2')
mygene<- getGene(cuff_data,'CTNNB1')
expressionBarplot(mygene)
expressionBarplot(isoforms(mygene))
cuff_data <- readCufflinks('E:/work/sw_RNASWQ/SWM1')
csDensity(genes(cuff_data))
csDensity(genes(cuff_data))
csScatter(genes(cuff_data),'SWM1','SWNC')
csScatter(genes(cuff_data),'SWM1','SWNC')
q()
source("http://bioconductor.org/biocLite.R")
biocLite("edgeR")
library(edgeR)
library(limma)
library(edgeR)
??DEGlist
q()
library(limma)
library(edgeR)
targets = readTargets("D:/htseq_result/targets.txt")
d = readDGE(targets$file,group = targets$group)
d
colnames(d) = targets$description
colnames
colnames(d)
??rowSums
??cpm
keep = rowSums(cpm(d) > 1) >= 2
d = d[keep,]
d$samples$lib.size = colSums(d$counts)
d = calcNormFactors(d)
pdf("D:/htseq_result/Information.par.pdf")
plotMDS??
q()
plotMDS(d)
d = estimateCommonDisp(d)
d = estimateTagwiseDisp(d)
plotMeanVar(d, show.tagwise.vars = TRUE, NBline = TRUE)
plotBCV(d)
de = exactTest(d, pair = c("TREAT","CONTROL"))
top = topTags(de)
deinfo = de$table
result = cbind(cpm(d)[rownames(de),],deinfo[rownames(de),])
write.table(result,"D:/htseq_result/",quote = F, col.names = T, row.names = T, sep = "\t")
write.table(result,"D:/htseq_result/DEG.txt",quote = F, col.names = T, row.names = T, sep = "\t")
summary(de.sig <- decideTestsDGE(de))
detags <- rownames(d)[as.logical(de.sig)]
result.sig = result[detags,]
write.table(result.sig,"D:/htseq_result/p.05.txt",quote = F, col.names = T, row.names = T, sep = "\t")
plotSmear(de,de.tags = detags)
abline(h =c(-1,1), col = "blue")
dev.off()
save.image("C:\\Users\\dell\\Desktop\\edger.RData")

edgeR运行过程（C2_C3_NC）

library(limma)
library("edgeR")
targets = readTargets("D:/htseq_result/targets.txt")  ##sample information 读取数据的方法不局限，只需要将计数过得reads数据以矩阵形式读入即可

d = readDGE(targets$file,group = targets$group)
colnames(d) = targets$description

keep = rowSums(cpm(d) > 1) >= 2   #edgeR::cpm 		Counts per Million or Reads per Kilobase per Million,rowSums:Give Column Sums of a Matrix or Data Frame
d = d[keep,]
## remove tags with low abundance

d$samples$lib.size = colSums(d$counts)

d = calcNormFactors(d)

pdf("D:/htseq_result/Information.par.pdf")

plotMDS(d) #Multidimensional scaling plot of digital gene expression profiles

d = estimateCommonDisp(d)
d = estimateTagwiseDisp(d)

plotMeanVar(d, show.tagwise.vars = TRUE, NBline = TRUE)
plotBCV(d)

de = exactTest(d, pair = c("TREAT","CONTROL"))
### differential expreesed gene
top = topTags(de)

deinfo = de$table
result = cbind(cpm(d)[rownames(de),],deinfo[rownames(de),])# ，cbind() 把矩阵横向合并成一个大矩阵（列方式），而rbind()是纵向合并（行方式）。在命令中> X <- cbind(arg 1 , arg 2 , arg 3 , ...) cbind() 的参数要么是任何长度的向量，要么是列长度一致的的矩阵（即行数一样）。结果将是一个合并arg1 , arg2 , . . . 的列形成的矩阵。
write.table(result,"D:/htseq_result/DEG.txt",quote = F, col.names = T, row.names = T, sep = "\t")

summary(de.sig <- decideTestsDGE(de))

detags <- rownames(d)[as.logical(de.sig)]
result.sig = result[detags,]
write.table(result.sig,"D:/htseq_result/p.05.txt",quote = F, col.names = T, row.names = T, sep = "\t")


plotSmear(de,de.tags = detags)
abline(h =c(-1,1), col = "blue")

dev.off()


整个过程的详细说明
https://wikis.utexas.edu/display/bioiteam/Differential+gene+expression+analysis#Differentialgeneexpressionanalysis-Analyzedifferentialgeneexpression

HTseq运行
samtools sort -n accepted_hits.bam bamsorted
samtools view -help
samtools view -h bamsorted.bam > SWNC.sam
htseq-count --help
htseq-count -s no -t gene SWNC.sam /Work1/home/wangdong/ensemble/genes.gtf > SWNC_final.count

C2
samtools sort -n accepted_hits.bam bamsorted
samtools view -h bamsorted.bam > SWC2.sam
htseq-count -s no -t gene SWC2.sam /Work1/home/wangdong/ensemble/genes.gtf > SWC2_test.count

C3
samtools sort -n accepted_hits.bam bamsorted
samtools view -h bamsorted.bam > SWC3.sam
htseq-count -s no -t gene SWC3.sam /Work1/home/wangdong/ensemble/genes.gtf > SWC3.count
m1
samtools sort -n accepted_hits.bam bamsorted
samtools view -h bamsorted.bam > SWM1.sam
htseq-count -s no -t gene SWM1.sam /Work1/home/wangdong/ensemble/genes.gtf > SWM1.count
M2
samtools sort -n accepted_hits.bam bamsorted
samtools view -h bamsorted.bam > SWM2.sam
htseq-count -s no -t gene SWM2.sam /Work1/home/wangdong/ensemble/genes.gtf > SWM2.count

将SW*.count几个文件做一下merge

安装库是一件十分繁琐的事情,依赖关系的等十分的麻烦,前面的方法十分重要,但是希望有个整合的版本 那就是
(https://store.continuum.io/cshop/anaconda/) 这个版本整合了大量的常用库 ,下载 
直接bash Anaconda-2.1.0-Linux-x86_64.sh  按照提示,一步步安装即可 ,安装完成后基本库就都已经装好了 十分方便

python-leveldb的安装,从(https://pypi.python.org/pypi/leveldb)下载,但是用4.9.1的gcc无法编译通过,用系统自带的gcc能编译过,python setup.py  build ----- python setup.py  install

python-protobuf的安装,从我们前面已经装过的protobuf中,有个python的文件夹,python setup.py  build ----- python setup.py  install就可以了

python-gflags的安装,从(https://pypi.python.org/pypi/python-gflags)下载,解压python setup.py  build ----- python setup.py  install就可以了
library("limma")
library("edgeR")
targets = readTargets("F:/lss/work/20150113/WHL/whl/final_result/edger/targets.txt")  ##sample information

d = readDGE(targets$file,group = targets$group)
colnames(d) = targets$description

keep = rowSums(cpm(d) > 1) >= 2
d = d[keep,]
## remove tags with low abundance

d$samples$lib.size = colSums(d$counts)

d = calcNormFactors(d)

pdf("Information.par_d1_6.pdf")

plotMDS(d)

d = estimateCommonDisp(d)
d = estimateTagwiseDisp(d)
pdf("MeanVar_2.pdf")
plotMeanVar(d, show.tagwise.vars = TRUE, NBline = TRUE)
pdf("BCV.pdf")
plotBCV(d)

de = exactTest(d, pair = c("TREAT","CONTROL"))
### differential expreesed gene
top = topTags(de)

deinfo = de$table
result = cbind(cpm(d)[rownames(de),],deinfo[rownames(de),])
write.table(result,"F:/lss/work/20150113/WHL/whl/final_result/edger/DEG.txt",quote = F, col.names = T, row.names = T, sep = "\t")

summary(de.sig <- decideTestsDGE(de))

detags <- rownames(d)[as.logical(de.sig)]
result.sig = result[detags,]
write.table(result.sig,"F:/lss/work/20150113/WHL/whl/final_result/edger/p.05.txt",quote = F, col.names = T, row.names = T, sep = "\t")

pdf("smear.pdf")
plotSmear(de,de.tags = detags)
abline(h =c(-1,1), col = "blue")

dev.off()


[wangdong@cluster ~/lss/data2/Unaligned/Project_H9WJ0ADXX/R]$Rscript correlation1.R 

进入R环境。（句首'>'为interpreter prefix）
>args = commandArgs()
>print(args)
[1] "/share/apps/R/R-2.5.0/gnu/lib/R/bin/exec/R"

`commandArgs()`数返回一个数组，其中包含命令行上的参数信息17
`print()`函数打印出数组的所有元素。

也可以不进入交互式的R环境，直接创建R脚本然后执行。脚本中包含三行：
args = commandArgs()
print(args)
q()

bowtie2-build -f ref.fasta ref_whl
bowtie2 -N 3 -x ref_whl WHL_C1_R1_antisense.fastq -S C1_3.sam
fastx_trimmer -f 25 -l 46 -Q 33 -i WHL_C1_R1.fastq -o WHL_C1_R1_antisense.fastq
bedtools安装
git clone https://github.com/arq5x/bedtools2.git
mv bedtools2/ app/
 ls
cd app/
cd ce
cd bedtools2/
make
ls
cd bin/
ls
pwd
vi ~/.bashrc
ll
le ~/.bash
le ~/.bashrc
source ~/.bashrc
ll
bedtools

scp wangdong@10.10.0.22:/Share/home/wangdong/lss/data2/Unaligned/Project_H9WJ0ADXX/Sample_WHL_C1/C1_1.bam ./

There was another thread very recently which covered this. You need to add the -Q33 parameter to tell it that you're using Illumina encoded quality scores, not Sanger encoding. 

scp wangdong@166.111.30.164:/Work1/home/wangdong/lss/20141022/new/SampleSheet.csv ./


BclToFastq.pl --input-dir /Share/home/wangdong/lss/data/150106_C00126_0162_AH9WJ0ADXX/Data/Intensities/BaseCalls/ --output ./Unaligned --sample-sheet ./SampleSheet.csv --no-eamss --use-bases-mask Y101,I7n,Y101

values=set()
for key in z.keys():
    val = z[key]
    if val in values: 
        del z[key]
    else:
        values.add(val)

 setdefault(key[, default])

    If key is in the dictionary, return its value. If not, insert key with a value of default and return default. default defaults to None.

    如果键在字典中，返回这个键所对应的值。如果键不在字典中，向字典 中插入这个键，并且以default为这个键的值，并返回 default。default的默认值为None
    >>> dict={}
>>> dict['key']='a'
>>> dict
{'key': 'a'}
>>> dict.setdefault('key', 'b')  # 键key存在，故返回简直a.
'a'
>>> dict
{'key': 'a'}
>>> dict.setdefault('key0', 'b') # 键key0不存在，故插入此键，并以b为键值.
'b'
>>> dict
{'key0': 'b', 'key': 'a'}


configureBclToFastq.pl --input-dir /Work1/home/wangdong/lss/data/141214_C00126_0157_AHB59AADXX/Data/Intensities/BaseCalls/ --output ./Unaligned_whl_why_re --sample-sheet /Work1/home/wangdong/lss/data/141214_C00126_0157_AHB59AADXX/SampleSheet_141214.csv --no-eamss --use-bases-mask Y101,I7n,Y101
ls
cd cd Unaligned_whl_why/


string.atof(s)将字符串转为浮点型数字


问题：查找一些英文词在哪些小句中出现了，当然是用python来实现，当然是用字典，但是怎么让一个key对应一个 类型为列表的value，直接用列表的append()是不行的，比如dic[key].append(value)，因为解释器并不知道 dic[key]的类型，当时赶时间，用了一个折衷的方案，就是先用value连成一个str，最后用str.split()作一个转换，生成一个列表.

    看了python cookbook，上面正好有一个recipe讲到如何处理这样的问题，好了，揭晓答案吧!

(1)value中允许有重复项.

复制代码 代码如下:

dic = {}
dic.setdefault(key，[]).append(value)
#如:
d1.setdefault('bob_hu'，[]).append(1)
d1.setdefault('bob_hu'，[]).append(2)
print d1['bob_hu'] # [1，2]

(2)value中无重复项.

复制代码 代码如下:

dic = {}
dic.setdefault(key，{})[value] = 1
#如:
d1.setdefault('bob'，{})['f'] = 1
d1.setdefault('bob'，{})['h'] = 1
d1.setdefault('bob'，{})['f'] = 1
print d1['bob'] #{'h': 1， 'f': 1}

您可能感兴趣的文章:

    Python获取文件ssdeep值的方法
    跟老齐学Python之赋值，简单也不简单
    shell脚本中执行python脚本并接收其返回值的例子
    Python使用函数默认值实现函数静态变量的方法
    Python采用raw_input读取输入值的方法
    python实现k均值算法示例(k均值聚类算法)
    python函数返回多个值的示例方法
    python读取注册表中值的方法
    python赋值操作方法分享
    python局部赋值的规则
    python函数缺省值与引用学习笔记分享
    python求crc32值的方法

QQ空间 新浪微博 腾讯微博 搜狐微博 人人网 开心网 百度搜藏 更多 0
Tags：字典 值是列表
复制链接收藏本文打印本文关闭本文返回首页
上一篇：python实现的各种排序算法代码
下一篇：python实现多线程暴力破解登陆路由器功能代码分享
相关文章

    2014-10-10Python入门篇之正则表达式
    2008-09-09Python 命令行参数sys.argv
    2014-02-02python使用7z解压软件备份文件脚本分享
    2013-03-03Python 用户登录验证的小例子
    2014-10-10Python数据结构之Array用法实例
    2014-03-03python文件和目录操作方法大全（含实例）
    2012-09-09python代码检查工具pylint 让你的python更规范
    2007-02-02Python入门
    2014-02-02urllib2自定义opener详解
    2013-11-11python聊天程序实例代码分享

文章评论

社交帐号登录:

    微博
    QQ
    人人
    豆瓣
    更多?

最新最早最热

    评论

    还没有评论，沙发等你来抢

脚本之家正在使用多说
友情提醒：本站文件的解压密码：www.jb51.net (请使用最新的winrar)
最 近 更 新

    Python操作Mysql实例代码教程在线版(查询
    Python基于twisted实现简单的web服务器
    python字符串排序方法
    Python入门篇之列表和元组
    Python实现的一个简单LRU cache
    python实现socket客户端和服务端简单示例
    python中使用urllib2获取http请求状态码的
    python进程类subprocess的一些操作方法例
    深入分析在Python模块顶层运行的代码引起
    python BeautifulSoup使用方法详解

热 点 排 行

    Python入门教程 超详细1小时学会
    python 中文乱码问题深入分析
    比较详细Python正则表达式操作指
    Python字符串的encode与decode研
    Python open读写文件实现脚本
    python 字符串split的用法分享
    Python enumerate遍历数组示例应
    Python 深入理解yield
    python 文件和肪恫僮骱数小17
    Python+Django在windows下的开发

Js与CSS工具

    CSS在线压缩格式化(中文)
    css 格式化整理工具(英文)
    CSS整形格式化
    JavaScript 格式化整理工具
    jsbeautifier Js格式化整理工具(英文)
    php 格式化整理工具(英文)
    HTML/JS互相转换工具
    javascript pack加密压缩工具
    JS Minifier压缩
    JS混淆工具
    在线JS脚本校验器错误
    JavaScript 正则表达式在线测试工具

代码转换工具

    Base64编码加密
    Escape加解密
    HTML/UBB代码转换
    GB2312/BIG5繁简字转换
    经典小工具集 数字转换
    HTML多功能代码转换器
    迅雷快车加/解密
    汉字转换拼音

字典 z 如下

z = {
    1: 'a',
    2: 'a',
    3: 'a',
    4: 'b',
    5: 'c',
    6: 'c',
    7: 'd'
}

要求删除重复的 value 的 item，只保留一个，如何实现？
values=set()
for key in z.keys():
    val = z[key]
    if val in values: 
        del z[key]
    else:
        values.add(val)
ls
nohup make -j 8


plot(data$exp1,data$exp2,lty=2,pch=20,cex=0.5,main="exp1_exp2",xlab="log10(exp1)",ylab="log10(exp2)")

configureBclToFastq.pl --input-dir /Work1/home/wangdong/lss/data/141214_C00126_0157_AHB59AADXX/Data/Intensities/BaseCalls/ --output ./Unaligned_whl_why --sample-sheet /Work1/home/wangdong/lss/data/141214_C00126_0157_AHB59AADXX/SampleSheet.csv --no-eamss --use-bases-mask Y101,I7n,Y101
ls
cd cd Unaligned_whl_why/
ls
nohup make -j 8


Linux系统cp:omitting directory`XXX'问题解决
 
在linux系统中复制文件夹时提示如下：
Shell代码  
cp: omitting directory `foldera/'  
其中foldera是我要复制的文件夹名，出现该警告的原因是因为foldera目录下还存在目录，所以不能直接拷贝。
   www.2cto.com  
解决办法：
使用递归拷贝，在cp命令后面加上-r参数，形如：
Shell代码  
[root@localhost opt]# cp -r foldera folderc  
这里的-r代表递归的意思。
 
同样，当我们在linux系统下删除目录时也需要加上-r参数 ，如果目录为空，则会直接删除，如果目录非空，则会级联删除。不过在级联删除时也会有一个问题就是如果目录下存在很多的文件或者子目录，系统会一个一个进行提示。如果想一步删除不用提示的话可以使用rm -rf命令。f是force的意思，代表强制删除，无提示！

epigetics缺BAP18探针

genomeCoverageBed -split -bg -ibam accepted_hits.sorted.bam -g dm3.chrom.sizes > accepted_hits.bedgraphwigToBigWig accepted_hits.bedgraph dm3.chrom.sizes myfile.bw
genomeCoverageBed -split -bg -i 123.bed -g /Work1/home/wangdong/hg19/ChromInfo.txt > SWC2_accepted_hits_sorted_change.bedgraph
genomeCoverageBed -split -bg -i 123.bed -g /Work1/home/wangdong/hg19/ChromInfo.txt > SWC2_accepted_hits_sorted_change_split2.bedgraph
遇到一个.tgz文件，可不知道如何解压缩
实就是tar zxvf就好了的，tgz跟tar.gz是一样的。gunzip -cd| taz -xvf还要通过重定向来中转，相对来说又麻烦了一点，而且命令也长了。
scp wangdong@166.111.30.164:/Share/home/wangdong/apps/src/Python-2.7.7.tgz ./ C集群上目前传东西
tar -xzvf Python-2.7.7.tgz

scp wangdong@166.111.30.164:/Share/home/wangdong/apps/src/RSeQC-2.5.tar.gz ./
tar -xvzf RSeQC-2.5.tar.gz
cd RSeQC-2.5
python setup.py install(说python版本要2.7）

没法看，估计是bed的chr1等没有chr，利用
perl -e 'open IN,"SWM2_accepted_hits_sorted.bed";open OUT,">> 123.bed";while (<IN>) {chomp;@a=split /\t/;print OUT"chr$a[0]\t$a[1]\t$a[2]\t$a[3]\t$a[4]\t$a[5]\n";}'
加入chr
重新转化为bed_graph

$genomeCoverageBed -bg -i 123.bed -g /Work1/home/wangdong/hg19/ChromInfo.txt > SWM2_accepted_hits_sorted_change.bedgraph
bedGraphToBigWig SWM2_accepted_hits_sorted_change.bedgraph /Work1/home/wangdong/hg19/ChromInfo.txt SWM2_change.bigwig

bedtools是source李洋的
在http://hgdownload.cse.ucsc.edu/admin/exe/linux.x86_64/下面下载bedGraphToBigWig，传到/Share/home/wangdong/apps/bin下
chmod 777 bedGraphToBigWig
将/Share/home/wangdong/apps/bin加入bash_rc, source
直接enter之便可以用了
bedGraphToBigWig
bedGraphToBigWig v 4 - Convert a bedGraph file to bigWig format.
usage:
   bedGraphToBigWig in.bedGraph chrom.sizes out.bw
where in.bedGraph is a four column file in the format:
      <chrom> <start> <end> <value>
and chrom.sizes is two column: <chromosome name> <size in bases>
and out.bw is the output indexed big wig file.
Use the script: fetchChromSizes to obtain the actual chrom.sizes information
from UCSC, please do not make up a chrom sizes from your own information.
The input bedGraph file must be sorted, use the unix sort command:
  sort -k1,1 -k2,2n unsorted.bedGraph > sorted.bedGraph
options:
   -blockSize=N - Number of items to bundle in r-tree.  Default 256
   -itemsPerSlot=N - Number of data points bundled at lowest level. Default 1024
   -unc - If set, do not use compression.

c2_c3_nc操作过程
 1036  perl -e 'open IN,"SWM2_accepted_hits_sorted.bed";open OUT,">> 123.bed";while (<IN>) {chomp;@a=split /\t/;print OUT"chr$a[0]\t$a[1]\t$a[2]\t$a[3]\t$a[4]\t$a[5]\n";}'
 1037  ll
 1038  le 123.bed 
 1039  ll
 1040  ll
 1041  ll
 1042  ls
 1043  cd ..
 1044  ls
 1045  cd SWC2_1
 1046  ls
 1047  ll
 1048  bamToBed -i accepted_hits.bam >> SWC2_accepted_hits.bed
 1049  perl -e 'open IN,"SWC2_accepted_hits.bed";open OUT,">> 123.bed";while (<IN>) {chomp;@a=split /\t/;print OUT"chr$a[0]\t$a[1]\t$a[2]\t$a[3]\t$a[4]\t$a[5]\n";}'
 1050  genomeCoverageBed -bg -i 123.bed -g /Work1/home/wangdong/hg19/ChromInfo.txt > SWC2_accepted_hits_sorted_change.bedgraph
 1051  bedGraphToBigWig SWC2_accepted_hits_sorted_change.bedgraph /Work1/home/wangdong/hg19/ChromInfo.txt SWC2_change.bigwig
 1052  ll
 1053  cd ..
 1054  ls
 1055  cd SWC3_1
 1056  LS
 1057  ls
 1058  bamToBed -i accepted_hits.bam >> SWC3_accepted_hits.bed
 1059  perl -e 'open IN,"SWC3_accepted_hits.bed";open OUT,">> 123.bed";while (<IN>) {chomp;@a=split /\t/;print OUT"chr$a[0]\t$a[1]\t$a[2]\t$a[3]\t$a[4]\t$a[5]\n";}'
 1060  genomeCoverageBed -bg -i 123.bed -g /Work1/home/wangdong/hg19/ChromInfo.txt > SWC3_accepted_hits_sorted_change.bedgraph
 1061  bedGraphToBigWig SWC3_accepted_hits_sorted_change.bedgraph /Work1/home/wangdong/hg19/ChromInfo.txt SWC3_change.bigwig
 1062  cd ..
 1063  ls
 1064  cd SWNC_1/
 1065  ls
 1066  ll
 1067  bamToBed -i accepted_hits.bam >> SWNC_accepted_hits.bed
 1068  perl -e 'open IN,"SWNC_accepted_hits.bed";open OUT,">> 123.bed";while (<IN>) {chomp;@a=split /\t/;print OUT"chr$a[0]\t$a[1]\t$a[2]\t$a[3]\t$a[4]\t$a[5]\n";}'
 1069  genomeCoverageBed -bg -i 123.bed -g /Work1/home/wangdong/hg19/ChromInfo.txt > SWNC_accepted_hits_sorted_change.bedgraph
 1070  bedGraphToBigWig SWNC_accepted_hits_sorted_change.bedgraph /Work1/home/wangdong/hg19/ChromInfo.txt SWNC.bigwig
bamtobed
bamToBed -i accepted_hits.bam >> SWM2_accepted_hits.bed
le SWM2_accepted_hits.bed
sort -k 1,1 SWM2_accepted_hits.bed > SWM2_accepted_hits_sorted.bed #-k 1,1(start pos__end_pos)
genomeCoverageBed
曾输入为bam，错误信息比较多(genomeCoverageBed -bg -ibam wgEncodeCshlLongRnaSeq/wgEncodeCshlLongRnaSeqK562ChromatinTotalAlnRep4.bam.sort -g hg19.chromInfo -strand + >K562-Chromatin-POS-4.bedgraph)
Input error: Chromosome 19 found in non-sequential lines. This suggests that the input file is not sorted correc
所以改输入为bed,经过上面的转化，排序，然后genomeCoverageBed进行bed转化为bedgraph
genomeCoverageBed -bg -i SWM2_accepted_hits_sorted.bed -g /Work1/home/wangdong/ensemble/ChromInfo.txt > SWM2_accepted_hits_sorted.bedgraph #那个染色体信息必须要与自己的bed一致，如果是chr1，就都chr1(/Work1/home/wangdong/hg19/ChromInfo.txt)
但现在的bed头为1，所以用这个
$genomeCoverageBed -bg -i SWM2_accepted_hits_sorted.bed -g /Work1/home/wangdong/ensemble/ChromInfo.txt > SWM2_accepted_hits_sorted.bedgraph 
$le SWM2_accepted_hits_sorted.bedgraph 
1       10017   10041   1
1       10041   10068   2
1       10068   10092   1
1       11593   11644   2
1       11696   11708   1
1       11708   11747   2
1       11747   11759   1
1       11761   11805   1
1       11805   11812   2
1       11812   11854   1
1       11854   11856   4
1       11856   11904   3
1       11904   11905   4
1       11905   11929   1
1       11929   11955   2

可成功进行转化
bedGraphToBigWig SWM2_accepted_hits_sorted.bedgraph /Work1/home/wangdong/ensemble/ChromInfo.txt SWM2_accepted_hits_sorted.bed_graph.bigwig
b
ig_wig二进制文件
"SWM2_accepted_hits_sorted.bed_graph.bigwig" may be a binary file

流程大致为
bamToBed -i accepted_hits.bam >> SWM2_accepted_hits.bed然后sort -k 1,1 SWM2_accepted_hits.bed > SWM2_accepted_hits_sorted.bed然后 genomeCoverageBed -bg -i SWM2_accepted_hits_sorted.bed -g /Work1/home/wangdong/ensemble/ChromInfo.txt > SWM2_accepted_hits_sorted.bedgraph然后bedGraphToBigWig SWM2_accepted_hits_sorted.bedgraph /Work1/home/wangdong/ensemble/ChromInfo.txt SWM2_accepted_hits_sorted.bed_graph.bigwig，成功的到了bigwig，可是我的bigwig放在ucsc上啥也没有呀，



被人的流程如下（上面的有学下面的）
Data Processing
BAM sort

samtools sort wgEncodeCshlLongRnaSeq/wgEncodeCshlLongRnaSeqK562ChromatinTotalAlnRep4.bam wgEncodeCshlLongRnaSeq/wgEncodeCshlLongRnaSeqK562ChromatinTotalAlnRep4.bam.sort
Genome Coverage

I refer to the standard manual of BEDtools, I'll use forward strand as example, and the reverse strand signal is generated in the same way.

genomeCoverageBed -bg -ibam wgEncodeCshlLongRnaSeq/wgEncodeCshlLongRnaSeqK562ChromatinTotalAlnRep4.bam.sort -g hg19.chromInfo -strand + >K562-Chromatin-POS-4.bedgraph

Note that I've used -strand flag to separate the two strands.
bedgraphtoBigWig

bedGraphToBigWig executive script available from UCSC exe list （http://hgdownload.cse.ucsc.edu/admin/exe/linux.x86_64/）各种软件均有

bedGraphToBigWig K562-Chromatin-POS-4.bedgraph hg19.chromInfo K562-Chromatin-POS-4.bigwig

20141209可以直接用 configureBclToFastq
configureBclToFastq.pl --input-dir /Work1/home/wangdong/lss/20141021_bcl/141021_C00126_0147_AHA4C4ADXX/Data/Intensities/BaseCalls/ --output ./Unaligned_test2 --sample-sheet /Work1/home/wangdong/lss/20141021_bcl/SampleSheet_141021.csv --no-eamss --use-bases-mask Y51,I6n,Y51
ls
cd Unaligned_test2
ls
nohup make -j 8


GO/KEGG富集分析enrichment analysis
2014年01月26日 ? Bioinformatics ? 字号 小 中 大 ? 评论 1 条 ? 阅读 1,384 次 [点击加入在线收藏夹]

本教程使用GOstats来对给定的基因symbols做富集分析。

> library("org.Hs.eg.db")
> library("GSEABase")
> library("GOstats")
> genes <- c("AREG", "FKBP5", "CXCL13", "KLF9", "ZC3H12A", "P4HA1", "TLE1", "CREB3L2", "TXNIP", "PBX1", "GJA1", "ITGB8", "CCL3", "CCND2", "KCNJ15", "CFLAR", "CXCL10", "CYSLTR1", "IGFBP7", "RHOB", "MAP3K5", "CAV2", "CAPN2", "AKAP13", "RND3", "IL6ST", "RGS1", "IRF4", "G3BP1", "SEL1L", "VEGFA", "SMAD1", "CCND1", "CLEC3B", "NEB", "AMD1", "PDCD4", "SCD", "TM2D3", "BACH2", "LDLR", "BMPR1B", "RFXAP", "ASPH", "PTK2B", "SLC1A5", "ENO2", "TRPM8", "SATB1", "MIER1", "SRSF1", "ATF3", "CCL5", "MCM6", "GCH1", "CAV1", "SLC20A1")
> ##GO BP enrichment analysis
> goAnn <- get("org.Hs.egGO")
> universe <- Lkeys(goAnn)
> head(universe)
[1] "1"         "10"        "100"       "1000"      "10000"     "100008586"
> entrezIDs <- mget(genes, org.Hs.egSYMBOL2EG, ifnotfound=NA)
> head(entrezIDs)
$AREG
[1] "374"
 
$FKBP5
[1] "2289"
 
$CXCL13
[1] "10563"
 
$KLF9
[1] "687"
 
$ZC3H12A
[1] "80149"
 
$P4HA1
[1] "5033"
 
> entrezIDs <- as.character(entrezIDs)
> head(entrezIDs)
[1] "374"   "2289"  "10563" "687"   "80149" "5033" 
> params <- new("GOHyperGParams",
+                  geneIds=entrezIDs,
+                  universeGeneIds=universe,
+                  annotation="org.Hs.eg.db",
+                  ontology="BP",
+                  pvalueCutoff=0.01,
+                  conditional=FALSE,
+                  testDirection="over")
> over <- hyperGTest(params)
> library(Category)
> glist <- geneIdsByCategory(over)
> glist <- sapply(glist, function(.ids) {
+ 	.sym <- mget(.ids, envir=org.Hs.egSYMBOL, ifnotfound=NA)
+ 	.sym[is.na(.sym)] <- .ids[is.na(.sym)]
+ 	paste(.sym, collapse=";")
+ 	})
> head(glist)
                                                                                                                                                                                         GO:0002690 
                                                                                                                                                              "PTK2B;CXCL10;CCL3;CCL5;VEGFA;CXCL13" 
                                                                                                                                                                                         GO:0002688 
                                                                                                                                                              "PTK2B;CXCL10;CCL3;CCL5;VEGFA;CXCL13" 
                                                                                                                                                                                         GO:0048518 
"RHOB;ASPH;ATF3;CCND1;BMPR1B;CAV1;CAV2;CCND2;PTK2B;GJA1;IL6ST;CXCL10;IRF4;LDLR;SMAD1;MAP3K5;PBX1;RFXAP;CCL3;CCL5;SLC20A1;TLE1;CLEC3B;VEGFA;CFLAR;CXCL13;TXNIP;CYSLTR1;AKAP13;MIER1;CREB3L2;ZC3H12A" 
                                                                                                                                                                                         GO:1901623 
                                                                                                                                                                           "PTK2B;CCL3;CCL5;CXCL13" 
                                                                                                                                                                                         GO:0002687 
                                                                                                                                                              "PTK2B;CXCL10;CCL3;CCL5;VEGFA;CXCL13" 
                                                                                                                                                                                         GO:2000403 
                                                                                                                                                                           "PTK2B;CCL3;CCL5;CXCL13" 
> bp <- summary(over)
> bp$Symbols <- glist[as.character(bp$GOBPID)]
> head(bp)
      GOBPID       Pvalue OddsRatio    ExpCount Count Size                                        Term
1 GO:0002690 3.490023e-08  39.24490  0.19267044     6   52 positive regulation of leukocyte chemotaxis
2 GO:0002688 9.278865e-08  32.80297  0.22601725     6   61          regulation of leukocyte chemotaxis
3 GO:0048518 1.170957e-07   4.28014 13.56103476    32 3660   positive regulation of biological process
4 GO:1901623 1.176400e-07 128.80174  0.04816761     4   13         regulation of lymphocyte chemotaxis
5 GO:0002687 1.496978e-07  30.05918  0.24454325     6   66  positive regulation of leukocyte migration
6 GO:2000403 2.969857e-07  96.58170  0.05928321     4   16 positive regulation of lymphocyte migration
                                                                                                                                                                                            Symbols
1                                                                                                                                                               PTK2B;CXCL10;CCL3;CCL5;VEGFA;CXCL13
2                                                                                                                                                               PTK2B;CXCL10;CCL3;CCL5;VEGFA;CXCL13
3 RHOB;ASPH;ATF3;CCND1;BMPR1B;CAV1;CAV2;CCND2;PTK2B;GJA1;IL6ST;CXCL10;IRF4;LDLR;SMAD1;MAP3K5;PBX1;RFXAP;CCL3;CCL5;SLC20A1;TLE1;CLEC3B;VEGFA;CFLAR;CXCL13;TXNIP;CYSLTR1;AKAP13;MIER1;CREB3L2;ZC3H12A
4                                                                                                                                                                            PTK2B;CCL3;CCL5;CXCL13
5                                                                                                                                                               PTK2B;CXCL10;CCL3;CCL5;VEGFA;CXCL13
6                                                                                                                                                                            PTK2B;CCL3;CCL5;CXCL13
> dim(bp)
[1] 525   8
> ##KEGG enrichment analysis
> keggAnn <- get("org.Hs.egPATH")
> universe <- Lkeys(keggAnn)
> params <- new("KEGGHyperGParams", 
+                     geneIds=entrezIDs, 
+                     universeGeneIds=universe, 
+                     annotation="org.Hs.eg.db", 
+                     categoryName="KEGG", 
+                     pvalueCutoff=0.01,
+                     testDirection="over")
> over <- hyperGTest(params)
> kegg <- summary(over)
> library(Category)
> glist <- geneIdsByCategory(over)
> glist <- sapply(glist, function(.ids) {
+ 	.sym <- mget(.ids, envir=org.Hs.egSYMBOL, ifnotfound=NA)
+ 	.sym[is.na(.sym)] <- .ids[is.na(.sym)]
+ 	paste(.sym, collapse=";")
+ 	})
> kegg$Symbols <- glist[as.character(kegg$KEGGID)]
> kegg
  KEGGID       Pvalue OddsRatio ExpCount Count Size                                   Term                                    Symbols
1  04510 9.643801e-05  7.873256 1.124361     7  200                         Focal adhesion    CCND1;CAPN2;CAV1;CAV2;CCND2;ITGB8;VEGFA
2  04060 5.487123e-04  5.821855 1.489779     7  265 Cytokine-cytokine receptor interaction BMPR1B;IL6ST;CXCL10;CCL3;CCL5;VEGFA;CXCL13
3  04062 3.739699e-03  5.486219 1.062521     5  189            Chemokine signaling pathway              PTK2B;CXCL10;CCL3;CCL5;CXCL13
> library("pathview")
> gIds <- mget(genes, org.Hs.egSYMBOL2EG, ifnotfound=NA)
> gEns <- unlist(gIds)
> gene.data <- rep(1, length(gEns))
> names(gene.data) <- gEns
> for(i in 1:3){pv.out <- pathview(gene.data, pathway.id=as.character(kegg$KEGGID)[i], species="hsa"


无root权限解决lib××× not found，以及Linux上设置良好的目录结构
本文帮助你解决在无root权限的linux系统上安装软件时候遇到的lib××× not found的问题，并推荐一个Linux上良好的目录结构。

1. 安装软件到自己的软件目录。

缺乏的lib×××库大多都能在网上下载到源码，可自己下载安装。

下图是我在服务器上的目录结构，软件都安装在～/local/app里面，在~/local/bin里面分别建立软链接指向所安装软件的可执行文件；如果该软件里面的可执行文件太多，方便的做法是讲其所在目录加入到环境变量$PATH中。
2. 设置环境变量

有的软件安装后只生成可执行文件，有的则产生一些库文件和头文件，则需要将其添加到相应环境变量中；share目录等可忽略。如下所示
# 可执行文件 export PATH=$HOME/local/app/bin:$PATH # 静态链接库 export LIBRARY_PATH=$HOME/local/app/libevent/lib:$LIBRARY_PATH # 动态链接库 export LD_LIBRARY_PATH=$HOME/local/app/libevent/lib:$LD_LIBRARY_PATH # gcc头文件 export C_INCLUDE_PATH=$HOME/local/app/libevent/include:$C_INCLUDE_PATH # g++头文件 export CPLUS_INCLUDE_PATH=$HOME/local/app/libevent/include:$CPLUS_INCLUDE_PATH
	
# 可执行文件
export PATH=$HOME/local/app/bin:$PATH
# 静态链接库
export LIBRARY_PATH=$HOME/local/app/libevent/lib:$LIBRARY_PATH
# 动态链接库
export LD_LIBRARY_PATH=$HOME/local/app/libevent/lib:$LD_LIBRARY_PATH
# gcc头文件
export C_INCLUDE_PATH=$HOME/local/app/libevent/include:$C_INCLUDE_PATH
# g++头文件
export CPLUS_INCLUDE_PATH=$HOME/local/app/libevent/include:$CPLUS_INCLUDE_PATH

注意：等号前面不要有空格。本例中，～/local/app/libevent/lib中包含了动态和静态链接库，不确定编译器类型，故加入到gcc和g++头文件搜索目录中。

怎样改变使UltraEdit有多个窗口出来
视图’――‘视图/列表’――‘打开文件标签


解决fastqc不能运行的问题
重新下载安装jdk,并且将安装位置local/java加入~/.bashrc
还是报找不到libjli.so的错，于是将新安装的/Share/home/wangdong/apps/src/jdk1.8.0_25/lib/i386/jli下面的这个库加到~/.bash_profile里
发现报找不到libjava.so，于是将自己安装的/Share/home/wangdong/apps/src/jdk1.8.0_25/jre/lib/i386的库，以及locate libjli.so libjava.s的所得信息全部一并加入
~/.bash_profile，上面同时也报Error: Could not find Java SE Runtime Environment，这是因为没有把安装的那个bin加入~/.bashrc
（java runtime environment 就是JRE，要么就是你没装JDK（java development kit），JDK包含了JRE，装完JDK就不要装JRE了，也有可能你没配置环境变量到JDK安装目录下的bin）
所以将/Share/home/wangdong/apps/src/jdk1.8.0_25/bin:\加入~/.bashrc
并且要加载x
 /Share/home/wangdong/local/java:\前面
 因为jdk是环境，后者是执行文件
LD_LIBRARY_PATH=/Share/home/wangdong/apps/src/jdk1.8.0_25/bin:\
/Share/home/wangdong/apps/src/jdk1.8.0_25/lib/i386/jli:\
/Share/home/wangdong/apps/src/jdk1.8.0_25/jre/lib/i386:\
/home/ibm/lsf8.3_lsfinstall/lap/jre/ibm-java-x86_64-60/jre/lib/amd64/jli:\
/opt/lsf83/8.3/install/lap/jre/ibm-java-x86_64-60/jre/lib/amd64/jli:\
/opt/pac83-new/jre/linux-x86_64/lib/amd64/jli:\
/usr/java/jre1.7.0_25/lib/amd64/jli:\
/usr/lib/jvm/java-1.6.0-openjdk-1.6.0.0.x86_64/jre/lib/amd64/jli:\
/home/ibm/lsf8.3_lsfinstall/lap/jre/ibm-java-x86_64-60/jre/lib/amd64:\
/opt/lsf83/8.3/install/lap/jre/ibm-java-x86_64-60/jre/lib/amd64:\
/opt/pac83-new/jre/linux-x86_64/lib/amd64:\
/usr/java/jre1.7.0_25/lib/amd64:\
/usr/lib/jvm/java-1.6.0-openjdk-1.6.0.0.x86_64/jre/lib/amd64
export LD_LIBRARY_PATH

最后输入java就可以发现好使了，然后运行fastqc，一切OK
/Share/home/wangdong/apps/src/jdk1.8.0_25/bin:\
$fastqc -o ./SW_24_R2 CTTGTA.R1.fastq

.bash_profile和.bashrc的什么区别
/etc/profile:此文件为系统的每个用户设置环境信息,当用户第一次登录时,该文件被执行.
并从/etc/profile.d目录的配置文件中搜集shell的设置.
/etc/bashrc:为每一个运行bash shell的用户执行此文件.当bash shell被打开时,该文件被读取.
~/.bash_profile:每个用户都可使用该文件输入专用于自己使用的shell信息,当用户登录时,该
文件仅仅执行一次!默认情况下,他设置一些环境变量,执行用户的.bashrc文件.
~/.bashrc:该文件包含专用于你的bash shell的bash信息,当登录时以及每次打开新的shell时,该
该文件被读取.
~/.bash_logout:当每次退出系统(退出bash shell)时,执行该文件. 

另外,/etc/profile中设定的变量(全局)的可以作用于任何用户,而~/.bashrc等中设定的变量(局部)只能继承/etc/profile中的变量,他们是"父子"关系.
 
~/.bash_profile 是交互式、login 方式进入 bash 运行的
~/.bashrc 是交互式 non-login 方式进入 bash 运行的
通常二者设置大致相同，所以通常前者会调用后者。 
我们在ubuntu图形界面下用eclipse写了一个动态库，到centos下调用时出现错误，   

error while loading shared libraries: libmysqlclientso.so.0: cannot open shared object file: No such file or directory

以为没装mysql-client，因为ubuntu下叫这个，但是centos下直接就叫mysql，服务器版本叫mysql-server，查找了一下libmysqlclient.so

find / -name libmysqlclient.so，果然发现不同：

这是因为没有把动态链接库的安装路径（例如说是 /usr/local/lib ）放到变量 LD_LIBRARY_PATH 里。

这时，可以用命令 export 来临时测试确认是不是这个问题：

export LD_LIBRARY_PATH=/usr/local/lib

在终端里运行上面这行命令，再运行这个可执行文件，如果运行正常就说明是这个问题。

接下来的问题是：以上做法，只是临时设置变量 LD_LIBRARY_PATH ，下次开机，一切设置将不复存在；如何把这个值持续写到 LD_LIBRARY_PATH 里呢？

我们可以在 ~/.bashrc 或者 ~/.bash_profile 中加入 export 语句，前者在每次登陆和每次打开 shell 都读取一次，后者只在登陆时读取一次。我的习惯是加到 ~/.bashrc 中，在该文件的未尾，可采用如下语句来使设置生效：

export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/lib

修改完后，记得关掉当前终端并重新打开一个新的终端，从而使上面的配置生效。



关于函数库和软件安装的相关问题请教
Hi Shasha,

  Yeah, we also have this kind of issue in our server, generally to solve this problem you need to add the path of your software to the PATH environment variable.
For example, suppose your FastQC is in $HOME/software/FastQC
Then, in the $HOME/.bashrc file add the line:
export PATH=$HOME/software/FastQC:$PATH

For the libraries (xx.so) you need to add it to the LD_LIBRARY_PATH environment variable

export LD_LIBRARY_PATH=$HOME/software/FastQC/lib:LD_LIBRARY_PATH

For me generally I install all the software in $HOME/.local

when installing a software you can configure it to be on the .local file, so it will put all the bin and library files there.

./configure --prefix=$HOME/.local

for python packages, you can install them using

python setup.py install --user


Then in the $HOME/.bashrc file I add the following lines

export PATH=$HOME/.local/bin:$HOME
export LD_LIBRARY_PATH=$HOME/.local/lib:$LD_LIBRARY_PATH

Hope it helps,
You are welcome any time :),

Regards,

-Nadhir
通过一下命令进行library路径设置


另外，如果不想每次新启一个shell都设置LD_LIBRARY_PATH，可以编辑~/.bash_profile文件：

$ vi ~/.bash_profile 

添加：

LD_LIBRARY_PATH=/usr/local/lib

export LD_LIBRARY_PATH

这两行，完成之后.bash_profile如下所示：

 

# .bash_profile

# Get the aliases and functions

if [ -f ~/.bashrc ]; then

        . ~/.bashrc

fi

# User specific environment and startup programs

PATH=$PATH:$HOME/bin

LD_LIBRARY_PATH=/usr/local/lib

export PATH

export LD_LIBRARY_PATH
然后运行
$ source ~/.bash_profile 就行了。


vi ~/.bash_profile 

探针放的顺序：d在a后面

运行fastqc,报下面的错误
java：error while loading shared libraries: libjli.so: cannot open shared object file: No such file or directory 
export LD_LIBRARY_PATH=/usr/lib/jvm/java-1.6.0-openjdk-1.6.0.0.x86_64/jre/lib/amd64/jli:$LD_LIBRARY_PATH
source /etc/profile
然后java可以出现了，但是还是会报下面的错
同时fastqc无法enter出来，所以需要source一下~/.bashrc
java: error while loading shared libraries: libjli.so: wrong ELF class: ELFCLASS64
网上显示是系统位数不对，可能改为32为就好了

LD_LIBRARY_PATH=/home/ibm/lsf8.3_lsfinstall/lap/jre/ibm-java-x86_64-60/jre/lib/amd64/jli
/Share/home/wangdong/local/java

ln -s /Share/home/wangdong/local/java/javac /usr/bin/javac
ln -s /Share/home/wangdong/local/java/jar /usr/bin/jar

ln -s /Share/home/wangdong/local/java/javac /usr/bin/java

整个过程的详细说明
https://wikis.utexas.edu/display/bioiteam/Differential+gene+expression+analysis#Differentialgeneexpressionanalysis-Analyzedifferentialgeneexpression

HTseq运行
samtools sort -n accepted_hits.bam bamsorted
samtools view -help
samtools view -h bamsorted.bam > SWNC.sam
htseq-count --help
htseq-count -s no SWNC.sam /Work1/home/wangdong/ensemble/genes.gtf > SWNC_final.count

C2
samtools sort -n accepted_hits.bam bamsorted
samtools view -h bamsorted.bam > SWC2.sam
htseq-count -s no SWC2.sam /Work1/home/wangdong/ensemble/genes.gtf > SWC2.count

C3
samtools sort -n accepted_hits.bam bamsorted
samtools view -h bamsorted.bam > SWC3.sam
htseq-count -s no SWC3.sam /Work1/home/wangdong/ensemble/genes.gtf > SWC3.count
m1
samtools sort -n accepted_hits.bam bamsorted
samtools view -h bamsorted.bam > SWM1.sam
htseq-count -s no SWM1.sam /Work1/home/wangdong/ensemble/genes.gtf > SWM1_2.count
M2
samtools sort -n accepted_hits.bam bamsorted
samtools view -h bamsorted.bam > SWM2.sam
htseq-count -s no SWM2.sam /Work1/home/wangdong/ensemble/genes.gtf > SWM2.count

将SW*.count几个文件做一下merge

安装库是一件十分繁琐的事情,依赖关系的等十分的麻烦,前面的方法十分重要,但是希望有个整合的版本 那就是
(https://store.continuum.io/cshop/anaconda/) 这个版本整合了大量的常用库 ,下载 
直接bash Anaconda-2.1.0-Linux-x86_64.sh  按照提示,一步步安装即可 ,安装完成后基本库就都已经装好了 十分方便

python-leveldb的安装,从(https://pypi.python.org/pypi/leveldb)下载,但是用4.9.1的gcc无法编译通过,用系统自带的gcc能编译过,python setup.py  build ----- python setup.py  install

python-protobuf的安装,从我们前面已经装过的protobuf中,有个python的文件夹,python setup.py  build ----- python setup.py  install就可以了

python-gflags的安装,从(https://pypi.python.org/pypi/python-gflags)下载,解压python setup.py  build ----- python setup.py  install就可以了

HTseq安装
一下为李洋的运行历史
  927  ./configure --help
  928  R/
  929  R
  930  ls
  931  cd src/
  932  ls
  933  ll
  934  cd
  935  ls
  936  cd apps/
  937  ls
  938  cd src/
  939  ls
  940  ls
  941  ll
  942  chmod 755 Anaconda-2.1.0-Linux-x86_64.sh 
  943  bash Anaconda-2.1.0-Linux-x86_64.sh 
  944  cd
  945  ls
  946  cd apps/
  947  pwd
  948  cd src/
  949  ls
  950  bash Anaconda-2.1.0-Linux-x86_64.sh 
  951  cd
  952  ls
  953  vi .bashrc
  954  which python
  955  vi .bashrc
  956  source .bashrc
  957  which python
  958  vi .bashrc
  959  source .bashrc
  960  which python
  961  python
  962  vi .bash_profile 
  963  vi .bashrc
  964  echo $PATH
  965  vi .bashrc
  966  source .bashrc
  967  echo $PATH
  968  which python
  969  cd
  970  ls
  971  cd 
  972  ls
  973  vi .bashrc
  974  /usr/bin/vim .bashrc
  975  source .bashrc
  976  echo $PATH
  977  which python

先下载Anaconda-2.1.0-Linux-x86_64.sh
 chmod 755 Anaconda-2.1.0-Linux-x86_64.sh 
bash Anaconda-2.1.0-Linux-x86_64.sh进行编译
yes选择安装
yes选择将路径加入PATH里

既可以自动导入一些库，不用重新安装，
此时which python就应该在~/anaconda/bin/python下
此时可以进行HTSEQ安装了
解压，cd 
 python setup.py build
 python setup.py install
 将/Share/home/wangdong/local/app/HTSeq-0.6.1/build/scripts-2.6:\加入bashrc中即可
 htseq-count就有了
然后直接利用python setup.py build进行build，然后python setup.py install进行安装即可
然后python
import HTSeq证明安装好了
一下为李洋的运行过程

/Share/home/wangdong/apps/anaconda/bin/python
 cd apps/
 1011  ls
 1012  cd src/
 1013  ls
 1014  cd HTSeq-0.6.1
 1015  LS
 1016  LS
 1017  ls
 1018  python setup.py build
 1019  python setup.py install
 1020  cd
 1021  ls
 1022  python
 1023  history

gFOLD安装
同济GFOLD软件是一款根据mapping结果直接进行差异表达分析的软件，http://www.tongji.edu.cn/~zhanglab/GFOLD/index.html，文献中提到，该软件在分析无重复的转录组数据的时候，不以p值为计算依据，而是以GFOLD值作为标准筛选差异表达基因，命令也比较简单。国产软件还是支持一下。
结果要优于其他DESeq、edgeR、cuffldiff等软件，故在此决定安装该软件。
在安装该软件之前需要先安装GSL的一款基于GNU的数值计算工具（个人这么认为，不管怎么样要安装）。
这是GSL的官方介绍http://www.gnu.org/software/gsl/
首先下载下来 这个比较简单我就不说了 wget命令
tar解压
cd 进入目录
进去以后参考INSTALL中说明，里面会把遇到的一些常见问题列出方便解决。但是基本步骤就是
>./confiure
>make
>make check
>make install
一般来说会把库文件 和lib文件安装在usr/local下的include和lib下 ，记住这两个目录方便后面使用。
上面步骤一般来说不会报错，所以就到了下一步。安装GFOLD软件

从上述网站下载GFOLD的安装包，同样wget
解压，进入目录，可以先打开READEME看一下，需要改环境变量，没错这里就蛋疼了，可以的话推荐手动改，
linux系统直接改~/.bashrc文件
在里面加上这两句
export CXXFLAGS="-g -O3 -I/usr/local/include -L/~/usr/local/lib";
export LD_LIBRARY_PATH="~/usr/local/lib:"$LD_LIBRARY_PATH
注意-I 和-L后面没有空格，不知道同济的开发者怎么想的 我试了半天，才搞定。一般来说这里改完就ok了。
然后就是编译：make
很喜人，这里没有报错，gfold文件终于生成了。
但是这里结束了么，我错，问题没有ok。
经常装软件的人一般都会装好习惯下的测试运行一下，你猜对了，我测试运行出错了哦。

./gfold: error while loading shared libraries: libgsl.so.0: cannot open shared object file: No such file or directory

这是报错的信息，我表示不解，我明明都装好，路径也设对了，但是这个就是找不到。
于是google神器爆发找到了答案，原因是在GSL装好以后可能会出现共享lib找不到的情况，这个是系统造成的，解决方法是手动的重新更改系统变量 设置LD_LIBRARY_PATH。

> LD_LIBRARY_PATH=/usr/local/lib
> export LD_LIBRARY_PATH

 /Work1/home/wangdong/lje/cufflinks-2.1.1.Linux_x86_64/cuffmerge -g /Work1/home/wangdong/ensemble/genes.gtf -s /Work1/home/wangdong/ensemble/genome.fa -p 4 /Work1/home/wangdong/lss/20141021/Undetermined_indices/sw4/RNA_seq/again/assemblies.txt
 library(ggplot2)
  4 library(scales)
  5 library(reshape2)
  6 library(hgu95av2.db)
  7 pdf(file="pri_finalres.pdf", height=15, width=20)
  8 data<-read.csv("./c2_nc_heat.csv",head=T)
  9 data<-data.matrix(data)
 10 hc<-hclust(dist(data))
 11 rowInd<-hc$order
 12 hc<-hclust(dist(t(data)))
 13 colInd<-hc$order
 14 data.m<-data[rowInd,]
 15 data.m<-apply(data.m,1,rescale)
 16 data.m<-t(data.m)
 17 coln<-colnames(data.m)
 18 rown<-rownames(data.m)
 19 colnames(data.m)<-1:ncol(data.m)
 20 rownames(data.m)<-1:nrow(data.m)
 21 data.m<-melt(data.m)
 22 head(data.m)
 23 base_size<-12
 24 (p <- ggplot(data.m, aes(Var2,Var1)) + geom_tile(aes(fill = value),colour = "white") + scale_fill_gradient(low = "green", high = "red")) + theme_grey(base_size = base_size)+ labs(x = "",y = "") + s    cale_x_continuous(expand = c(0,0),labels=coln,breaks=1:length(coln)) + scale_y_continuous(expand = c(0, 0),labels=rown,breaks=1:length(rown)) + theme(axis.ticks = element_blank(), axis.text.x = ele    ment_text(size = base_size *0.6, angle = 90, hjust = 0, colour = "grey50"),  axis.text.y = element_text(size = base_size * 0.6, hjust=1, colour="grey50"))
 25 #pdf(file="finalresult", height=40, width=50)
 26 dev.off()

我现在的任务都是qw状态，能查找这种状态出现的原因吗？？
【活跃】gyfeng-葛云峰(285570784) 2014-11-20 10:09:59
qstat -j +作业ID

> mydata <- read.csv("s1_s2_3.csv")
> tiff("s1_s2_3.tiff")
> plot(mydata$splint0.1,mydata$splint0.2,lty=2,pch=20,cex=0.5,xlab="log10(splint0.1_normalization)",ylab="log10(splint0.2_normalization)")
> dev.off()
null device 
          1 
> 

> mydata <- read.csv("c2_c3.csv")
> tiff("c2_c3_edit.tiff")
> plot(mydata$ctnnb1_shRNA1_rpkm,mydata$ctnnb1_shRNA2_rpkm,lty=2,pch=20,cex=0.5,xlab="log10(ctnnb1_shRNA1_rpkm)",ylab="log10(ctnnb1_shRNA2_rpkm)")
> dev.off()
null device 
          1 

m1_m2
> mydata <- read.csv("m1_m2_com.csv")
> tiff("m1_m2_final.tiff")
> plot(mydata$c_myc_shRNA1,mydata$c_myc_shRNA2,lty=2,pch=20,cex=0.5,xlab="log10(c_myc_shRNA1_rpkm)",ylab="log10(c_myc_shRNA2_rpkm)")
> dev.off()
null device 
          1 
> q() 

C2_NC
> mydata <- read.csv("C2_NC.csv")
> tiff("C2_nc_final.tiff")
> plot(mydata$CTNNB1_shRNA1_RPKM,mydata$NC_RPKM,lty=2,pch=20,cex=0.5,xlab="log10(ctnnb1_shRNA1_rpkm)",ylab="log10(NC_rpkm)")
> dev.off()
null device 
          1 
> q()

C3_NC
> mydata <- read.csv("C3_NC.csv")
> tiff("C3_NC_final.tiff")
> plot(mydata$CTNNB1_shRNA2_RPKM,mydata$NC_RPKM,lty=2,pch=20,cex=0.5,xlab="log10(ctnnb1_shRNA2_rpkm)",ylab="log10(NC_rpkm)")
> dev.off()
null device 
          1 
M1_NC
 > mydata <- read.csv("m1_NC.csv")
> tiff("m1_nc_final.tiff")
> plot(mydata$myc_shRNA1_RPKM,mydata$NC_RPKM,lty=2,pch=20,cex=0.5,xlab="log10(c_myc_shRNA1_rpkm)",ylab="log10(NC_rpkm)") 
> dev.off()
null device 
          1 

M2_NC
plot(mydata$myc_shRNA2_RPKM,mydata$NC_RPKM,lty=2,pch=20,cex=0.5,xlab="log10(c_myc_shRNA2_rpkm)",ylab="log10(NC_rpkm)")    
任务提交bjobs -l
[wangdong@mgt /Work1/home/wangdong/lss/20141021/Undetermined_indices/sw4/RNA_seq/cuffdiff/com]$bjobs -l

Job <484207>, Job Name <cuffdiff_test>, User <wangdong>, Project <default>, Sta
                     tus <RUN>, Queue <TINY>, Command <#!/bin/sh;#BSUB -J cuffd
                     iff_test;#BSUB -o cuffdiff_test.o%J;#BSUB -e cuffdiff_test
                     .e%J;#BSUB -n 4;#BSUB -R "span[hosts=1]"; # Environment;so
                     urce ./source.txt; # JOBS;echo -ne "Start cuffdiff_test at
                      ... ";date; /Share/home/wangdong/local/bin/cuffdiff -o /W
                     ork1/home/wangdong/lss/20141021/Undetermined_indices/sw4/R
                     NA_seq/cuffdiff/c1_c2/cuff_diff2 -b /Work1/home/wangdong/e
                     nsemble/genome.fa -L SWC2,SWC3 /Work1/home/wangdong/lss/20
                     141021/Undetermined_indices/sw4/RNA_seq/cuffdiff/c1_c2/mer
                     ged_asm/merged.gtf /Work1/home/wangdong/lss/20141021/Undet
                     ermined_indices/sw4/RNA_seq/SWC2_1/accepted_hits.bam /Work
                     1/home/wangdong/lss/20141021/Undetermined_indices/sw4/RNA_
                     seq/SWC3_1/accepted_hits.bam;  echo -ne "End cuffdiff_test
                      at ";date>
Tue Nov 18 15:03:32: Submitted from host <mgt>, CWD </Share/home/wangdong/pipel
                     ine/RNA_finder/bin>, Output File <cuffdiff_test.o%J>, Erro
                     r File <cuffdiff_test.e%J>, 4 Processors Requested, Reques
                     ted Resources <span[hosts=1]>;
Tue Nov 18 15:03:34: Started on 4 Hosts/Processors <4*node04>, Execution Home <
                     /Share/home/wangdong/>, Execution CWD </Share/home/wangdon
                     g/pipeline/RNA_finder/bin>;
Wed Nov 19 15:41:20: Resource usage collected.
                     The CPU time used is 88632 seconds.
                     MEM: 3 Gbytes;  SWAP: 3 Gbytes;  NTHREAD: 6
                     PGID: 11149;  PIDs: 11149 11150 11154 11157 


 SCHEDULING PARAMETERS:
           r15s   r1m  r15m   ut      pg    io   ls    it    tmp    swp    mem
 loadSched   -     -     -     -       -     -    -     -     -      -      -  
 loadStop    -     -     -     -       -     -    -     -     -      -      -  

          adapter_windows     poe nrt_windows 
 loadSched             -       -           -  
 loadStop              -       -           -
 
将两个样品放在一起做cuffmerge，保证用同一个注释标准
/Share/home/wangdong/local/bin/cuffmerge -g /Work1/home/wangdong/ensemble/genes.gtf -s /Work1/home/wangdong/ensemble/genome.fa -p 4 /Work1/home/wangdong/lss/20141021/Undetermined_indices/sw4/RNA_seq/cuffdiff/c1_c2/assemblies.txt
/Share/home/wangdong/local/bin/cuffmerge -g /Work1/home/wangdong/ensemble/genes.gtf -s /Work1/home/wangdong/ensemble/genome.fa -p 4 /Work1/home/wangdong/lss/20141021/Undetermined_indices/sw4/RNA_seq/cuffdiff/m1_m2/assemblies.txt
重新做一个cuff_diff
/Share/home/wangdong/local/bin/cuffdiff -o /Work1/home/wangdong/lss/20141021/Undetermined_indices/sw4/RNA_seq/cuffdiff/c1_c2/cuff_diff -b /Work1/home/wangdong/ensemble/genome.fa -L SWC2,SWC3 /Work1/home/wangdong/lss/20141021/Undetermined_indices/sw4/RNA_seq/cuffdiff/c1_c2/merged_asm/merged.gtf /Work1/home/wangdong/lss/20141021/Undetermined_indices/sw4/RNA_seq/SWC2_1/accepted_hits.bam /Work1/home/wangdong/lss/20141021/Undetermined_indices/sw4/RNA_seq/SWC3_1/accepted_hits.bam
/Share/home/wangdong/local/bin/cuffdiff -o /Work1/home/wangdong/lss/20141021/Undetermined_indices/sw4/RNA_seq/cuffdiff/m1_m2/cuff_diff -b /Work1/home/wangdong/ensemble/genome.fa -L SWM1,SWM2 /Work1/home/wangdong/lss/20141021/Undetermined_indices/sw4/RNA_seq/cuffdiff/m1_m2/merged_asm/merged.gtf /Work1/home/wangdong/lss/20141021/Undetermined_indices/sw4/RNA_seq/SWM1_1/accepted_hits.bam /Work1/home/wangdong/lss/20141021/Undetermined_indices/sw4/RNA_seq/SWM2_1/accepted_hits.bam
 biocLite(c("cummeRbund"))
for i in /Share/home/wangdong/.Work1/lss/20141021/Undetermined_indices/sw4/RNA_seq/SW*/accepted_hits.bam; do echo $i; samtools index $i ; done;
for i in /Share/home/wangdong/.Work1/lss/20141021/Undetermined_indices/sw4/RNA_seq/SW*/accepted_hits.bam; do echo $i; samtools idxstats $i ; done;
R下载网址
http://cran.r-project.org/bin/windows/base/old/
更新
biocLite("BiocUpgrade")
http://bioconductor.org/packages/3.0/bioc/src/contrib/GenomicAlignments_1.2.1.tar.gz'
'http://bioconductor.org/packages/3.0/bioc/src/contrib/Rsamtools_1.18.1.tar.gz'
 'http://bioconductor.org/packages/3.0/bioc/src/contrib/VariantAnnotation_1.12.3.tar.gz'
 
trying URL 'http://cran.fhcrc.org/src/contrib/RCurl_1.95-4.3.tar.gz'
'http://bioconductor.org/packages/3.0/bioc/src/contrib/Rsamtools_1.18.1.tar.gz'
 'http://bioconductor.org/packages/3.0/bioc/src/contrib/GenomicAlignments_1.2.1.tar.gz'
 'http://bioconductor.org/packages/3.0/bioc/src/contrib/biomaRt_2.22.0.tar.gz'
 'http://bioconductor.org/packages/3.0/bioc/src/contrib/GenomicFeatures_1.18.2.tar.gz'
  'http://bioconductor.org/packages/3.0/bioc/src/contrib/BSgenome_1.34.0.tar.gz'
   'http://bioconductor.org/packages/3.0/bioc/src/contrib/biovizBase_1.14.0.tar.gz'
   trying URL 'http://bioconductor.org/packages/3.0/bioc/src/contrib/rtracklayer_1.26.1.tar.gz'
Content type 'application/x-gzip' length 1338692 bytes (1.3 Mb)
opened URL
==================================================
downloaded 1.3 Mb

trying URL 'http://bioconductor.org/packages/3.0/bioc/src/contrib/Gviz_1.10.2.tar.gz'
Content type 'application/x-gzip' length 2550604 bytes (2.4 Mb)
trying URL 'http://bioconductor.org/packages/3.0/bioc/src/contrib/cummeRbund_2.8.2.tar.gz'
Content type 'application/x-gzip' length 2313860 bytes (2.2 Mb)

关于解压
.tar.gz的，用tar zxvf
.tar.bz2的，用tar jxvf
.tar的，用tar xvf
 filename.gz
 gzip -d filename.gz （或gunzip filename.gz） 解压，但要注意，用此命令解压，会删除原文件。（filename换成相应文件名）

如果想保留原文件，用
zcat filename.gz > filename

abi的sra数据转化为fastq
fastq-dump得到数值
solidstandard的一个perl程序可以将数字转化为fastq
BCL2FASTQ报错
[wangdong@mgt /Work1/home/wangdong/lss/20141022/whl_sw]$sh 7_index.sh
[2014-10-23 00:49:42]   [configureBclToFastq.pl]        INFO: Creating directory '/Work1/home/wangdong/lss/20141022/whl_sw/Unaligned'
could not find ParserDetails.ini in /usr/lib/perl5/site_perl/5.8.8/XML/SAX
[2014-10-23 00:49:43]   [configureBclToFastq.pl]        INFO: Basecalling software: RTA
[2014-10-23 00:49:43]   [configureBclToFastq.pl]        INFO:              version: 1.18 (build 61)
[2014-10-23 00:49:43]   [configureBclToFastq.pl]        INFO: Original use-bases mask: Y51,I7n,Y51
[2014-10-23 00:49:43]   [configureBclToFastq.pl]        INFO: Guessed use-bases mask: YYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYY,IIIIIIIn,YYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYY
[2014-10-23 00:49:43]   [configureBclToFastq.pl]        WARNING: SampleSheet flowcell ID 'H9RYYADXX' does not match. Expected: 'HA4C4ADXX'
Creating directory '/Work1/home/wangdong/lss/20141022/whl_sw/Unaligned/Project_H9RYYADXX/Sample_B4'
Creating directory '/Work1/home/wangdong/lss/20141022/whl_sw/Unaligned/Project_H9RYYADXX/Sample_C1'
Creating directory '/Work1/home/wangdong/lss/20141022/whl_sw/Unaligned/Project_H9RYYADXX/Sample_C2'
Creating directory '/Work1/home/wangdong/lss/20141022/whl_sw/Unaligned/Project_H9RYYADXX/Sample_B1'
Creating directory '/Work1/home/wangdong/lss/20141022/whl_sw/Unaligned/Project_H9RYYADXX/Sample_C3'
Creating directory '/Work1/home/wangdong/lss/20141022/whl_sw/Unaligned/Project_H9RYYADXX/Sample_B8'
Creating directory '/Work1/home/wangdong/lss/20141022/whl_sw/Unaligned/Project_H9RYYADXX/Sample_SWS2'
Creating directory '/Work1/home/wangdong/lss/20141022/whl_sw/Unaligned/Project_H9RYYADXX/Sample_B2'
Creating directory '/Work1/home/wangdong/lss/20141022/whl_sw/Unaligned/Project_H9RYYADXX/Sample_SWT4'
Creating directory '/Work1/home/wangdong/lss/20141022/whl_sw/Unaligned/Project_H9RYYADXX/Sample_SWS1'
Creating directory '/Work1/home/wangdong/lss/20141022/whl_sw/Unaligned/Project_H9RYYADXX/Sample_B3'
Creating directory '/Work1/home/wangdong/lss/20141022/whl_sw/Unaligned/Project_H9RYYADXX/Sample_C6'
Creating directory '/Work1/home/wangdong/lss/20141022/whl_sw/Unaligned/Undetermined_indices/Sample_lane1'
Creating directory '/Work1/home/wangdong/lss/20141022/whl_sw/Unaligned/Project_H9RYYADXX/Sample_B5'
Creating directory '/Work1/home/wangdong/lss/20141022/whl_sw/Unaligned/Project_H9RYYADXX/Sample_B6'
Creating directory '/Work1/home/wangdong/lss/20141022/whl_sw/Unaligned/Project_H9RYYADXX/Sample_B7'
Creating directory '/Work1/home/wangdong/lss/20141022/whl_sw/Unaligned/Project_H9RYYADXX/Sample_C4'
Creating directory '/Work1/home/wangdong/lss/20141022/whl_sw/Unaligned/Project_H9RYYADXX/Sample_C5'
Creating directory '/Work1/home/wangdong/lss/20141022/whl_sw/Unaligned/Undetermined_indices/Sample_lane2'
[2014-10-23 00:49:43]   [configureBclToFastq.pl]        INFO: Read 1: length = 51: mask = YYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYY
[2014-10-23 00:49:43]   [configureBclToFastq.pl]        WARNING: use-base-mask length is 8 for read 2: expected length from config file is 7
[2014-10-23 00:49:43]   [configureBclToFastq.pl]        INFO: Read 2: length = 8: mask = IIIIIIIn
[2014-10-23 00:49:43]   [configureBclToFastq.pl]        WARNING: Read 3: current cycle in mask is 60: first cycle in read is 59
[2014-10-23 00:49:43]   [configureBclToFastq.pl]        INFO: Read 3: length = 51: mask = YYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYY
[2014-10-23 00:49:44]   [configureBclToFastq.pl]        WARNING: use-base-mask length is 8 for read 2: expected length from config file is 7
[2014-10-23 00:49:44]   [configureBclToFastq.pl]        WARNING: Read 3: current cycle in mask is 60: first cycle in read is 59
[2014-10-23 00:49:44]   [configureBclToFastq.pl]        INFO: Running self tests: 'make self_test'
[2014-10-23 00:49:46]   [configureBclToFastq.pl]        WARNING:
output of 'make self_test':

mkdir -p Basecall_Stats_HA4C4ADXX || ( sleep 5 && mkdir -p Basecall_Stats_HA4C4ADXX ); touch Basecall_Stats_HA4C4ADXX/.sentinel
[2014-10-23 00:49:46]   [mgt]   [Basecall_Stats_HA4C4ADXX/.sentinel]    make Target:    Basecall_Stats_HA4C4ADXX/.sentinel
[2014-10-23 00:49:46]   [mgt]   [Basecall_Stats_HA4C4ADXX/.sentinel]    make Reason:
[2014-10-23 00:49:46]   [mgt]   [Basecall_Stats_HA4C4ADXX/.sentinel]    make Prereqs:
[2014-10-23 00:49:46]   [mgt]   [Basecall_Stats_HA4C4ADXX/.sentinel]    make Cmd:       mkdir -p Basecall_Stats_HA4C4ADXX || ( sleep 5 && mkdir -p Basecall_Stats_HA4C4ADXX ); touch Basecall_Stats_HA4C4ADXX/.sentinel
/Share/home/lulab1/users/liyang/apps/bcl2fastq-1.8.4/libexec/bcl2fastq-1.8.4/BaseCalls/copyConfig.pl --input-file /Work1/home/wangdong/lss/20141022/141021_C00126_0147_AHA4C4ADXX/Data/Intensities/BaseCalls/config.xml --tiles "" > Basecall_Stats_HA4C4ADXX/config.xml.tmp && mv Basecall_Stats_HA4C4ADXX/config.xml.tmp Basecall_Stats_HA4C4ADXX/config.xml
[2014-10-23 00:49:46]   [mgt]   [Basecall_Stats_HA4C4ADXX/config.xml]   make Target:    Basecall_Stats_HA4C4ADXX/config.xml
[2014-10-23 00:49:46]   [mgt]   [Basecall_Stats_HA4C4ADXX/config.xml]   make Reason:    /Work1/home/wangdong/lss/20141022/141021_C00126_0147_AHA4C4ADXX/Data/Intensities/BaseCalls/config.xml Basecall_Stats_HA4C4ADXX/.sentinel
[2014-10-23 00:49:46]   [mgt]   [Basecall_Stats_HA4C4ADXX/config.xml]   make Prereqs:   /Work1/home/wangdong/lss/20141022/141021_C00126_0147_AHA4C4ADXX/Data/Intensities/BaseCalls/config.xml Basecall_Stats_HA4C4ADXX/.sentinel
[2014-10-23 00:49:46]   [mgt]   [Basecall_Stats_HA4C4ADXX/config.xml]   make Cmd:       /Share/home/lulab1/users/liyang/apps/bcl2fastq-1.8.4/libexec/bcl2fastq-1.8.4/BaseCalls/copyConfig.pl --input-file /Work1/home/wangdong/lss/20141022/141021_C00126_0147_AHA4C4ADXX/Data/Intensities/BaseCalls/config.xml --tiles "" > Basecall_Stats_HA4C4ADXX/config.xml.tmp && mv Basecall_Stats_HA4C4ADXX/config.xml.tmp Basecall_Stats_HA4C4ADXX/config.xml
[2014-10-23 00:49:46]   [mgt]   [Basecall_Stats_HA4C4ADXX/config.xml]   could not find ParserDetails.ini in /usr/lib/perl5/site_perl/5.8.8/XML/SAX
xsltproc \
          --stringparam DEMUX_READS_PARAM '1 2' \
          --stringparam READ_DEMUX_FIRST_CYCLES_PARAM '1 52' \
          --stringparam READ_DEMUX_LAST_CYCLES_PARAM '51 102' \
          /Share/home/lulab1/users/liyang/apps/bcl2fastq-1.8.4/share/bcl2fastq-1.8.4/ExcludeReadFromBustardConfig.xsl Basecall_Stats_HA4C4ADXX/config.xml > DemultiplexedBustardConfig.xml.tmp && mv DemultiplexedBustardConfig.xml.tmp DemultiplexedBustardConfig.xml
[2014-10-23 00:49:46]   [mgt]   [DemultiplexedBustardConfig.xml]        make Target:    DemultiplexedBustardConfig.xml
[2014-10-23 00:49:46]   [mgt]   [DemultiplexedBustardConfig.xml]        make Reason:    Basecall_Stats_HA4C4ADXX/config.xml
[2014-10-23 00:49:46]   [mgt]   [DemultiplexedBustardConfig.xml]        make Prereqs:   Basecall_Stats_HA4C4ADXX/config.xml
[2014-10-23 00:49:46]   [mgt]   [DemultiplexedBustardConfig.xml]        make Cmd:       xsltproc  --stringparam DEMUX_READS_PARAM '1 2'  --stringparam READ_DEMUX_FIRST_CYCLES_PARAM '1 52'  --stringparam READ_DEMUX_LAST_CYCLES_PARAM '51 102'  /Share/home/lulab1/users/liyang/apps/bcl2fastq-1.8.4/share/bcl2fastq-1.8.4/ExcludeReadFromBustardConfig.xsl Basecall_Stats_HA4C4ADXX/config.xml > DemultiplexedBustardConfig.xml.tmp && mv DemultiplexedBustardConfig.xml.tmp DemultiplexedBustardConfig.xml
[2014-10-23 00:49:46]   [mgt]   [DemultiplexedBustardConfig.xml]        /bin/bash: line 4:  2391 Segmentation fault      xsltproc --stringparam DEMUX_READS_PARAM '1 2' --stringparam READ_DEMUX_FIRST_CYCLES_PARAM '1 52' --stringparam READ_DEMUX_LAST_CYCLES_PARAM '51 102' /Share/home/lulab1/users/liyang/apps/bcl2fastq-1.8.4/share/bcl2fastq-1.8.4/ExcludeReadFromBustardConfig.xsl Basecall_Stats_HA4C4ADXX/config.xml > DemultiplexedBustardConfig.xml.tmp
make: *** [DemultiplexedBustardConfig.xml] Error 139


Self test command exited with error 2 (signal 0)

Investigate and fix errors, then retry
[2014-10-23 00:49:46]   [configureBclToFastq.pl]        Self test command exited with error 2 (signal 0)
[2014-10-23 00:49:46]   [configureBclToFastq.pl]        BACKTRACE:  at /Share/home/lulab1/users/liyang/apps/bcl2fastq-1.8.4/lib/bcl2fastq-1.8.4/perl/Casava/Demultiplex.pm line 871.
        Casava::Demultiplex::selfTest(Casava::Demultiplex=HASH(0x2037a6d0), "/Work1/home/wangdong/lss/20141022/whl_sw/Unaligned") called at /Share/home/lulab1/users/liyang/apps/bcl2fastq-1.8.4/bin/configureBclToFastq.pl line 438
Died at /Share/home/lulab1/users/liyang/apps/bcl2fastq-1.8.4/lib/bcl2fastq-1.8.4/perl/Casava/Common/Log.pm line 310.
: command not found
nohup: appending output to `nohup.out'
应该是该包放的地方路径不对

/usr/lib/perl5/vendor_perl/5.8.8/XML/SAX/ParserDetails.ini
/usr/lib/perl5/site_perl/5.8.8/XML/SAX
export PKG_CONFIG_PATH=/Share/home/wangdong/lib/lib/pkgconfig:$PKG_CONFIG_PATH
$echo $PATH (打印输出)
将/usr/lib/perl5/vendor_perl/5.8.8/XML/SAX加入PATH后无这个错，但是下面的错还在


/opt/mysql/bin:/opt/lsf83/8.3/linux2.6-glibc2.3-x86_64/etc:/opt/lsf83/8.3/linux2.6-glibc2.3-x86_64/bin:/opt/xcat/bin:/opt/xcat/sbin:/usr/kerberos/sbin:/usr/kerberos/bin:/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin:/root/bin:/usr/lpp/mmfs/bin:/usr/lpp/mmfs/bin:/Share/home/wangdong/apps/sratoolkit.2.3.5-2-centos_linux64/bin:/Share/home/wangdong/apps/samtools-0.1.17:/Share/home/wangdong/local/bin:/Work1/home/wangdong/lje/cufflinks-2.1.1.Linux_x86_64:/Work1/home/wangdong/lje/cutadapt-1.3/bin:/Share/home/wangdong/pipeline/RNA_finder/blast/ncbi-blast-2.2.29+/bin:/Work1/home/wangdong/tool_package/bzip2-1.0.6:/Share/home/wangdong/apps/src:/Share/home/wangdong/.Work1/lje/bowtie2-2.1.0:/Share/home/wangdong/local/java:/Share/home/wangdong/apps/src/FastQC:/Share/home/wangdong/local/lib:/usr/local/bin:/Share/home/wangdong/apps/src/ncbi-blast-2.2.29+/bin:/share/home/wangdong/apps/src/libgtextutils-0.1:/Share/home/wangdong/apps/bin:/etc/apache/bin

20141102 邵伟RNA_SEQ分析
/Share/home/wangdong/local/bin/tophat -p 4 -o /Work1/home/wangdong/lss/20141021/Undetermined_indices/sw4/RNA_seq/SWC2_1 /Work1/home/wangdong/lss/mm9/index /Work1/home/wangdong/lss/20141021/Undetermined_indices/sw4/SWC2_1_t.fastq;
/Share/home/wangdong/local/bin/tophat -p 4 -o /Work1/home/wangdong/lss/20141021/Undetermined_indices/sw4/RNA_seq/SWC3_1 /Work1/home/wangdong/lss/mm9/index /Work1/home/wangdong/lss/20141021/Undetermined_indices/sw4/SWC3_1_t.fastq;
/Share/home/wangdong/local/bin/tophat -p 4 -o /Work1/home/wangdong/lss/20141021/Undetermined_indices/sw4/RNA_seq/SWM1_1 /Work1/home/wangdong/lss/mm9/index /Work1/home/wangdong/lss/20141021/Undetermined_indices/sw4/SWM1_1_t.fastq;
/Share/home/wangdong/local/bin/tophat -p 4 -o /Work1/home/wangdong/lss/20141021/Undetermined_indices/sw4/RNA_seq/SWM2_1 /Work1/home/wangdong/lss/mm9/index /Work1/home/wangdong/lss/20141021/Undetermined_indices/sw4/SWM2_1_t.fastq;
/Share/home/wangdong/local/bin/tophat -p 4 -o /Work1/home/wangdong/lss/20141021/Undetermined_indices/sw4/RNA_seq/SWNC_1 /Work1/home/wangdong/lss/mm9/index /Work1/home/wangdong/lss/20141021/Undetermined_indices/sw4/SWNC_1_t.fastq;
报错1
[2014-11-02 20:41:52] Beginning TopHat run (v2.0.13)
-----------------------------------------------
[2014-11-02 20:41:52] Checking for Bowtie
/Work1/home/wangdong/lje/bowtie2-2.1.0/bowtie2-align: /usr/lib64/libstdc++.so.6: version `GLIBCXX_3.4.11' not found (required by /Work1/home/wangdong/lje/bowtie2-2.1.0/bowtie2-align)
/Work1/home/wangdong/lje/bowtie2-2.1.0/bowtie2-align: /usr/lib64/libstdc++.so.6: version `GLIBCXX_3.4.9' not found (required by /Work1/home/wangdong/lje/bowtie2-2.1.0/bowtie2-align)
Died at /Work1/home/wangdong/lje/bowtie2-2.1.0/bowtie2 line 75.
Traceback (most recent call last):
  File "/Share/home/wangdong/local/bin/tophat", line 4088, in ?
    sys.exit(main())
  File "/Share/home/wangdong/local/bin/tophat", line 3885, in main
    check_bowtie(params)
  File "/Share/home/wangdong/local/bin/tophat", line 1512, in check_bowtie
    bowtie_version = get_bowtie_version()
  File "/Share/home/wangdong/local/bin/tophat", line 1385, in get_bowtie_version
    bowtie_out = stdout_value.splitlines()[0]
IndexError: list index out of range
重新安装bowtie2.2.3.0解决了(估计均由于函数库更新导致)

报错2
[2014-11-02 21:43:11] Beginning TopHat run (v2.0.13)                                                                                                            
-----------------------------------------------                                                                                                                 
[2014-11-02 21:43:11] Checking for Bowtie                                                                                                                       
                  Bowtie version:        2.2.3.0                                                                                                                
[2014-11-02 21:43:11] Checking for Bowtie index files (genome)..                                                                                                
[2014-11-02 21:43:11] Checking for reference FASTA file                                                                                                         
[2014-11-02 21:43:11] Generating SAM header for /Work1/home/wangdong/lss/mm9/index                                                                              
[2014-11-02 21:43:23] Preparing reads                                                                                                                           
        [FAILED]                                                                                                                                                 
Error running 'prep_reads'                                                                                                                                      
/Share/home/wangdong/local/bin/prep_reads: /usr/lib64/libstdc++.so.6: version `GLIBCXX_3.4.11' not found (required by /Share/home/wangdong/local/bin/prep_reads)
重新安装tophat解决

cufflinks试运行
Warning: Your version of Cufflinks is not up-to-date. It is recommended that you upgrade to Cufflinks v2.2.1 to benefit from the most recent features and bug fixes (http://cufflinks.cbcb.umd.edu).
[16:05:53] Inspecting reads and determining fragment length distribution.
> Processed 1 loci.                            [*************************] 100%
Warning: Using default Gaussian distribution due to insufficient paired-end reads in open ranges.  It is recommended that correct parameters (--frag-len-mean and --frag-len-std-dev) be provided.
> Map Properties:
>       Normalized Map Mass: 92.00
>       Raw Map Mass: 92.00
>       Fragment Length Distribution: Truncated Gaussian (default)
>                     Default Mean: 200
>                  Default Std Dev: 80
[16:05:53] Assembling transcripts and estimating abundances.
> Processed 1 loci.                            [*************************] 100%
cuffmerge试运行
[wangdong@mgt /Work1/home/wangdong/lss/tophat_test/test_data/test4]$/Work1/home/wangdong/lje/cufflinks-2.1.1.Linux_x86_64/cuffmerge -g /Work1/home/wangdong/ensemble/genes.gtf -s /Work1/home/wangdong/ensemble/genome.fa -p 4 /Work1/home/wangdong/lss/tophat_test/test_data/test4/cuffmerge/assemblies.txt -o /Work1/home/wangdong/lss/tophat_test/test_data/test4/cuffmerge

[Tue Nov  4 16:16:45 2014] Beginning transcriptome assembly merge
-------------------------------------------

[Tue Nov  4 16:16:45 2014] Preparing output location ./merged_asm/
[Tue Nov  4 16:16:54 2014] Converting GTF files to SAM
[16:16:54] Loading reference annotation.
[16:16:54] Loading reference annotation.
[Tue Nov  4 16:16:54 2014] Quantitating transcripts
Warning: Your version of Cufflinks is not up-to-date. It is recommended that you upgrade to Cufflinks v2.2.1 to benefit from the most recent features and bug fixes (http://cufflinks.cbcb.umd.edu).
Command line:
cufflinks -o ./merged_asm/ -F 0.05 -g /Work1/home/wangdong/ensemble/genes.gtf -q --overhang-tolerance 200 --library-type=transfrags -A 0.0 --min-frags-per-transfrag 0 --no-5-extend -p 4 ./merged_asm/tmp/mergeSam_fileDgeYh1 
[bam_header_read] EOF marker is absent.
[bam_header_read] invalid BAM binary header (this is not a BAM file).
File ./merged_asm/tmp/mergeSam_fileDgeYh1 doesn't appear to be a valid BAM file, trying SAM...
[16:16:55] Loading reference annotation.
[16:17:15] Inspecting reads and determining fragment length distribution.
Processed 36885 loci.                       
> Map Properties:
>       Normalized Map Mass: 36886.00
>       Raw Map Mass: 36886.00
>       Fragment Length Distribution: Truncated Gaussian (default)
>                     Default Mean: 200
>                  Default Std Dev: 80
[16:17:16] Assembling transcripts and estimating abundances.

6:126102306-130463972   Warning: Skipping large bundle.
Processed 36884 loci.                       
[Tue Nov  4 16:20:30 2014] Comparing against reference file /Work1/home/wangdong/ensemble/genes.gtf
Warning: Your version of Cufflinks is not up-to-date. It is recommended that you upgrade to Cufflinks v2.2.1 to benefit from the most recent features and bug fixes (http://cufflinks.cbcb.umd.edu).
No fasta index found for /Work1/home/wangdong/ensemble/genome.fa. Rebuilding, please wait..
Fasta index rebuilt.
Warning: couldn't find fasta record for 'GL000191.1'!
Warning: couldn't find fasta record for 'GL000192.1'!
Warning: couldn't find fasta record for 'GL000193.1'!
Warning: couldn't find fasta record for 'GL000194.1'!
Warning: couldn't find fasta record for 'GL000195.1'!
Warning: couldn't find fasta record for 'GL000196.1'!
Warning: couldn't find fasta record for 'GL000199.1'!
Warning: couldn't find fasta record for 'GL000201.1'!
Warning: couldn't find fasta record for 'GL000204.1'!!
Warning: couldn't find fasta record for 'test_chromosome'!
[Tue Nov  4 16:23:27 2014] Comparing against reference file /Work1/home/wangdong/ensemble/genes.gtf
Warning: Your version of Cufflinks is not up-to-date. It is recommended that you upgrade to Cufflinks v2.2.1 to benefit from the most recent features and bug fixes (http://cufflinks.cbcb.umd.edu).
Warning: couldn't find fasta record for 'GL000191.1'!
Warning: couldn't find fasta record for 'GL000192.1'!
Warning: couldn't find fasta record for 'GL000193.1'!
Warning: couldn't find fasta record for 'GL000194.1'!
Warning: couldn't find fasta record for 'GL000195.1'!
Warning: couldn't find fasta record for 'GL000196.1'!
Warning: couldn't find fasta record for 'GL000199.1'!
Warning: couldn't find fasta record for 'GL000201.1'!
Warning: couldn't find fasta record for 'GL000204.1'!
Warning: couldn't find fasta record for 'GL000205.1'!
Warning: couldn't find fasta record for 'GL000209.1'!
Warning: couldn't find fasta record for 'GL000211.1'!
Warning: couldn't find fasta record for 'GL000212.1'!
Warning: couldn't find fasta record for 'GL000213.1'!
Warning: couldn't find fasta record for 'test_chromosome'!
但是运行结束依旧
RNA-seq运行过程
/pipeline/RNAFinder/bin下
tophat_sw.sh
cufflinks_sw.sh
每个样品与配对比较的样品之间做一个assemblies.txt，为cuffmerge做准备
cuffmerge（本地运行，bsub提交结果不知输出到哪里，估计为/pipeline/RNAFinder/bin）
/Work1/home/wangdong/lje/cufflinks-2.1.1.Linux_x86_64/cuffmerge -g /Work1/home/wangdong/ensemble/genes.gtf -s /Work1/home/wangdong/ensemble/genome.fa -p 4 /Work1/home/wangdong/lss/20141021/Undetermined_indices/sw4/RNA_seq/cuffmerge/SWC2/assemblies.txt
/Work1/home/wangdong/lje/cufflinks-2.1.1.Linux_x86_64/cuffmerge -g /Work1/home/wangdong/ensemble/genes.gtf -s /Work1/home/wangdong/ensemble/genome.fa -p 4 /Work1/home/wangdong/lss/20141021/Undetermined_indices/sw4/RNA_seq/cuffmerge/SWC3/assemblies.txt
/Work1/home/wangdong/lje/cufflinks-2.1.1.Linux_x86_64/cuffmerge -g /Work1/home/wangdong/ensemble/genes.gtf -s /Work1/home/wangdong/ensemble/genome.fa -p 4 /Work1/home/wangdong/lss/20141021/Undetermined_indices/sw4/RNA_seq/cuffmerge/SWM1/assemblies.txt
/Work1/home/wangdong/lje/cufflinks-2.1.1.Linux_x86_64/cuffmerge -g /Work1/home/wangdong/ensemble/genes.gtf -s /Work1/home/wangdong/ensemble/genome.fa -p 4 /Work1/home/wangdong/lss/20141021/Undetermined_indices/sw4/RNA_seq/cuffmerge/SWM2/assemblies.txt
cuffdiff运行
/Work1/home/wangdong/lje/cufflinks-2.1.1.Linux_x86_64/cuffdiff -o /Work1/home/wangdong/lss/20141021/Undetermined_indices/sw4/RNA_seq/cuffdiff/SWC2 -b /Work1/home/wangdong/ensemble/genome.fa -L SWC2,SWNC /Work1/home/wangdong/lss/20141021/Undetermined_indices/sw4/RNA_seq/cuffmerge/merged_asm_SWC2/merged.gtf /Work1/home/wangdong/lss/20141021/Undetermined_indices/sw4/RNA_seq/SWC2_1/accepted_hits.bam /Work1/home/wangdong/lss/20141021/Undetermined_indices/sw4/RNA_seq/SWNC_1/accepted_hits.bam
/Work1/home/wangdong/lje/cufflinks-2.1.1.Linux_x86_64/cuffdiff -o /Work1/home/wangdong/lss/20141021/Undetermined_indices/sw4/RNA_seq/cuffdiff/SWC3 -b /Work1/home/wangdong/ensemble/genome.fa -L SWC3,SWNC /Work1/home/wangdong/lss/20141021/Undetermined_indices/sw4/RNA_seq/cuffmerge/merged_asm_SWC3/merged.gtf /Work1/home/wangdong/lss/20141021/Undetermined_indices/sw4/RNA_seq/SWC3_1/accepted_hits.bam /Work1/home/wangdong/lss/20141021/Undetermined_indices/sw4/RNA_seq/SWNC_1/accepted_hits.bam
/Work1/home/wangdong/lje/cufflinks-2.1.1.Linux_x86_64/cuffdiff -o /Work1/home/wangdong/lss/20141021/Undetermined_indices/sw4/RNA_seq/cuffdiff/SWM1 -b /Work1/home/wangdong/ensemble/genome.fa -L SWM1,SWNC /Work1/home/wangdong/lss/20141021/Undetermined_indices/sw4/RNA_seq/cuffmerge/merged_asm_SWM1/merged.gtf /Work1/home/wangdong/lss/20141021/Undetermined_indices/sw4/RNA_seq/SWM1_1/accepted_hits.bam /Work1/home/wangdong/lss/20141021/Undetermined_indices/sw4/RNA_seq/SWNC_1/accepted_hits.bam
/Work1/home/wangdong/lje/cufflinks-2.1.1.Linux_x86_64/cuffdiff -o /Work1/home/wangdong/lss/20141021/Undetermined_indices/sw4/RNA_seq/cuffdiff/SWM2 -b /Work1/home/wangdong/ensemble/genome.fa -L SWM2,SWNC /Work1/home/wangdong/lss/20141021/Undetermined_indices/sw4/RNA_seq/cuffmerge/merged_asm_SWM2/merged.gtf /Work1/home/wangdong/lss/20141021/Undetermined_indices/sw4/RNA_seq/SWM2_1/accepted_hits.bam /Work1/home/wangdong/lss/20141021/Undetermined_indices/sw4/RNA_seq/SWNC_1/accepted_hits.bam
运行过程
Warning: Your version of Cufflinks is not up-to-date. It is recommended that you upgrade to Cufflinks v2.2.1 to benefit from the most recent features and bug fixes (http://cufflinks.cbcb.umd.edu).
[18:12:26] Loading reference annotation and sequence.
This contig will not be bias corrected.
Warning: couldn't find fasta record for 'HSCHR9_3_CTG35'!
This contig will not be bias corrected.
Warning: No conditions are replicated, switching to 'blind' dispersion method
[18:17:18] Inspecting maps and determining fragment length distributions.
[16:29:37] Modeling fragment count overdispersion.
> Map Properties:
>       Normalized Map Mass: 26054451.91
>       Raw Map Mass: 25086373.49
>       Fragment Length Distribution: Truncated Gaussian (default)
>                     Default Mean: 200
>                  Default Std Dev: 80
> Map Properties:
>       Normalized Map Mass: 26054451.91
>       Raw Map Mass: 27073412.27
>       Fragment Length Distribution: Truncated Gaussian (default)
>                     Default Mean: 200
>                  Default Std Dev: 80
[16:31:39] Calculating preliminary abundance estimates
processing locus步奏相当慢
Processed 43715 loci.                        [*************************] 100%
[20:50:23] Learning bias parameters.
 Testing for differential expression and regulation in locus.
> Processing Locus 1:206511376-206511708       [*                        ]  

$ ./configure
安装fastxtoolkit                         
自己安装                                 
libgtextutils-0.1                         $ make
ar -xjf libgtextutils-0.6.tar.bz2         
                                         
                                         $ sudo make install
 $ cd libgtextutils-0.6                  将/Share/home/wangdong/apps/src/libgtextutils-0.1
并且执行下面语句后，再对fastxtoolkit进行./configure即可运行configure了
export PKG_CONFIG_PATH=/Share/home/wangdong/lib/lib/pkgconfig:$PKG_CONFIG_PATH
cd fastx_toolkit-0.0.12
 

$ ./configure
$ make
$  make install


perl state.pl 需要将输入文件格式改为unix系统的
dos2unix
vi替换方法
:%s/原来的/替换成/g
/Share/home/lulab1/users/liyang/apps/bcl2fastq-1.8.4/bin:\

帅帅6个index,双端测序各为51个
configureBclToFastq.pl --input-dir ./Data/Intensities/BaseCalls/ --output ./Unaligned_test --sample-sheet ./SampleSheet.csv --no-eamss --use-bases-mask Y51,I6n,Y51

以前bcl2fastq数据位置和信息
/Share/home/wangdong/.Work1/mv_20140905/140822_C00126_0135_AH9RYYADXX

fastqc安装过程和运行过程
如安装的那个word说明书一样
只需要下载fastqc，解压，将fastqc这个文件 $chmod 755 fastqc
将这个文件夹加入到bashrc,source一下即可
但前提是要有一个1.6版本的java，及通过java -version就可以查看是不是有了，所以到java官网下载一下这个版本，安装java
java --version 
java
无，下载
tar xzvf jdk-8u25-linux-i586.tar.gz 
将Java里面一个bin里面所有文件拷到local/java文件夹中，再将local/java加入bashrc即可
cd /Share/home/wangdong/local/java
cp /Share/home/wangdong/apps/src/jdk1.8.0_25/bin/* ./
ls
java 
java -version
vi ~/.bashrc 
source ~/.bashrc
java -version
即可
fasqc运行
 fastqc -o ./ tss480_1_t.fastq tss480_2_t.fastq tss620_1_t.fastq tss620_2_t.fastq
$mkdir SRR1286228_1_qc
[wangdong@mgt /Work1/home/wangdong/lss/RNA_seq_web]$fastqc -o ./SRR1286228_1_qc SRR1286228_1.fastq
当然在运行fastqc时，后面必须要跟文件，最好给个输出目录，否则就会报下面的错
“No X11 DISPLAY variable was set
意思是没有图形化接入接口

/Share/home/wangdong/.Work1/ensemble(参考序列和GTF所在位置)

bsub提交任务
[wangdong@mgt /Share/home/wangdong/pipeline/RNA_finder/bin]$perl bsub.pl mapping_2.sh 1 TINY ./source.txt
C集群
perl qsub.pl -d(sh的文件夹如mapping_2.sh在的文件夹) -o(输出文件夹) 

李洋
fastq-dump SRR029215.sra
fastx_trimmer -l 25 -i SRR029215.sra -Q 33 -o SRR029215.fastq.process
bowtie -p 6 -S -q -m 1 --best --strata  ../indexes/c_elegans_ws220 SRR029215.fastq.process SRR029215_process.sam
samtools view SRR029215_process.sam >SRR029215_process.bam
 
macs14 -t SRR029217_process.bam -c SRR029215_process.bam -f BAM -g ce -n H3K9me3_N2 -m 10,30 - p 1e-5 --bw=300 -w -S
wignorm -t H3K9me3_N2_treat_afterfiting_all.wig -c H3K9me3_N2_control_afterfiting_all.wig -n H3K9me3_N2_norm --gsize=90000000


ncar上自己摸索安装
bowtie1（bowtie-0.12.7）,bowtie2（bowtie2-2.2.3）,,tophat(tophat-2.0.13.Linux_x86_64)
将包解压到apps/src，然后将里面所有可执行文件拷到local下面
samtools（samtools-0.1.17）需要make一下产生绿色的samtools，然后拷贝samtools,bcftools，mics里面的可执行文件到local下面
最后将export PATH=/home/lijine/local:$PATH
然后运行下面程序即可

 tophat -o test2 -r 20 test_ref reads_1.fq reads_2.fq 

[2014-10-16 18:23:22] Beginning TopHat run (v2.0.13)
-----------------------------------------------
[2014-10-16 18:23:22] Checking for Bowtie
                  Bowtie version:        2.2.3.0
[2014-10-16 18:23:22] Checking for Bowtie index files (genome)..
        Found both Bowtie1 and Bowtie2 indexes.
[2014-10-16 18:23:22] Checking for reference FASTA file
[2014-10-16 18:23:22] Generating SAM header for test_ref
[2014-10-16 18:23:22] Preparing reads
         left reads: min. length=75, max. length=75, 100 kept reads (0 discarded)
        right reads: min. length=75, max. length=75, 100 kept reads (0 discarded)
[2014-10-16 18:23:22] Mapping left_kept_reads to genome test_ref with Bowtie2 
[2014-10-16 18:23:22] Mapping left_kept_reads_seg1 to genome test_ref with Bowtie2 (1/3)
[2014-10-16 18:23:22] Mapping left_kept_reads_seg2 to genome test_ref with Bowtie2 (2/3)
[2014-10-16 18:23:22] Mapping left_kept_reads_seg3 to genome test_ref with Bowtie2 (3/3)
[2014-10-16 18:23:23] Mapping right_kept_reads to genome test_ref with Bowtie2 
[2014-10-16 18:23:23] Mapping right_kept_reads_seg1 to genome test_ref with Bowtie2 (1/3)
[2014-10-16 18:23:23] Mapping right_kept_reads_seg2 to genome test_ref with Bowtie2 (2/3)
[2014-10-16 18:23:23] Mapping right_kept_reads_seg3 to genome test_ref with Bowtie2 (3/3)
[2014-10-16 18:23:23] Searching for junctions via segment mapping
[2014-10-16 18:23:23] Retrieving sequences for splices
[2014-10-16 18:23:23] Indexing splices
Building a SMALL index
[2014-10-16 18:23:24] Mapping left_kept_reads_seg1 to genome segment_juncs with Bowtie2 (1/3)
[2014-10-16 18:23:24] Mapping left_kept_reads_seg2 to genome segment_juncs with Bowtie2 (2/3)
[2014-10-16 18:23:24] Mapping left_kept_reads_seg3 to genome segment_juncs with Bowtie2 (3/3)
[2014-t

illumina-dump: Convert SRA data into Illumina native formats (qseq, etc.)

prefetch: Allows command-line downloading of SRA, dbGaP, and ADSP data

sam-dump: Convert SRA data to sam format

sff-dump: Convert SRA data to sff format

sra-stat: Generate statistics about SRA data (quality distribution, etc.)

sra-pileup: Generate pileup statistics on aligned SRA data

vdb-config: Display and modify VDB configuration information

vdb-dump: Output the native VDB format of SRA data.

vdb-encrypt: Encrypt non-SRA dbGaP data ("phenotype data")

vdb-decrypt: Decrypt non-SRA dbGaP data ("phenotype data")

vdb-validate: Validate the integrity of downloaded SRA data
。主要有samtools（命令示例：samtools view -bS file1.sam -o file.bam)， bedtools （bamToFastq等 命令示例：bamToFastq -i file1.bam -fa file1.fastq)，sratools等，还有些软件自己附带的工具，如igvtools等等.
很多工具如tophat等还会基于samtools的API或者library，所以安装这些工具也是运行后续分析工具的前提
将安装软件加入PATH
vi ~/.bashrc
source  ~/.bashrc
查看PATH
echo $PATH
1)对于.tar结尾的文件
　　tar -xf all.tar
　　2)对于.gz结尾的文件
　　gzip -d all.gz
　　gunzip all.gz
　　3)对于.tgz或.tar.gz结尾的文件
　　tar -xzf all.tar.gz
　　tar -xzf all.tgz
　　4)对于.bz2结尾的文件
　　bzip2 -d all.bz2
　　bunzip2 all.bz2
　　5)对于tar.bz2结尾的文件
　　tar -xjf all.tar.bz2
　　6)对于.Z结尾的文件
　　uncompress all.Z
　　7)对于.tar.Z结尾的文件
　　tar -xZf all.tar.z

　　另外对于Window下的常见压缩文件.zip和.rar，Linux也有相应的方法来解压它
们：
　　1)对于.zip
　　linux下提供了zip和unzip程序，zip是压缩程序，unzip是解压程序。它们的参
数选项很多，这里只做简单介绍，依旧举例说明一下其用法：
　　# zip all.zip *.jpg
　　这条命令是将所有.jpg的文件压缩成一个zip包
# unzip all.zip
　　这条命令是将all.zip中的所有文件解压出来
　　2)对于.rar
　　要在linux下处理.rar文件，需要安装RAR for Linux，可以从网上下载，但要记住，RAR for Linux 不是免费的；可从http://www.rarsoft.com/download.htm下载RARfor Linux 3.2.
0，然后安装：
　　# tar -xzpvf rarlinux-3.2.0.tar.gz
　　# cd rar
　　# make
　　这样就安装好了，安装后就有了rar和unrar这两个程序，rar是压缩程序，unrar 是解压程序。它们的参数选项很多，这里只做简单介绍，依旧举例说明一下其用法：

　　# rar a all *.jpg
　　这条命令是将所有.jpg的文件压缩成一个rar包，名为all.rar，该程序会将.rar
扩展名将自动附加到包名后。
　　# unrar e all.rar
　　这条命令是将all.rar中的所有文件解压出来
　　到此为至，我们已经介绍过linux下的tar、gzip、gunzip、bzip2、bunzip2、compress 、 uncompress、 zip、unzip、rar、unrar等程式，你应该已经能够使用它们对.tar 、.gz、.tar.gz、.tgz、.bz2、.tar.bz2、. Z、.tar.Z、.zip、.rar这10种压缩文
件进行解压了，以后应该不需要为下载了一个软件而不知道如何在Linux下解开而烦恼了。而且以上方法对于Unix也基本有效。
　　本文介绍了linux下的压缩程式tar、gzip、gunzip、bzip2、bunzip2、compress 、uncompress、 zip、 unzip、rar、unrar等程式，以及如何使用它们对.tar、.gz 、.tar.gz、.tgz、.bz2、.tar.bz2、.Z、. tar.Z、.zip、.rar这10种压缩文件进行
操作。

以下补充

tar

-c: 建立压缩档案
-x：解压
-t：查看内容
-r：向压缩归档文件末尾追加文件
-u：更新原压缩包中的文件

这五个是独立的命令，压缩解压都要用到其中一个，可以和别的命令连用但只能用其中一个。下面的参数是根据需要在压缩或解压档案时可选的。

-z：有gzip属性的
-j：有bz2属性的
-Z：有compress属性的
-v：显示所有过程
-O：将文件解开到标准输出

下面的参数-f是必须的
-f: 使用档案名字，切记，这个参数是最后一个参数，后面只能接档案名。
# tar -cf all.tar *.jpg这条命令是将所有.jpg的文件打成一个名为all.tar的包。-c是表示产生新的包，-f指定包的文件名。
# tar -rf all.tar *.gif
这条命令是将所有.gif的文件增加到all.tar的包里面去。-r是表示增加文件的意思。
# tar -uf all.tar logo.gif
这条命令是更新原来tar包all.tar中logo.gif文件，-u是表示更新文件的意思。
# tar -tf all.tar
这条命令是列出all.tar包中所有文件，-t是列出文件的意思
# tar -xf all.tar
这条命令是解出all.tar包中所有文件，-x是解开的意思
压缩
tar Ccvf jpg.tar *.jpg //将目录里所有jpg文件打包成tar.jpg
tar Cczf jpg.tar.gz *.jpg //将目录里所有jpg文件打包成jpg.tar后，并且将其用gzip压缩，生成一个gzip压缩过的包，命名为jpg.tar.gz
tar Ccjf jpg.tar.bz2 *.jpg //将目录里所有jpg文件打包成jpg.tar后，并且将其用bzip2压缩，生成一个bzip2压缩过的包，命名为jpg.tar.bz2
tar CcZf jpg.tar.Z *.jpg //将目录里所有jpg文件打包成jpg.tar后，并且将其用compress压缩，生成一个umcompress压缩过的包，命名为jpg.tar.Z
rar a jpg.rar *.jpg //rar格式的压缩，需要先下载rar for linux
zip jpg.zip *.jpg //zip格式的压缩，需要先下载zip for linux

解压
tar Cxvf file.tar //解压 tar包
tar -xzvf file.tar.gz //解压tar.gz
tar -xjvf file.tar.bz2 //解压 tar.bz2
tar CxZvf file.tar.Z //解压tar.Z
unrar e file.rar //解压rar
unzip file.zip //解压zip

总结
1、*.tar 用 tar Cxvf 解压
2、*.gz 用 gzip -d或者gunzip 解压
3、*.tar.gz和*.tgz 用 tar Cxzf 解压
4、*.bz2 用 bzip2 -d或者用bunzip2 解压
5、*.tar.bz2用tar Cxjf 解压
6、*.Z 用 uncompress 解压
7、*.tar.Z 用tar CxZf 解压
8、*.rar 用 unrar e解压
9、*.zip 用 unzip 解压 
Bcl2fastq
configureBclToFastq.pl --input-dir ./Data/Intensities/BaseCalls/ --output /Share/home/wangdong/whl20140828_3/Unaligned --sample-sheet ./SampleSheet.csv --no-eamss --use-bases-mask Y51,I7n
cd /Share/home/wangdong/whl20140828/Unaligned
nohup make -j 8(或者查询李洋步骤cd $path0/Unaligned
screen
make -j 8 >$path0/Unaligned/log)

nohup和screen命令作用  

2009-08-10 22:42:17|  分类： linux |举报 |字号 订阅

通常linux的进程在父目录被KILL以后，就会接收到sighup命令，然后退出运行。
nohup和screen 都可以对sighup命令起到屏蔽作用。nohup比较小巧，screen命令比较强大。各有用处

nohup 命令对sighup信号有屏蔽作用，让命令可以持续运行。如果需要在终端关闭后还可以运行，
需在后面加 &
格式 nohup <command> [argument...]  &

screen 命令的的用法。
运行screen命令后会自动打开一个新shell，在这个新的shell里可以运行任何命令。
在临时离开，或者电脑关机后
如果你要直接退出这个shell，运行:
exit;
如果这个shell中运行的命令比较重要，可以先按:
crtl+a,然后按d键
临时退出这个shell终端，但任务会继续运行。
想重新连接这个screen  shell ,但是忘记了是那个screen ，用：
screen -ls 来查看screen info

然后重新连接，运行:
screen -r SCREENID

如果screen的进程打开的比较多，为了方便识别不同的screen，可以加上 -S 参数，给每个screen不同的
名称 比如 screen -S test ,进程里就会看到这个screen被标为 test.ttyn.host,而不是进程ID了

 1000  /Share/home/wangdong/pipeline/RNA_finder/bin
 1001  cd /Share/home/wangdong/pipeline/RNA_finder/bin
 1002  ld
 1003  ls
 1004  less qsub.pl 
 1005  ls
 1006  bsub
 1007  perl bsub.pl 
 1008  bqueues 
 1009  ls
 1010  bsub.pl /Share/home/wangdong/140822_C00126_0135_AH9RYYADXX/bcl2fq.sh 1
 1011  perl bsub.pl /Share/home/wangdong/140822_C00126_0135_AH9RYYADXX/bcl2fq.sh 1
 1012  bjobs 
 1013  ll
 1014  less bcl2fq.e405497
 1015  ls
 1016  touch source.txt
 1017  ls
 1018  vi source.txt 
 1019  ls
 1020  vi source.txt 
 1021  ls
 1022  vi source.txt 
 1023  perl bsub.pl
 1024  perl bsub.pl /Share/home/wangdong/140822_C00126_0135_AH9RYYADXX/bcl2fq.sh 1 TINY ./source.txt 
 1025  bjobs 
 1026  ls
 1027  ll
 1028  less bcl2fq.e405498
 1029  perl bsub.pl /Share/home/wangdong/140822_C00126_0135_AH9RYYADXX/bcl2fq.sh 1 TINY ./source.txt 
 1030  bjobs 
 1031  ls
 1032  ll
 1033  less bcl2fq.e405499
 1034  ls
 1035  bsub -h
 1036  cp /Share/home/wangdong/140822_C00126_0135_AH9RYYADXX/bcl2fq.sh ./
 1037  ls
 1038  bsub bcl2fq.sh 
 1039  bjobs 
 1040  bjobs 
 1041  bjobs 
 1042  bjobs 
 1043  history 

linux下如何编译安装bzip2

下载源文件安装包：

http://www.bzip.org/downloads.html

解压：

tar -xzvf bzip2-1.0.6.tar.gz

进入解压后的目录：

cd bzip2-1.0.6

为编译做准备，创建libbz2.so动态链接库(这一步很重要，安装python的时候如果没有这一步，python安装不上bz2模块)：

make -f Makefile-libbz2_so

编译&&安装：

make && make install

至此，大功告成！

ll -al查看隐藏文件
vi ~/.bashrc
source  ~/.bashrc

fq2fa
awk 'NR % 4 < 3 '  K_ATCTCGT_TOTAL.fastq | awk 'NR % 3 > 0 ' | perl -p -e 's/^@/>/g' > K_ATCTCGT_TOTAL.fasta
grep 部分不匹配
grep '[^TAG]TGAAGCCACAGATGTA' L_AAGCACT.fastq > test

 cat /proc/cpuinfo |grep "physical id"| sort| uniq| wc -1 2
每个物理CPU中core的个数(即核数):
 cat /proc/cpuinfo| grep "cpu cores"| uniq cpu cores　: 1 
逻辑CPU的个数:
 cat /proc/cpuinfo| grep "processor"| wc -l  4
gzip -d解压gz文件
UltraEdit如何将相同开头的的文本替换
如：将UltraEdit-1213，UltraEdit-12241，UltraEdit-adf全部替换为UltraEdit-haha!
2012-01-06 09:13 提问者采纳
1.如果后面有空格
将：UltraEdit-* 
替换为：UltraEdit-haha
注意，*后有空格

2.如果后面有回车，则改为
将：UltraEdit-*^p
替换为：UltraEdit-haha^p

 UltraEdit中如何删除含有特定内容的行/删除不含有特定内容的行
分类： 杂类 2010-01-06 21:34 6026人阅读 评论(0) 收藏 举报
正则表达式list
如某文件中有些行含有特定内容PTTAddress

1. UltraEdit中如何删除含有特定内容的行

a. 使用“替换”功能，勾选正则表达式(Regular Expressions)，“替换”内容为“%*PTTAddress*^p”，“替换为”为空；

b. 删除空行，使用“替换”功能，“替换”内容为“^r^n^r^n”，“替换为”为“^r^n”；

2. UltraEdit中如何删除不含有特定内容的行

a. 使用“搜索”功能，勾选“列出所含内容的行”(List Lines Containing Characters)，“搜索”内容为“PTTAddress”；

b. 在搜索结果中选择“拷贝到粘贴板”；

c. 新建一空白文件，然后ctrl+v。
在自己路径拷贝了小蒙的参考序列
cp /home/xuxm/zhihe/Ss046.fasta ./ 
bwa index Ss046.fasta
bwa aln Ss046.fasta SH7_1.fq > SH1_7.sai

bwa sampe Ss046.fasta SH7_1.sai SH7_2.sai SH7_1.fq SH7_2.fq > SH7_bwa.sam

samtools faidx Ss046.fasta 
 samtools import Ss046.fasta.fai SH7_bwa.sam SH7_bwa.bam

lishasha@guest-AltixXE250[xm] samtools                                [ 9:48AM]

Program: samtools (Tools for alignments in the SAM format)
Version: 0.1.19-44428cd

Usage:   samtools <command> [options]

Command: view        SAM<->BAM conversion
         sort        sort alignment file
         mpileup     multi-way pileup
         depth       compute the depth
         faidx       index/extract FASTA
         tview       text alignment viewer
         index       index alignment
         idxstats    BAM index stats (r595 or later)
         fixmate     fix mate information
         flagstat    simple stats
         calmd       recalculate MD/NM tags and '=' bases
         merge       merge sorted alignments
         rmdup       remove PCR duplicates
         reheader    replace BAM header
         cat         concatenate BAMs
         bedcov      read depth per BED region
         targetcut   cut fosmid regions (for fosmid pool only)
         phase       phase heterozygotes
         bamshuf     shuffle and group alignments by name

 samtools sort SH7_bwa.bam SH7_bam_sort
 lishasha@guest-AltixXE250[xm] samtools sort                           [ 9:48AM]

Usage:   samtools sort [options] <in.bam> <out.prefix>

Options: -n        sort by read name
         -f        use <out.prefix> as full file name instead of prefix
         -o        final output to stdout
         -l INT    compression level, from 0 to 9 [-1]
         -@ INT    number of sorting and compression threads [1]
         -m INT    max memory per thread; suffix K/M/G recognized [768M]
         
 samtools index SH7_bam_sort.bam 
samtools mpileup -uD -I -q20 -f Ss046.fasta SH7_bam_sort.bam > SH7.var
lishasha@guest-AltixXE250[xm] samtools mpileup                        [11:41AM]

Usage: samtools mpileup [options] in1.bam [in2.bam [...]]

Input options:

       -6           assume the quality is in the Illumina-1.3+ encoding
       -A           count anomalous read pairs
       -B           disable BAQ computation
       -b FILE      list of input BAM filenames, one per line [null]
       -C INT       parameter for adjusting mapQ; 0 to disable [0]
       -d INT       max per-BAM depth to avoid excessive memory usage [250]
       -E           recalculate extended BAQ on the fly thus ignoring existing BQs
       -f FILE      faidx indexed reference sequence file [null]
       -G FILE      exclude read groups listed in FILE [null]
       -l FILE      list of positions (chr pos) or regions (BED) [null]
       -M INT       cap mapping quality at INT [60]
       -r STR       region in which pileup is generated [null]
       -R           ignore RG tags
       -q INT       skip alignments with mapQ smaller than INT [0]
       -Q INT       skip bases with baseQ/BAQ smaller than INT [13]
       --rf INT     required flags: skip reads with mask bits unset []
       --ff INT     filter flags: skip reads with mask bits set []

Output options:

       -D           output per-sample DP in BCF (require -g/-u)
       -g           generate BCF output (genotype likelihoods)
       -O           output base positions on reads (disabled by -g/-u)
       -s           output mapping quality (disabled by -g/-u)
       -S           output per-sample strand bias P-value in BCF (require -g/-u)
       -u           generate uncompress BCF output

SNP/INDEL genotype likelihoods options (effective with `-g' or `-u'):

       -e INT       Phred-scaled gap extension seq error probability [20]
       -F FLOAT     minimum fraction of gapped reads for candidates [0.002]
       -h INT       coefficient for homopolymer errors [100]
       -I           do not perform indel calling
       -L INT       max per-sample depth for INDEL calling [250]
       -m INT       minimum gapped reads for indel candidates [1]
       -o INT       Phred-scaled gap open sequencing error probability [40]
       -p           apply -m and -F per-sample to increase sensitivity
       -P STR       comma separated list of platforms for indels [all]
 bcftools view SH7_SNP.bcf > SH7.vcf 
 bcftools view -N SH7_SNP.bcf > SH7_SNP_N.bcf
 
lishasha@guest-AltixXE250[xm] bcftools                                [12:11PM]

Program: bcftools (Tools for data in the VCF/BCF formats)
Version: 0.1.17-dev (r973:277)

Usage:   bcftools <command> <arguments>

Command: view      print, extract, convert and call SNPs from BCF
         index     index BCF
         cat       concatenate BCFs
         ld        compute all-pair r^2
         ldpair    compute r^2 between requested pairs 
 lishasha@guest-AltixXE250[xm] bcftools view                           [12:11PM]

Usage: bcftools view [options] <in.bcf> [reg]

Input/output options:

       -A        keep all possible alternate alleles at variant sites
       -b        output BCF instead of VCF
       -D FILE   sequence dictionary for VCF->BCF conversion [null]
       -F        PL generated by r921 or before (which generate old ordering)
       -G        suppress all individual genotype information
       -l FILE   list of sites (chr pos) or regions (BED) to output [all sites]
       -L        calculate LD for adjacent sites
       -N        skip sites where REF is not A/C/G/T
       -Q        output the QCALL likelihood format
       -s FILE   list of samples to use [all samples]
       -S        input is VCF
       -u        uncompressed BCF output (force -b)

Consensus/variant calling options:

       -c        SNP calling (force -e)
       -d FLOAT  skip loci where less than FLOAT fraction of samples covered [0]
       -e        likelihood based analyses
       -g        call genotypes at variant sites (force -c)
       -i FLOAT  indel-to-substitution ratio [-1]
       -I        skip indels
       -p FLOAT  variant if P(ref|D)<FLOAT [0.5]
       -P STR    type of prior: full, cond2, flat [full]
       -t FLOAT  scaled substitution mutation rate [0.001]
       -T STR    constrained calling; STR can be: pair, trioauto, trioxd and trioxs (see manual) [null]
       -v        output potential variant sites only (force -c)

Contrast calling and association test options:

       -1 INT    number of group-1 samples [0]
       -C FLOAT  posterior constrast for LRT<FLOAT and P(ref|D)<0.5 [1]
       -U INT    number of permutations for association testing (effective with -1) [0]
       -X FLOAT  only perform permutations for P(chi^2)<FLOAT [0.01]

lishasha@guest-AltixXE250[xm] vcfutils.pl                             [12:14PM]

Usage:   vcfutils.pl <command> [<arguments>]

Command: subsam       get a subset of samples
         listsam      list the samples
         fillac       fill the allele count field
         qstats       SNP stats stratified by QUAL

         hapmap2vcf   convert the hapmap format to VCF
         ucscsnp2vcf  convert UCSC SNP SQL dump to VCF

         varFilter    filtering short variants (*)
         vcf2fq       VCF->fastq (**)

Notes: Commands with description endting with (*) may need bcftools
       specific annotations.
lishasha@guest-AltixXE250[xm] vcfutils.pl varFilter                   [12:14PM]

Usage:   vcfutils.pl varFilter [options] <in.vcf>

Options: -Q INT    minimum RMS mapping quality for SNPs [10]
         -d INT    minimum read depth [2]
         -D INT    maximum read depth [10000000]
         -a INT    minimum number of alternate bases [2]
         -w INT    SNP within INT bp around a gap to be filtered [3]
         -W INT    window size for filtering adjacent gaps [10]
         -1 FLOAT  min P-value for strand bias (given PV4) [0.0001]
         -2 FLOAT  min P-value for baseQ bias [1e-100]
         -3 FLOAT  min P-value for mapQ bias [0]
         -4 FLOAT  min P-value for end distance bias [0.0001]
                 -e FLOAT  min P-value for HWE (plus F<0) [0.0001]
         -p        print filtered variants

Note: Some of the filters rely on annotations generated by SAMtools/BCFtools.

 vcfutils.pl varFilter -Q20 -d10 -D100 SH7_SNP_N.bcf > SH7_flitered_2.vcf
20140424帮小蒙的SNP
bwa map
bwa index Ss046.fasta 
bwa aln Ss046.fasta SH1_1.fq > SH1_1.sai
bwa aln Ss046.fasta SH1_2.fq > SH1_2.sai  

bwa sampe
bwa sampe Ss046.fasta SH1_1.sai SH1_2.sai SH1_1.fq SH1_2.fq > SH1_bwa.sam 
                                                                        
samtools sort bam                                                                  
samtools faidx Ss046.fasta                                                                         
samtools import Ss046.fasta.fai SH1_bwa.sam SH1_bwa.bam 
samtools sort SH1_bwa.bam SH1_bwa_sorted
samtools index SH1_bwa_sorted.bam


SNP
samtools mpileup -uD -I -q20 -f Ss046.fasta SH1_bwa_sorted.bam | bcftools view -bvcg -> samtools_snps/SH1.bcf
bcftools view -N samtools_snps/SH1.bcf | vcfutils.pl varFilter -D100 -d5 -Q20 > samtools_snps/SH1.vcf




20140122
 /vol3/home/fanhang/bin/ia64-suse-linux/blat
blat - Standalone BLAT v. 35 fast sequence search command line tool
usage:
   blat database query [-ooc=11.ooc] output.psl
where:
   database and query are each either a .fa , .nib or .2bit file,
   or a list these files one file name per line.
   -ooc=11.ooc tells the program to load over-occurring 11-mers from
               and external file.  This will increase the speed
               by a factor of 40 in many cases, but is not required
   output.psl is where to put the output.
   Subranges of nib and .2bit files may specified using the syntax:
      /path/file.nib:seqid:start-end
   or
      /path/file.2bit:seqid:start-end
   or
      /path/file.nib:start-end
   With the second form, a sequence id of file:start-end will be used.
options:
   -t=type     Database type.  Type is one of:
                 dna - DNA sequence
                 prot - protein sequence
                 dnax - DNA sequence translated in six frames to protein
               The default is dna
   -q=type     Query type.  Type is one of:
                 dna - DNA sequence
                 rna - RNA sequence
                 prot - protein sequence
                 dnax - DNA sequence translated in six frames to protein
                 rnax - DNA sequence translated in three frames to protein
               The default is dna
   -prot       Synonymous with -t=prot -q=prot
   -ooc=N.ooc  Use overused tile file N.ooc.  N should correspond to 
               the tileSize
   -tileSize=N sets the size of match that triggers an alignment.  
               Usually between 8 and 12
               Default is 11 for DNA and 5 for protein.
   -stepSize=N spacing between tiles. Default is tileSize.
   -oneOff=N   If set to 1 this allows one mismatch in tile and still
               triggers an alignments.  Default is 0.
   -minMatch=N sets the number of tile matches.  Usually set from 2 to 4
               Default is 2 for nucleotide, 1 for protein.
   -minScore=N sets minimum score.  This is the matches minus the 
               mismatches minus some sort of gap penalty.  Default is 30
   -minIdentity=N Sets minimum sequence identity (in percent).  Default is
               90 for nucleotide searches, 25 for protein or translated
               protein searches.
   -maxGap=N   sets the size of maximum gap between tiles in a clump.  Usually
               set from 0 to 3.  Default is 2. Only relevent for minMatch > 1.
   -noHead     suppress .psl header (so it's just a tab-separated file)
   -makeOoc=N.ooc Make overused tile file. Target needs to be complete genome.
   -repMatch=N sets the number of repetitions of a tile allowed before
               it is marked as overused.  Typically this is 256 for tileSize
               12, 1024 for tile size 11, 4096 for tile size 10.
               Default is 1024.  Typically only comes into play with makeOoc.
               Also affected by stepSize. When stepSize is halved repMatch is
               doubled to compensate.
   -mask=type  Mask out repeats.  Alignments won't be started in masked region
               but may extend through it in nucleotide searches.  Masked areas
               are ignored entirely in protein or translated searches. Types are
                 lower - mask out lower cased sequence
                 upper - mask out upper cased sequence
                 out   - mask according to database.out RepeatMasker .out file
                 file.out - mask database according to RepeatMasker file.out
   -qMask=type Mask out repeats in query sequence.  Similar to -mask above but
               for query rather than target sequence.
   -repeats=type Type is same as mask types above.  Repeat bases will not be
               masked in any way, but matches in repeat areas will be reported
               separately from matches in other areas in the psl output.
   -minRepDivergence=NN - minimum percent divergence of repeats to allow 
               them to be unmasked.  Default is 15.  Only relevant for 
               masking using RepeatMasker .out files.
   -dots=N     Output dot every N sequences to show program's progress
   -trimT      Trim leading poly-T
   -noTrimA    Don't trim trailing poly-A
   -trimHardA  Remove poly-A tail from qSize as well as alignments in 
               psl output
   -fastMap    Run for fast DNA/DNA remapping - not allowing introns, 
               requiring high %ID. Query sizes must not exceed 5000.
   -out=type   Controls output file format.  Type is one of:
                   psl - Default.  Tab separated format, no sequence
                   pslx - Tab separated format with sequence
                   axt - blastz-associated axt format
                   maf - multiz-associated maf format
                   sim4 - similar to sim4 format
                   wublast - similar to wublast format
                   blast - similar to NCBI blast format
                   blast8- NCBI blast tabular format
                   blast9 - NCBI blast tabular format with comments
   -fine       For high quality mRNAs look harder for small initial and
               terminal exons.  Not recommended for ESTs
   -maxIntron=N  Sets maximum intron size. Default is 750000
   -extendThroughN - Allows extension of alignment through large blocks of N's


/vol3/home/fanhang/bin/ia64-suse-linux/blat  /vol2/biodb/human/hg19-fa/hg19.fasta   /vol3/home/liss/2014122/IonXpress_030.R_2012_08_28_17_42_56_user_20140120-2.fa  -fastMap -out=blast  /vol3/home/liss/2014122/119_30.blast  &



python /vol3/home/wangw/tools/blast2tab-n1+e+2cover+nohits-pick.py 119_30.blast 119_30.tab 1 &
提取tab文件中的id号

根据序列号从原始序列中提取reads  并将原始reads分为提取和剩余两部分
python fa-pick-according-list.py  IonXpress_030.R_2012_08_28_17_42_56_user_20140120-2.fa  119_30_id.fa human-reads-2.fa
将nohuman-reads与数据库进行比对
/vol3/home/wangw/blast/blast-2.2.22/bin/blastall -p blastn -d /vol2/nt-fultit/nt-fultit -i /vol3/home/liss/2014122/human-reads-2_non.fa -o 119_30.blastn -v 5 -b 5 -W 35 -a 20 &  

blast2tab
for k in {0..49}; do python /vol3/home/wangw/tools/blast2tab-n1+e+2cover+nohits-pick.py 46_$k.blastn 46_$k.tab 1 & done
python blast2tab-n1+e+2cover+nohits-pick.py 119_30.blastn 119_30_blastn.tab 1 &

tab2sta
python tab2sta-FulTit.py 119_30_blastn.tab 119_30_blastn.sta species &

tax_rank: 1397108
['17065927', 279] ['34740422', 266]
TidLenNum: 4772
['10129445', 121, 1] ['10186549', 155, 1]
TLN: 1659
['Homo sapiens', 121, 1, '9606'] ['Homo sapiens', 155, 1, '9606']
rlt: 1657
notax: 2
['Acetobacter pasteurianus', 42, 1, '438'] ['Achromobacter xylosoxidans', 827, 6, '85698']
result: 699




20131206
 

REAPR version: 1.0.16
Usage:
    reapr <task> [options]

Common tasks:
    facheck    - checks IDs in fasta file
    smaltmap   - map read pairs using SMALT: makes a BAM file to be used as
                 input to the pipeline
    perfectmap - make perfect uniquely mapping plot files
    pipeline   - runs the REAPR pipeline, using an assembly and mapped reads
                 as input, and optionally results of perfectmap.
                 (It runs facheck, preprocess, stats, fcdrate, score,
                 summary and break)
    plots      - makes Artemis plot files for a given contig, using results
                 from stats (and optionally results from score)
    seqrename  - renames all sequences in a BAM file: use this if you already
                 mapped your reads but then found facheck failed - saves
                 remapping the reads so that pipeline can be run

Advanced tasks:
    preprocess - preprocess files: necessary for running stats
    stats      - generates stats from a BAM file
    fcdrate    - estimates FCD cutoff for score, using results from stats
    score      - calculates scores and assembly errors, using results from stats
    summary    - make summary stats file, using results from score
    break      - makes broken assembly, using results from score
    gapresize  - experimental, calculates gap sizes based on read mapping
    perfectfrombam - generate perfect mapping plots from a bam file (alternative
                     to using perfectmap for large genomes)

Reapr用matepair数据检查拼接序列

先检查input文件
fa文件（拼接好的序列）
/home/xuxm/software/Reapr_1.0.16/reapr facheck A16R_ref_gapclose_1.fa #如果报错,运行下一步， 正确
reapr facheck assembly.fa new_assembly #重新命名

fastq文件（matepair reads)
smalt check *.fastq

lishasha@guest-AltixXE250[A16R_cheak] smalt                           [ 4:27AM]

              SMALT - Sequence Mapping and Alignment Tool
                             (version: 0.7.5)
SYNOPSIS:
    smalt <task> [TASK_OPTIONS] [<index_name> <file_name_A> [<file_name_B>]]

Available tasks:
    smalt check   - checks FASTA/FASTQ input
    smalt help    - prints a brief summary of this software
    smalt index   - builds an index of k-mer words for the reference
    smalt map     - maps single or paired reads onto the reference
    smalt sample  - sample insert sizes for paired reads
    smalt version - prints version information

Help on individual tasks:
    smalt <task> -H

lishasha@guest-AltixXE250[A16R_cheak] smalt check *.fastq             [ 4:33AM]
# Command line:  smalt check A16R_split_1.fastq A16R_split_2.fastq
# checked 551022 read pairs: ok.



分两步先把matepair reads map到拼接序列上生成bam文件，然后再做分析
/home/xuxm/software/Reapr_1.0.16/reapr smaltmap A16_complete_new.fa.fa A16R_split_1.fastq A16R_split_2.fastq A16R_complete_mapped.bam

[fai_build_core] different line length in sequence 'A16R_whole_genome_1'.
[REAPR smaltmap] Error in system call:
/home/xuxm/software/Reapr_1.0.16/src/samtools faidx A16R_ref_gapclose_1.fa

This means samtools is unhappy with the assembly
fasta file 'A16R_ref_gapclose_1.fa'.

Common causes are empty lines in the file, or inconsistent line lengths
(all sequence lines must have the same length, except the last line of
any sequence which can be shorter). Cannot continue.

于是再运行下面命令
/home/xuxm/software/Reapr_1.0.16/reapr facheck A16R_ref_gapclose_1.fa A16R_new.fa
再运行
/home/xuxm/software/Reapr_1.0.16/reapr smaltmap A16R_new.fa A16R_split_1.fastq A16R_split_2.fastq A16R_mapped.bam 还是错误 人工检查
/home/xuxm/software/Reapr_1.0.16/reapr smaltmap A16R_split_1.fastq A16R_split_2.fastq A16R_mapped.b
usage:
task_smaltmap.pl [options] <assembly.fa> <reads_1.fastq> <reads_2.fastq> <out.bam>

Maps read pairs to an asembly with SMALT, making a final BAM file that
is sorted, indexed and has duplicates removed.

The n^th read in reads_1.fastq should be the mate of the n^th read in
reads_2.fastq.

It is assumed that reads are 'innies', i.e. the correct orientation
is reads in a pair pointing towards each other (---> <---).

Options:

-k <int>
        The -k option (kmer hash length) when indexing the genome
        with 'smalt index' [13]
-s <int>
        The -s option (step length) when indexing the genome
        with 'smalt index' [2]
-m <int>
        The -m option when mapping reads with 'smalt map' [not used by default]
-y <float>
        The -y option when mapping reads with 'smalt map'.
        The default of 0.5 means that at least 50% of each read must map
        perfectly. Depending on the quality of your reads, you may want to
        increase this to be more stringent (or consider using -m) [0.5]
-x
        Use this to just print the commands that will be run, instead of
        actually running them
-u <int>
        The -u option of 'smalt sample'. This is used to estimate the insert
        size from a sample of the reads, by mapping every n^th read pair [1000]
        
正式跑mapping
 /home/xuxm/software/Reapr_1.0.16/reapr smaltmap A16_complete_new.fa.fa A16R_split_1.fastq A16R_split_2.fastq A16R_complete.bam 正常跑上了

      

/home/xuxm/software/Reapr_1.0.16/reapr pipeline A16_complete_new.fa.fa A16R_complete.bam A16R_complete_reapr
pipeline运行过程
 /home/xuxm/software/Reapr_1.0.16/reapr pip
eline A16R_ref_gapclose_1_circled_checked.fa.fa A16r_mapping.bam A16_checkedby_reapr
Running reapr version 1.0.16 pipeline:
/home/xuxm/software/Reapr_1.0.16/reapr pipeline A16R_ref_gapclose_1_circled_checked.fa.fa A16r_mapping.bam A16_checkedby_reapr
[REAPR pipeline] Running facheck
[REAPR pipeline] Running preprocess
[REAPR preprocess] sampling region A16R_whole_genome_1:3096-1003095.  Already sampled 0 bases
[REAPR preprocess] Sampled 1000000 bases for GC/coverage estimation
[REAPR pipeline] Running stats
[REAPR pipeline] Running fcdrate
[REAPR pipeline] Running score
[REAPR pipeline] Running break
[REAPR pipeline] Running summary



perl /data2/tongyg/software/GapFiller_v1-11_linux-x86_64/GapFiller.pl -l lib.txt -s A16R_with_small_gap.fasta -m 30 -o 2 -r 0.7 -n 10 -d 50 -t 10  -T 3 -i 3 -b A16R_with_small_gap_filled.fa

20131128

Reapr用matepair数据检查拼接序列

先检查input文件
fa文件（拼接好的序列）
reapr facheck assembly.fa #如果报错,运行下一步
reapr facheck assembly.fa new_assembly #重新命名

fastq文件（matepair reads)
smalt check *.fastq

分两步先把matepair reads map到拼接序列上生成bam文件，然后再做分析
reapr smaltmap assembly.fa long_1.fq long_2.fq long_mapped.bam
reapr pipeline assembly.fa long_mapped.bam output_directory

20131127
perl /data2/tongyg/software/GapFiller_v1-11_linux-x86_64/GapFiller.pl -l lib.txt -s pxo1_withgap.fasta -m 30 -o 2 -r 0.7 -n 10 -d 50 -t 10  -T 3 -i 3 -b pxo1_gap_filled
perl /data2/tongyg/software/GapFiller_v1-11_linux-x86_64/GapFiller.pl -l lib.txt -s pxo1_f_e_2.fasta -m 30 -o 2 -r 0.7 -n 10 -d 50 -t 10  -T 3 -i 3 -b pxo1_gap_filled_2

gapcloser
perl gapcloser_newbler

perl gapclose_newbler.pl 1gap_gapcloser_4.fa 108WT_pe.fasta out.fa 5000 0
for k in {1..50}; do perl gapclose_newbler_pack.pl temp/split_$k  temp/ctg_3and5r_ends  & done


gapfiller
perl /data2/tongyg/software/GapFiller_v1-11_linux-x86_64/GapFiller.pl -l libraries.txt -s 108WT_fix_linked.fa -m 30 -o 2 -r 0.7 -n 10 -d 500 -t 10  -T 10 -i 1 -b 108WT_fix_linked_f.fa
perl /data2/tongyg/software/GapFiller_v1-11_linux-x86_64/GapFiller.pl -l lib.txt -s pxo1_withgap.fasta -m 30 -o 2 -r 0.7 -n 10 -d 500 -t 10  -T 3 -i 1 -b pxo1_gap_filled.fa
perl /data2/tongyg/software/GapFiller_v1-11_linux-x86_64/GapFiller.pl -l lib.txt -s pxo1_withgap.fasta -m 30 -o 2 -r 0.7 -n 10 -d 500 -t 10  -T 3 -i 1 -b pxo1_gap_filled.fa



查看版本
linux:/vol3/home/liss # cat /proc/version 
Linux version 2.6.16.60-0.23.PTF.403865.0-default (geeko@buildhost) (gcc version 4.1.2 20070115 (SUSE Linux)) #1 SMP Thu May 15 06:38:31 UTC 2008

yxy COG号
rast 最后一个tab文件
他们服务器上的blast_database进行blastp
结束后去cog库搜索cog号


blast2go
本地建库blastp后要去blast2go做导入file-

做mapping，本地联网打开blast2go, 改blast2go的ip为192.168.1.5，改自己机子ip：192.168.1.18

/vol1/home/tongyg/blast-2.2.22-ia64/bin/blastall -p blastp -d /vol1/home/zhangzhiyi/azeem/blast/human.protein.faa -i /vol1/home/zhangzhiyi/azeem/blast/Agy99_protein.fasta -o Mulcerans.m9v1.blast -m 9 -e 0.005 -v 1 &
/vol1/home/tongyg/blast-2.2.22-ia64/bin/blastall -p blastp -d library_16.fa -o /vol1/home/tongyg/raw-seq/phage-IME10/aa.blast -m 9 -v 10 -b 0 &

/vol1/home/tongyg/blast-2.2.22-ia64/bin/blastall -p blastp -d library_21.fa -i cds_edit.fa -o mm_21cds_blastp.xml -m 7 -v 1 &


runAssembly -o mate -cpu 7 pe_1.fastq pe_2.fastq se.fastq  (可以直接将二者混一起跑）

有细菌库的路径

/vol1/home/zhangzhiyi/NCBI/nt/nt2012/sorted/PICK

20130828 blast建库
/vol1/home/tongyg/blast-2.2.22-ia64/bin/formatdb -p T -i library_16.fa

/vol1/home/tongyg/blast-2.2.22-ia64/bin/formatdb --help

formatdb 2.2.22   arguments:

  -t  Title for database file [String]  Optional
  -i  Input file(s) for formatting [File In]  Optional
  -l  Logfile name: [File Out]  Optional
    default = formatdb.log
  -p  Type of file
         T - protein   
         F - nucleotide [T/F]  Optional
    default = T
  -o  Parse options
         T - True: Parse SeqId and create indexes.
         F - False: Do not parse SeqId. Do not create indexes.
 [T/F]  Optional
    default = F
  -a  Input file is database in ASN.1 format (otherwise FASTA is expected)
         T - True, 
         F - False.
 [T/F]  Optional
    default = F
  -b  ASN.1 database in binary mode
         T - binary, 
         F - text mode.
 [T/F]  Optional
    default = F
  -e  Input is a Seq-entry [T/F]  Optional
    default = F
  -n  Base name for BLAST files [String]  Optional
  -v  Database volume size in millions of letters [Integer]  Optional
    default = 4000
  -s  Create indexes limited only to accessions - sparse [T/F]  Optional
    default = F
  -V  Verbose: check for non-unique string ids in the database [T/F]  Optional
    default = F
  -L  Create an alias file with this name
        use the gifile arg (below) if set to calculate db size
        use the BLAST db specified with -i (above) [File Out]  Optional
  -F  Gifile (file containing list of gi's) [File In]  Optional
  -B  Binary Gifile produced from the Gifile specified above [File Out]  Optional
  -T  Taxid file to set the taxonomy ids in ASN.1 deflines [File In]  Optional
blast成功
/vol1/home/tongyg/blast-2.2.22-ia64/bin/blastall -p blastn -d morganella_mor.fa（即将上一步的-i变为-d，不需要变名字） -i mm1_circle_final.fa -o mm_tax581.blast -K 1 -v 5 -b 5 -e 1E-5 -a 25 &


 fq2fa格式转换(每隔4行取前2行，将行首@变成>)
awk 'NR % 4 < 3 '  IonXpress_015.fastq | awk 'NR % 3 > 0 ' | perl -p -e 's/^@/>/g' > IonXpress_015.fasta
命令行拼接454 newbler
/opt/454/bin/runAssembly IonXpress_015.fasta
显示行数
wc -l IonXpress_015.fastq
文件切割，打印前500k行 (三人之一reads)
awk 'NR < 500001' IonXpress_015.fastq > one-third.fq
/opt/454/bin/runAssembly one-third.fq
 
 命令行直接拼接mate-pair数据
 runAssembly -o mate -cpu 7 pe_1.fastq pe_2.fastq 

20130826
/vol1/home/tongyg/blast-2.2.22-ia64/bin/blastall -p blastn -d morganella_mor.fa -i mm1_circle_final.fa -o mm_tax581.blast -K 1 -v 5 -b 5 -e 1E-5 -a 25 &
>>/vol1/home/tongyg/blast-2.2.22-ia64/bin/blastall -p blastn -d /vol1/home/tongyg/database/nt -i /vol3/home/liss/mm/mm1_circle_final.fa -o mm.blast -e 1e-10 -v 5 -b 5 &
>/vol2/biodb
/vol1/home/tongyg/blast-2.2.22-ia64/bin/blastall -p blastn -d /vol1/home/tongyg/database/nt/nt -i /vol3/home/liss/mm/mm1_circle_final.fa -o mm.blast -e 1e-10 -v 5 -b 5 &

20130725
grep -v 'Unmapped' 454ReadStatus_2.txt > m150_2_mapped.fa                                                              
cut -f 1  m150_2_mapped.fa >  m150_2_mapped_cut.fa                                           
sort m150_2_mapped_cut.fa > m150_2_mapped_cut_sotted.fa                                      
perl getreads_index.pl m150_9_second.fasta m150_2_mapped_cut_sotted.fa > m150_9_indexed.fasta
cut -b -30 m150_shuffle_index.fasta |sort |uniq -c |sort -g -r -o m150_shuffle_se_termini.fa                                                                                             







20130701
末端分析
cut -b -30 TM4.fasta |sort |uniq -c |sort -g -r -o TM4_termini.fa
将绿脓两次数据合并
 perl shuffleSequences_fastq.pl IonXpress_016_R_2012_07_17_21_02_17_user_20130517-R2-P1-P2a6p_20130517-R2-P1-P2a6p_56.fastq IonXpress_016_R_2012_07_20_11_12_00_user_20130530-2-3-pa26p1_20130530-2-3-pa26p1_58.fastq pa26p1.fastq 
 后来手动合并
  perl fq_all2std.pl fq2fa pa26phebing.fastq > pa26phebing.fasta
cut -b -30 R_2012_06_08_21_56_18_user_20120627-2_Auto_20120627-2_4.fasta |sort |uniq -c |sort -g -r -o AB2_termini.fa
20130630
20130626
用上面相同的方法进行muave知道顺序后的3,18间gap确定
取3，18scaf两端3_end,3_start，18_end,18_start
gsMapping3_end,3_start，18_end,18_start +all-pe.fq)

grep '_end' 454ReadStatus.txt | grep '+' | cut -f 1 > all_end_f.list
grep '_start'  454ReadStatus.txt | grep '-' | cut -f 1 > all_start_r.list


grep '\/1' all_end_f.list > all_end_f_1.list
grep '\/2' all_end_f.list > all_end_f_2.list
grep '\/1' all_start_r.list > all_start_r_1.list
grep '\/2' all_start_r.list > all_start_r_2.list

在此/1变/2，/2变/1得到4个文件，加原文件
将以上8个文件1合一起2合一起的两个文件

去掉/1, /2
sed 's/\/1//g' 1_4he.fa > 1_4he.list
sed 's/\/2//g' 2_4he.fa > 2_4he.list
read名字排序
sort 1_4he.list > 1_4he_sort.list
sort 2_4he.list > 2_4he_sort.list
行尾增加/1, /2
sed 's/$/\/1/g' 1_4he_sort.list > 1_4he_same_1.fa
sed 's/$/\/2/g' 2_4he_sort.list > 2_4he_same_2.fa

 提取mate-pair
perl getreads_index.pl bar_shuffle_2.fasta 1_4he_same_1.fa > 1_4he_same_1_reads.fa 
perl getreads_index.pl bar_shuffle_2.fasta 2_4he_same_2.fa > 2_4he_same_2_reads.fa 
fa2fq
perl fq_all2std.pl fa2std 1_4he_same_1_reads.fa > 1_4he_same_1_reads.fq
perl fq_all2std.pl fa2std 2_4he_same_2_reads.fa > 2_4he_same_2_reads.fq

得到gapside变长的片段再走一次程序
将补上gap的scafold1,2变为对应的1_end ,2_start
gsMapping(1_end ,2_start +all-pe.fq)
grep '1_end' 454ReadStatus.txt | grep '+' | cut -f 1 > 1_end_f.list
grep '2_start'  454ReadStatus.txt | grep '-' | cut -f 1 > 2_start_r.list

即合并一下三个步奏
（grep '1_end' 454ReadStatus.txt > 1_end.fa
grep '2_start'  454ReadStatus.txt > 2_start.fa
grep '+' 1_end.fa > 1_end_f.fa
grep '-' 2_start.fa >2_start_r.fa

cut -f 1 1_end_f.fa > 1_end_f.list
cut -f 1 2_start_r.fa > 2_start_r.list）


grep '\/1' 1_end_f.list > 1_end_f_1.list
grep '\/2' 1_end_f.list > 1_end_f_2.list
grep '\/1' 2_start_r.list > 2_start_r_1.list
grep '\/2' 2_start_r.list > 2_start_r_2.list

在此/1变/2，/2变/1得到4个文件，加原文件
将以上8个文件1合一起2合一起的两个文件

去掉/1, /2
sed 's/\/1//g' 1_4he.fa > 1_4he.list
sed 's/\/2//g' 2_4he.fa > 2_4he.list
read名字排序
sort 1_4he.list > 1_4he_sort.list
sort 2_4he.list > 2_4he_sort.list
提取相同的reads(其实就是找mate-pair配对的reads)
comm -1 -2 1_4he_sort.list 2_4he_sort.list > 1_he_2_he_same.fa

行尾增加/1, /2
sed 's/$/\/1/g' 1_he_2_he_same.fa > 1_he_2_he_same_1.fa
sed 's/$/\/2/g' 1_he_2_he_same.fa > 1_he_2_he_same_2.fa

 提取mate-pair
perl getreads_index.pl bar_shuffle_2.fasta 1_he_2_he_same_1.fa > 1_he_2_he_same_1_reads.fa 
perl getreads_index.pl bar_shuffle_2.fasta 1_he_2_he_same_2.fa > 1_he_2_he_same_2_reads.fa 
fa2fq
perl fq_all2std.pl fa2std 1_he_2_he_same_1_reads.fa > 1_he_2_he_same_1_reads.fq
perl fq_all2std.pl fa2std 1_he_2_he_same_2_reads.fa > 1_he_2_he_same_2_reads.fq
20130627SA1,SA2
20号机器上，(不能用)
截取bam文件前1000行。
samtools view -h IonXpress_016_R_2012_06_23_08_10_13_user_20130131-solid-3-4-5-ime-sa2_20130131-solid-3-4-5-ime-sa2_29.basecaller.bam | head -1000 |samtools view -bS - > NewIonXpress.bam
sff2bam [-c] [-o out.bam] in.sff [in2.sff ...]
bam2sff [-o out.sff] in.bam
将
ion上直接打sff2bam即可出如下信息
sff2bam - Converts sff file(s) to unmapped bam file.
Usage: 
  sff2bam [-c] [-o out.bam] in.sff [in2.sff ...]
Options:
  -h,--help              this message
  -c,--combine-sffs      combine all input sff files to a single bam file
                         all sffs must have same flow orders and same keys.
  -o,--out-filename      specify output file name

ion上
sff2bam回车即可有信息
sff2bam -o SA1.bam 454Reads.MID014D.sff 
#'sumtool' from package 'mtd-utils' (universe)
 Command 'samtools' from package 'samtools' (universe
 
samtools view -h SA2_2.bam | head - 1000 |samtools view -bS - > SA2_3.bam
samtools view -h可看view参数
Usage:   samtools view [options] <in.bam>|<in.sam> [region1 [...]]

Options: -b       output BAM
         -h       print header for the SAM output
         -H       print header only (no alignments)
         -S       input is SAM
         -u       uncompressed BAM output (force -b)
         -1       fast compression (force -b)
         -x       output FLAG in HEX (samtools-C specific)
         -X       output FLAG in string (samtools-C specific)
         -c       print only the count of matching records
         -L FILE  output alignments overlapping the input BED FILE [null]
         -t FILE  list of reference names and lengths (force -S) [null]
         -T FILE  reference sequence file (force -S) [null]
         -o FILE  output file name [stdout]
         -R FILE  list of read groups to be outputted [null]
         -f INT   required flag, 0 for unset [0]
         -F INT   filtering flag, 0 for unset [0]
         -q INT   minimum mapping quality [0]
         -l STR   only output reads in library STR [null]
         -r STR   only output reads in read group STR [null]
         -s FLOAT fraction of templates to subsample; integer part as seed [-1]
         -?       longer help
SFFRandom -n 280000 -o SA1_10.sff 454Reads.MID014D.sff 
bam2sff -o SA2_2.sff SA2_2.bam
] no @SQ lines in the header.
[sam_read1] missing header? Abort


20130626
用上面相同的方法进行28全部关联
取28scaf两端28_end,28_start

gsMapping28_end ,28_start +all-pe.fq)
grep '_end' 454ReadStatus.txt | grep '+' | cut -f 1 > all_end_f.list
grep '_start'  454ReadStatus.txt | grep '-' | cut -f 1 > all_start_r.list

即合并一下三个步奏
（grep '_end' 454ReadStatus.txt > all_end.fa
grep '_start'  454ReadStatus.txt > all_start.fa
grep '+' all_end.fa > all_end_f.fa
grep '-' 2_start.fa >2_start_r.fa

cut -f 1 1_end_f.fa > 1_end_f.list
cut -f 1 2_start_r.fa > 2_start_r.list）


grep '\/1' all_end_f.list > all_end_f_1.list
grep '\/2' all_end_f.list > all_end_f_2.list
grep '\/1' all_start_r.list > all_start_r_1.list
grep '\/2' all_start_r.list > all_start_r_2.list

在此/1变/2，/2变/1得到4个文件，加原文件
将以上8个文件1合一起2合一起的两个文件

去掉/1, /2
sed 's/\/1//g' 28_1_4he.fa > 28_1_4he.list
sed 's/\/2//g' 28_2_4he.fa > 28_2_4he.list
read名字排序
sort 28_1_4he.list > 28_1_4he_sort.list
sort 28_2_4he.list > 28_2_4he_sort.list
提取相同的reads(其实就是找mate-pair配对的reads)
comm -1 -2 28_1_4he_sort.list 28_2_4he.list > 28_1_he_2_he_same.fa

行尾增加/1, /2
sed 's/$/\/1/g' 28_1_he_2_he_same.fa > 28_1_he_2_he_same_1.fa
sed 's/$/\/2/g' 28_1_he_2_he_same.fa > 28_1_he_2_he_same_2.fa

 提取mate-pair
perl getreads_index.pl bar_shuffle_2.fasta 28_1_4he.fa > 28_1_4he_same_1_reads.fa 
perl getreads_index.pl bar_shuffle_2.fasta 28_2_4he.fa > 28_2_4he_same_2_reads.fa 
fa2fq
perl fq_all2std.pl fa2std 28_1_4he_same_1_reads.fa > 28_1_4he_same_1_reads.fq
perl fq_all2std.pl fa2std 28_2_4he_same_2_reads.fa > 28_2_4he_same_2_reads.fq

得到gapside变长的片段再走一次程序
将补上gap的scafold1,2变为对应的1_end ,2_start
gsMapping(1_end ,2_start +all-pe.fq)
grep '1_end' 454ReadStatus.txt | grep '+' | cut -f 1 > 1_end_f.list
grep '2_start'  454ReadStatus.txt | grep '-' | cut -f 1 > 2_start_r.list

即合并一下三个步奏
（grep '1_end' 454ReadStatus.txt > 1_end.fa
grep '2_start'  454ReadStatus.txt > 2_start.fa
grep '+' 1_end.fa > 1_end_f.fa
grep '-' 2_start.fa >2_start_r.fa

cut -f 1 1_end_f.fa > 1_end_f.list
cut -f 1 2_start_r.fa > 2_start_r.list）


grep '\/1' 1_end_f.list > 1_end_f_1.list
grep '\/2' 1_end_f.list > 1_end_f_2.list
grep '\/1' 2_start_r.list > 2_start_r_1.list
grep '\/2' 2_start_r.list > 2_start_r_2.list

在此/1变/2，/2变/1得到4个文件，加原文件
将以上8个文件1合一起2合一起的两个文件

去掉/1, /2
sed 's/\/1//g' 1_4he.fa > 1_4he.list
sed 's/\/2//g' 2_4he.fa > 2_4he.list
read名字排序
sort 1_4he.list > 1_4he_sort.list
sort 2_4he.list > 2_4he_sort.list
提取相同的reads(其实就是找mate-pair配对的reads)
comm -1 -2 1_4he_sort.list 2_4he_sort.list > 1_he_2_he_same.fa

行尾增加/1, /2
sed 's/$/\/1/g' 1_he_2_he_same.fa > 1_he_2_he_same_1.fa
sed 's/$/\/2/g' 1_he_2_he_same.fa > 1_he_2_he_same_2.fa

 提取mate-pair
perl getreads_index.pl bar_shuffle_2.fasta 1_he_2_he_same_1.fa > 1_he_2_he_same_1_reads.fa 
perl getreads_index.pl bar_shuffle_2.fasta 1_he_2_he_same_2.fa > 1_he_2_he_same_2_reads.fa 
fa2fq
perl fq_all2std.pl fa2std 1_he_2_he_same_1_reads.fa > 1_he_2_he_same_1_reads.fq
perl fq_all2std.pl fa2std 1_he_2_he_same_2_reads.fa > 1_he_2_he_same_2_reads.fq


20130624
将SE.PE合在一个文件里
cat bar_shuffle.fa lihao_shortgun.fa >> se_pe.fa
python fill_gap_with_mp.py scaffold_name 5000 reads.fa
cat split_pe.pl可以看整个文件
20130624找scaffold间的序列


取scaf1,2间的末端1_end ,2_start
gsMapping(1_end ,2_start +all-pe.fq)
grep '1_end' 454ReadStatus.txt | grep '+' | cut -f 1 > 1_end_f.list
grep '2_start'  454ReadStatus.txt | grep '-' | cut -f 1 > 2_start_r.list

即合并一下三个步奏
（grep '1_end' 454ReadStatus.txt > 1_end.fa
grep '2_start'  454ReadStatus.txt > 2_start.fa
grep '+' 1_end.fa > 1_end_f.fa
grep '-' 2_start.fa >2_start_r.fa

cut -f 1 1_end_f.fa > 1_end_f.list
cut -f 1 2_start_r.fa > 2_start_r.list）


grep '\/1' 1_end_f.list > 1_end_f_1.list
grep '\/2' 1_end_f.list > 1_end_f_2.list
grep '\/1' 2_start_r.list > 2_start_r_1.list
grep '\/2' 2_start_r.list > 2_start_r_2.list

在此/1变/2，/2变/1得到4个文件，加原文件
将以上8个文件1合一起2合一起的两个文件

去掉/1, /2
sed 's/\/1//g' 1_4he.fa > 1_4he.list
sed 's/\/2//g' 2_4he.fa > 2_4he.list
read名字排序
sort 1_4he.list > 1_4he_sort.list
sort 2_4he.list > 2_4he_sort.list
提取相同的reads(其实就是找mate-pair配对的reads)
comm -1 -2 1_4he_sort.list 2_4he_sort.list > 1_he_2_he_same.fa

行尾增加/1, /2
sed 's/$/\/1/g' 1_he_2_he_same.fa > 1_he_2_he_same_1.fa
sed 's/$/\/2/g' 1_he_2_he_same.fa > 1_he_2_he_same_2.fa

 提取mate-pair
perl getreads_index.pl bar_shuffle_2.fasta 1_he_2_he_same_1.fa > 1_he_2_he_same_1_reads.fa 
perl getreads_index.pl bar_shuffle_2.fasta 1_he_2_he_same_2.fa > 1_he_2_he_same_2_reads.fa 
fa2fq
perl fq_all2std.pl fa2std 1_he_2_he_same_1_reads.fa > 1_he_2_he_same_1_reads.fq
perl fq_all2std.pl fa2std 1_he_2_he_same_2_reads.fa > 1_he_2_he_same_2_reads.fq

得到gapside变长的片段再走一次程序
将补上gap的scafold1,2变为对应的1_end ,2_start
gsMapping(1_end ,2_start +all-pe.fq)
grep '1_end' 454ReadStatus.txt | grep '+' | cut -f 1 > 1_end_f.list
grep '2_start'  454ReadStatus.txt | grep '-' | cut -f 1 > 2_start_r.list

即合并一下三个步奏
（grep '1_end' 454ReadStatus.txt > 1_end.fa
grep '2_start'  454ReadStatus.txt > 2_start.fa
grep '+' 1_end.fa > 1_end_f.fa
grep '-' 2_start.fa >2_start_r.fa

cut -f 1 1_end_f.fa > 1_end_f.list
cut -f 1 2_start_r.fa > 2_start_r.list）


grep '\/1' 1_end_f.list > 1_end_f_1.list
grep '\/2' 1_end_f.list > 1_end_f_2.list
grep '\/1' 2_start_r.list > 2_start_r_1.list
grep '\/2' 2_start_r.list > 2_start_r_2.list

在此/1变/2，/2变/1得到4个文件，加原文件
将以上8个文件1合一起2合一起的两个文件

去掉/1, /2
sed 's/\/1//g' 1_4he.fa > 1_4he.list
sed 's/\/2//g' 2_4he.fa > 2_4he.list
read名字排序
sort 1_4he.list > 1_4he_sort.list
sort 2_4he.list > 2_4he_sort.list
提取相同的reads(其实就是找mate-pair配对的reads)
comm -1 -2 1_4he_sort.list 2_4he_sort.list > 1_he_2_he_same.fa

行尾增加/1, /2
sed 's/$/\/1/g' 1_he_2_he_same.fa > 1_he_2_he_same_1.fa
sed 's/$/\/2/g' 1_he_2_he_same.fa > 1_he_2_he_same_2.fa

 提取mate-pair
perl getreads_index.pl bar_shuffle_2.fasta 1_he_2_he_same_1.fa > 1_he_2_he_same_1_reads.fa 
perl getreads_index.pl bar_shuffle_2.fasta 1_he_2_he_same_2.fa > 1_he_2_he_same_2_reads.fa 
fa2fq
perl fq_all2std.pl fa2std 1_he_2_he_same_1_reads.fa > 1_he_2_he_same_1_reads.fq
perl fq_all2std.pl fa2std 1_he_2_he_same_2_reads.fa > 1_he_2_he_same_2_reads.fq


20130608 
从ion上拖到这个目录后解压
R_2012_06_24_03_44_01_user_2013-02-27-9-10-11-ime-sa2_2013-02-27-9-10-11-ime-sa2_30.barcode.sff.zip
unzip R_2012_06_24_03_44_01_user_2013-02-27-9-10-11-ime-sa2_2013-02-27-9-10-11-ime-sa2_30.barcode.sff.zip

用sff_extract#对sff_extract程序进行了一定程度的修改，增加了以下功能：
#1、	能够输出干净的fastq序列，（-c），头尾均去掉了小写字母的序列和对应的质量参数。
#2、	能够对fastq序列进行统计，得出其平均长度，并且具有扩展性，能简单地加载各种模块统计参数
#3、	能够根据参数剪切长度过小的Reads（--remove_short），并能给出被去掉的Reads的数量。
#4、	去掉左侧接头 --min_left_clip

python sff_extract_0_2_13_modified.py -Q IonXpress_010_R_2012_06_24_03_44_01_user_2013-02-27-9-10-11-ime-sa2_2013-02-27-9-10-11-ime-sa2_30.sff -o ime-sa2_18   -c --remove_short 30 --min_left_clip 18
格式转换fq2fa
perl /vol1/home/tongyg/tools/fq_all2std.pl fq2fa IonXpress_010_R_2012_06_24_03_44_01_user_2013-02-27-9-10-11-ime-sa2_2013-02-27-9-10-11-ime-sa2_30.sff.fastq > SA2.fasta
取前30bp进行排序
cut -b -30 SA2_18.fq.fasta |sort |uniq -c |sort -g -r -o SA2-termini-2convert.fa




方案2
直接用分出来的fastq格式，利用awk命令
每隔4行提取第2行得到序列
awk 'NR % 4 == 1'
再cut -b -30 SA2.fasta |sort |uniq -c |sort -g -r -o IME09_DA_30bp.stat
即一条命令
awk 'NR % 4 == 2' sff.fastq|cut -b -30 |sort |uniq -c |sort -g -r -o SA2-termini-awk.fa


5028c提取mate-pair reads
4700上
grep -v 'Unmapped' 454ReadStatus.txt > 5028c_mp-mapped.txt  
cut -f 1 5028c_mp-mapped.txt > 5028c_mp-mapped.list
grep '\/1' 5028c_mp-mapped.list > 5028c_mp-mapped_1.list
grep '\/2' 5028c_mp-mapped.list > 5028c_mp-mapped_2.list
去掉/1, /2
sed 's/\/1//g' 5028c_mp-mapped_1.list > 5028c_mp-mapped_1a.list
sed 's/\/2//g' 5028c_mp-mapped_2.list > 5028c_mp-mapped_2a.list
read名字排序
sort 5028c_mp-mapped_1a.list > 5028c_mp-mapped_1a_sort.list
sort 5028c_mp-mapped_2a.list > 5028c_mp-mapped_2a_sort.list
提取相同的reads
comm -1 -2 5028c_mp-mapped_1a_sort.list 5028c_mp-mapped_2a_sort.list > 5028c_mp-mapped_comm.list
行尾增加/1, /2
sed 's/$/\/1/g' 5028c_mp-mapped_comm.list > 5028c_mp-mapped_comm_1.list
sed 's/$/\/2/g' 5028c_mp-mapped_comm.list > 5028c_mp-mapped_comm_2.list
 perl /vol1/home/tongyg/tools/fq_all2std.pl fq2fa barcode48.fastq >  barcode48.fasta
 提取mate-pair
perl /vol1/home/tongyg/tools/getreads_index.pl barcode48.fasta 5028c_mp-mapped_comm_1.list > mp-mapped_pe_1.fa  (index时就应提取用
后分别提取的，不然没有反向）
perl /vol1/home/tongyg/tools/getreads_index.pl barcode48.fasta 5028c_mp-mapped_comm_2.list > mp-mapped_pe_2.fa 
fa2fq
 perl /vol1/home/tongyg/tools/fq_all2std.pl fa2std mp-mapped_pe_1.fasta > fa2std mp-mapped_pe_1.fq
 perl /vol1/home/tongyg/tools/fq_all2std.pl fa2std mp-mapped_pe_2.fasta > mp-mapped_pe_2.fq

perl /vol1/home/tongyg/tools/shuffleSequences_fasta.pl  mp-mapped_pe_1.fasta mp-mapped_pe_2.fasta file3
 
 
 
Usage:  runAssembly [-o projdir] [-nrm] [-p (sfffile | [regionlist:]analysisDir)]... (sfffile | [regionlist:]analysisDir)...





/vol1/home/tongyg/tools/fq_all2std.pl
python ionMP_split.py  m150-unmapped.fq m150-unmapped123
20130607、
python fill_gap_with_mp.py scaffold_name 5000 reads.fa
python fill_gap_with_mp.py 454Scaffolds.fna 5000 bar-pe.fastq


20130607
python /vol1/home/liss/yy/sff_extract_0_2_13_modified.py -Q IonXpress_010_R_2012_06_24_03_44_01_user_2013-02-27-9-10-11-ime-sa2_2013-02-27-9-10-11-ime-sa2_30.sff -o IonXpress_010_R_2012_06_24_03_44_01_user_2013-02-27-9-10-11-ime-sa2_2013-02-27-9-10-11-ime-sa2_30   -c --remove_short 30 --min_left_clip 19



python /results/analysis/output/Home/tools/sff_extract_0_2_13_modified.py -Q 454Reads.MID004R.sff -o 454Reads.MID004R   -c --remove_short 30 --min_left_clip 23      #-Q: output fq format, -c:cut lower case base and quality value
perl /vol1/home/tongyg/tools/fq_all2std.pl fq2fa R_2011_06_30_13_39_04_user_SN1-8_Auto_SN1-8_8.fastq > R_2011_06_30_13_39_04_user_SN1-8_Auto_SN1-8_8.fasta
去掉长度小于40bp的reads
先变为一行
/vol1/home/tongyg/fastx_toolkit-0.0.13/src/fasta_formatter/fasta_formatter -w 0 -i SA1.fasta -o SA1-formated.fa

/vol1/home/tongyg/fastx_toolkit-0.0.13/src/fastx_clipper/fastx_clipper -l 40 -i SA1-formated.fa -o SA1.fa

cut -b -30 SA2.fasta |sort |uniq -c |sort -g -r -o SA2-termini.fa
cut -b -30 SA1.fa |sort |uniq -c |sort -g -r -o SA1-termini.fa
20上
perl extend_end.pl SA2-Q.fa SA2-2th.fasta 20 50 SA2-ex.fa
blastall参数
1、BLASTP是蛋白序列到蛋白库中的一种查询。库中存在的每条已知序列将逐一地同每条所查序列作一对一的序列比对。
2、BLASTX是核酸序列到蛋白库中的一种查询。先将核酸序列翻译成蛋白序列（一条核酸序列会被翻译成可能的六条蛋白），再对每一条作一对一的蛋白序列比对。
3、BLASTN是核酸序列到核酸库中的一种查询。库中存在的每条已知序列都将同所查序列作一对一地核酸序列比对。
4、TBLASTN是蛋白序列到核酸库中的一种查询。与BLASTX相反，它是将库中的核酸序列翻译成蛋白序列，再同所查序列作蛋白与蛋白的比对。
5、TBLASTX是核酸序列到核酸库中的一种查询。此种查询将库中的核酸序列和所查的核酸序列都翻译成蛋白（每条核酸序列会产生6条可能的蛋白序列），这样每次比对会产生36种比对阵列。
bl2seq参数
  -F  Filter query sequence (DUST with blastn, SEG with others) [String]
    default = T
  -m  Use Mega Blast for search [T/F]  Optional
    default = F (寻找大片段重复：-m T)
4700上
bl2seq -m T -i SA1-circle.fa -j SA1-circle.fa -p blastn -F F -o SA1-repeat.fa
bl2seq -m T -i SA2-circle.fa -j SA2-circle.fa -p blastn -F F -o SA2-repeat.fa


20130605
手工补gap

取末端1_end ,2_start
gsMapping(1_end ,2_start +all-pe.fq)
grep '1_end' 454ReadStatus.txt > 1_end.fa
grep '2_start'  454ReadStatus.txt > 2_start.fa
grep '+' 1_end.fa > 1_end_f.fa
grep '-' 2_start.fa >2_start_r.fa

Cut -f 1 1_end_f.fa > 1_end_f.list
cut -f 1 2_start_r.fa > 2_start_r.list

grep '\/1' 1_end_f.list > 1_end_f_1.list
grep '\/2' 1_end_f.list > 1_end_f_2.list
grep '\/1' 2_start_r.list > 2_start_r_1.list
grep '\/2' 2_start_r.list > 2_start_r_2.list

在此/1变/2，/2变/1得到8个文件
将以上8个文件1合一起2合一起的两个文件

去掉/1, /2
sed 's/\/1//g' 1_4he.fa > 1_4he.list
sed 's/\/2//g' 2_4he.fa > 2_4he.list
read名字排序
sort 1_4he.list > 1_4he_sort.list
sort 2_4he.list > 2_4he_sort.list
提取相同的reads
comm -1 -2 1_4he_sort.list 2_4he_sort.list > 1_he_2_he_same.fa

行尾增加/1, /2
sed 's/$/\/1/g' 1_he_2_he_same.fa > 1_he_2_he_same_1.fa
sed 's/$/\/2/g' 1_he_2_he_same.fa > 1_he_2_he_same_2.fa

 提取mate-pair
perl /home/tongyg/tools/getreads_index.pl bar-pe-all.fa 1_he_2_he_same_1.fa > 1_he_2_he_same_1_reads.fa 
perl /home/tongyg/tools/getreads_index.pl bar-pe-all.fa 1_he_2_he_same_2.fa > 1_he_2_he_same_2_reads.fa 
fa2fq
perl /home/tongyg/tools/fq_all2std.pl fa2std 1_he_2_he_same_1_reads.fa > 1_he_2_he_same_1_reads.fq
perl /home/tongyg/tools/fq_all2std.pl fa2std 1_he_2_he_same_2_reads.fa > 1_he_2_he_same_2_reads.fq


4700上
grep -v 'Unmapped' 454ReadStatus.txt > 28-scaf_mp-mapped.txt  
cut -f 1 28-scaf_mp-mapped.txt > 28-scaf_mp-mapped.list
grep '\/1' 28-scaf_mp-mapped.list > 28-scaf_mp-mapped_1.list
grep '\/2' 28-scaf_mp-mapped.list > 28-scaf_mp-mapped_2.list
去掉/1, /2
sed 's/\/1//g' 28-scaf_mp-mapped_1.list > 28-scaf_mp-mapped_1a.list
sed 's/\/2//g' 28-scaf_mp-mapped_2.list > 28-scaf_mp-mapped_2a.list
read名字排序
sort 28-scaf_mp-mapped_1a.list > 28-scaf_mp-mapped_1a_sort.list
sort 28-scaf_mp-mapped_2a.list > 28-scaf_mp-mapped_2a_sort.list
提取相同的reads
comm -1 -2 28-scaf_mp-mapped_1a_sort.list 28-scaf_mp-mapped_2a_sort.list > 28-scaf_mp-mapped_comm.list
行尾增加/1, /2
sed 's/$/\/1/g' 28-scaf_mp-mapped_comm.list > 28-scaf_mp-mapped_1.list
sed 's/$/\/2/g' 28-scaf_mp-mapped_comm.list > 28-scaf_mp-mapped_2.list
 perl /vol1/home/tongyg/tools/fq_all2std.pl fq2fa bar-pe.fastq >  bar-pe-all.fa
 提取mate-pair（#有时需要用dos2unix变换格式， 需要bioperl（20号机器没有））
perl /vol1/home/tongyg/tools/getreads_index.pl all-mp.fa mp-mapped_pe_1.list > mp-mapped_pe_1.fa  
perl /vol1/home/tongyg/tools/getreads_index.pl all-mp.fa mp-mapped_pe_2.list > mp-mapped_pe_2.fa 
fa2fq
 perl /vol1/home/tongyg/tools/fq_all2std.pl fa2std 28-scaf_mp-mapped_1.fa > 28-scaf_mp-mapped_1.fq
 perl /vol1/home/tongyg/tools/fq_all2std.pl fa2std 28-scaf_mp-mapped_2.fa >  28-scaf_mp-mapped_2.fq
 
 
 
寻找关联步骤：
1、对数据库处理
使用tea.pl程序将shuffle在一起的fa数据文件转换。配对的两条reads置于同一行，分为四列
perl -p -e 's/:/_/' <file-pe.fa >link/file.fa  将其中的：换成_号

perl link/tea.pl file-pe.fa link/file-pe-one.fa 置于同行
格式例子：ZG0N4_00040_00094/1:AGTGGGCTTGATGTAACTTTGAAAATATTTATTTAAAAAAATGTGTGAATACACAACAACAAGT:ZG0N4_00040_00094/2:CTTTTGTTGGTAAAAACGACACAGTTAATTCA

2、构建contig文件
先做reverse-complement； 将reverse-complement的序列名称改成rc；合并文件 cat 454Scaffolds-lh.fna 454rvt.fna >contigs.fa  grep '>' -c contigs.fa

contig1
contig2
contigrc1
contigrc2
```
3、寻找关联
1、初步运行 里面的searchtime设置为0。功能就是将database拆分成100个，放在temp文件夹下
运行： perl makelink.PL contigs.fa filepe-one.fa 56 5000

将里面的searchtime设置为1.否则下次运行还会拆分数据。
2、分配数据
for k in {1..100}; do perl makelink_pack01.pl temp/split_$k  temp/ctg_3_ends  temp/ctg_5r_ends ; done

3、解析数组，统计每个link上的copy数

 perl makelink.PL contigs.fa filepe-one.fa 56 5000

以后的步骤重复2和3


ION
 /home/ionadmin/454/bin/sfffile -s -o 5028c -mcf /home/ionguest/results/analysis/output/Home/20130516-mate-pair-B-h_54_061/basecaller_results/MID_CONF.parse /home/ionguest/results/analysis/output/Home/20130516-mate-pair-B-h_54_061/basecaller_results/R_2012_06_14_13_50_21_user_5028c-5047c-fenchang002-han2-2012-10-16_Auto_5028c-5047c-fenchang002-han2-2012-10-16_16.sff
 20
  cp /home/huangyong/lss/IonXpress_047_R_2012_07_16_21_26_40_user_20130516-mate-pair-B-h_20130516-mate-pair-B-h_54.sff ./
 /home/tongyg/tools> python sff_extract_0_3_0.py -l mp_link.fa -o /data2/lishasha/lh2013526/bar-ex-pe /data2/lishasha/lh2013526/IonXpress_047_R_2012_07_16_21_26_40_user_20130516-mate-pair-B-h_20130516-mate-pair-B-h_54.sff
  python ionMP_split.py  bar-ex-pe.fastq matepair-split123
拼接



20
 -c  /data2/zhangzhiyi/2013_0516_zhangjiusongM150/TEST/M150_d_results/M150_out.contig -m /data2/zhangzhiyi/2013_0516_zhangjiusongM150/TEST/M150_d_results/M150_out.mates -C /data2/zhangzhiyi/2013_0516_zhangjiusongM150/TEST/M150_d_results/M150_out.conf -o bambus
 
根用户
printscaff
/data2/software/IonTorrent/bambus-2.33/bambus/bin/goBambus -c /data2/zhangzhiyi/2013_0516_zhangjiusongM150/TEST/M150_d_results/M150_out.contig -m /data2/zhangzhiyi/2013_0516_zhangjiusongM150/TEST/M150_d_results/M150_out.mates -C /data2/zhangzhiyi/2013_0516_zhangjiusongM150/TEST/M150_d_results/M150_out.conf -o bambus

 /data2/software/IonTorrent/bambus-2.33/bambus/bin/printScaff -e /data2/zhangzhiyi/2013_0516_zhangjiusongM150/bambus/M150_out_bambus.evidence.xml -s /data2/zhangzhiyi/2013_0516_zhangjiusongM150/bambus/bambus.evidence.xml -l /data2/zhangzhiyi/2013_0516_zhangjiusongM150/bambus/bambus.lib -f /data2/zhangzhiyi/2013_0516_zhangjiusongM150/TEST/M150_d_results/M150_out.unpadded.fasta -merge -o /data2/zhangzhiyi/2013_0516_zhangjiusongM150/TEST/bambus2/M150test-bambus_scaffold -page -dot

/data2/software/IonTorrent/bambus-2.33/bambus/bin/printScaff -e bambus.evidence.xml -s bambus.out.xml -l bambus.lib -f /data2/zhangzhiyi/2013_0516_zhangjiusongM150/TEST/M150_d_results/M150_out.unpadded.fasta -merge  -o bambus_scaffold -page -dot


bambus
从第5步开始
convert_project -f caf -t ace M150_out.caf M150_out

/data2/software/IonTorrent/amos-3.1.0/bin/ace2contig -i M150_out.ace -o M150_out.contig

/data2/software/IonTorrent/bambus-2.33/bambus/bin/goBambus -c M150_out.
contig  -m M150_out.mates -C M150_out.conf -o bambus

/data2/zhangzhiyi/2013_0516_zhangjiusongM150/assembly/M150_assembly/M150_d_r
esults # /data2/software/IonTorrent/bambus-2.33/bambus/bin/printScaff -e bambus.evidence.xml -s bambus.out.xml -l bambus.lib -f M150_out.unpadded.fasta -merge -o bambus_scaffold -page -dot

20130529
4700
perl split_n.pl infile outfile
 cp /vol1/home/liss/mate-pair201357/R_2012_06
_11_19_17_00_user_SN1-10-lihao-2012.8.31_Auto_SN1-10-lihao-2012.8.31_11.fastq ./

/mate-pair201357/lhnew> perl fq_all2std.pl fq2fa R_2012_06_11_19_17_
00_user_SN1-10-lihao-2012.8.31_Auto_SN1-10-lihao-2012.8.31_11.fastq > lihao.fasta

perl extend_end.pl lhscaffold-split.fa lihao.fasta 20 50 lhscaffold-extend.fa
perl extend_end.pl lhscaffold-split.fa lh-se-half.fasta 20 50 lh-se-half-ex.fasta


perl extend_end.pl lh-se-half-ex.fasta lihao.fasta 20 50 lhscaffold-extend.fa
perl rev
/home/ionadmin/454/bin
/opt/454/bin/sfffile -s -mcf /home/ionguest/results/analysis/output/Home/20130516-mate-pair-B-h_54_061/basecaller_results/MID_CONF.parse /home/ionguest/results/analysis/output/Home/20130516-mate-pair-B-h_54_061/basecaller_results/R_2012_06_14_13_50_21_user_5028c-5047c-fenchang002-han2-2012-10-16_Auto_5028c-5047c-fenchang002-han2-2012-10-16_16.sff
 /opt/454/bin/sfffile -s -mcf mid_config_file sfffile.sff

20
python /results/analysis/output/Home/tools/sff_extract_0_2_13.py -Q /home/ionguest/results/analysis/output/Home/20130516-mate-pair-B-h_54_061/basecaller_results/IonXpress_047_R_2012_07_16_21_26_40_user_20130516-mate-pair-B-h_20130516-mate-pair-B-h_54.sff -o bar-pe.fastq          #-Q: output fq format

22上
 rm -R /home/lishasha/lh-newextract/forthpe+forth-se/sff

4700

xian
rev_com_rev.pl
perl extend_end.pl lh-se-half-ex.fasta lihao_eight.fa 20 50 lhscaffold-extend-2.fa


4700
perl rev_com_rev.pl lhscaffold-extend-8.fa lhscaffold-extend-8-rc.fa

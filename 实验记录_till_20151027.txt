library(pheatmap)

data<-read.table("231_pathway.txt",head=T,row.names=1,sep='\t')
data<-data.matrix(data)
ann_col<-read.table("ano_col_231.txt",head=T,row.names=1,sep='\t')
ann_row<-read.table("ano_row_all.txt",head=T,row.names=1,sep='\t')


annotation_col = data.frame(
Time=factor(ann_col$time),Med_property=factor(ann_col$property)
)

rownames(annotation_col)=rownames(ann_col)

annotation_row = data.frame(
Gene_pathway=factor(ann_row$pathway)
)

rownames(annotation_row) =rownames(ann_row)

pdf(file="231_pathway2.pdf",height=8,width=22)
pheatmap(data,cluster_row=TRUE,cluster_col=TRUE,show_rownames=F,show_colnames=F,color = colorRampPalette(c("blue", "white", "red"))(50),cellwidth=6,cellheight=4,fontsize_col=8,annotation_col = annotation_col,annotation_row = annotation_row)
dev.off()


library(pheatmap)

data<-read.table("hepg2_pathway.txt",head=T,row.names=1,sep='\t')
data<-data.matrix(data)
ann_col<-read.table("ano_col_hepg2.txt",head=T,row.names=1,sep='\t')
ann_row<-read.table("ano_row_all.txt",head=T,row.names=1,sep='\t')


annotation_col = data.frame(
Time=factor(ann_col$time),Med_property=factor(ann_col$property)
)

rownames(annotation_col)=rownames(ann_col)

annotation_row = data.frame(
Gene_pathway=factor(ann_row$pathway)
)

rownames(annotation_row) =rownames(ann_row)

pdf(file="test_pathway2.pdf",height=8,width=22)
pheatmap(data,cluster_row=TRUE,cluster_col=TRUE,show_rownames=F,show_colnames=F,cellwidth=6,cellheight=4,fontsize_col=8,annotation_col = annotation_col,annotation_row = annotation_row)
dev.off()

library(pheatmap)

data<-read.table("thp1_pathway_24h.txt",head=T,row.names=1,sep='\t')
data<-data.matrix(data)
ann_col<-read.table("ano_col_thp1_24h.txt",head=T,row.names=1,sep='\t')
ann_row<-read.table("ano_row_all.txt",head=T,row.names=1,sep='\t')


annotation_col = data.frame(
Time=factor(ann_col$time),Med_property=factor(ann_col$property)
)

rownames(annotation_col)=rownames(ann_col)

annotation_row = data.frame(
Gene_pathway=factor(ann_row$pathway)
)

rownames(annotation_row) =rownames(ann_row)

pdf(file="thp1_pathway_24h.pdf",height=8,width=22)
pheatmap(data,cluster_row=TRUE,cluster_col=FALSE,show_rownames=F,show_colnames=F,color = colorRampPalette(c("navy", "white", "firebrick3"))(50),cellwidth=6,cellheight=4,fontsize_col=8,annotation_col = annotation_col,annotation_row = annotation_row)
dev.off()

pheatmap改变字体
library(pheatmap)

data<-read.table("ctnnb_total_ranked.txt",head=T,row.names=1,sep='\t')
data<-data.matrix(data)
pdf(file="ctnnb_total_ranked3.pdf")
pheatmap(data,cluster_row=FALSE,cluster_col=TRUE,cellwidth=2,cellheight=2,fontsize_col=2,fontsize_row=2)
dev.off()


mydata <-read.table("total.txt",head=T)
d<-density(mydata$million)
plot(d)
polygon(d,col="red",border="blue")
rug(mydata$million,col="brown")

library(grid)                                                                                                                                                                                                                                             
library(VennDiagram)                                                                                                                                                                                                                                      
                                                                                                                                                                                                                                                          
mydata1<-read.table("/Users/lishasha/Desktop/home/new_work/20150904_7000compound/capital_bio/plot_for_shanghai/microarray/RASL_YA.txt",head=T)                                                                                                            
mydata2<-read.table("/Users/lishasha/Desktop/home/new_work/20150904_7000compound/capital_bio/plot_for_shanghai/microarray/array_YA.txt",head=T)                                                                                                           
venn.diagram(list("RASL"=mydata1$RASL,"Aarray"=mydata2$Microarray),fill=c("paleturquoise2","thistle1"),"/Users/lishasha/Desktop/home/new_work/20150904_7000compound/capital_bio/plot_for_shanghai/microarray/DY.png",alpha = c(0.6, 0.6), cex = 2,cat.fontface = 4,lty =2)

a<-matrix(1:20,nrow=4,ncol=5)
＃pip, setuptools, rpy2-2.7.1安装


  508  cd home/tools/
  526  cd pip-7.0.3/
  527  ls
  528  python setup.py install
  529  sudo python setup.py install
  530  cd ..
  531  ls
  532  unzip -d setuptools-18.4.zip 
  533  ls -t
  534  unzip setuptools-18.4.zip -d setuptools-18.4
  535  ls -t
  536  cd setuptools-18.4
  537  ls
  538  cd setuptools-18.4/
  539  ls
  540  python setup.py install
  541  sudo python setup.py install
  542  ls
  543  cd ..
  544  ls
  545  cd ..
  546  ls
  547  cd pip-7.0.3/
  548  ls
  549  ll
  550  sudo python setup.py install
  551  which python
  552  which pip
  553  which pip/
  554  vim ~/.bashrc 
  555  source ~/.bashrc 

  sudo easy_install rpy2
  
  
Windows环境下安装pip，方便你的开发
python , Python

1.在以下地址下载最新的PIP安装文件：http://pypi.python.org/pypi/pip#downloads

2.解压安装

3.下载Windows的easy installer，然后安装：http://pypi.python.org/pypi/setuptools

4.安装setuptools工具

5.命令行工具cd切换到pip的目录，找到setup.py文件，然后输入python setup.py install，运行即可（之所以能运行这步，是因为之前安装的setuptools工具，以后就可以随意安装python的库了，只要找对setup.py文件的路径，运行上述命令，就可以方便的安装了）

6.把python的安装路径添加到环境变量path中，例如G:\python2.6\Scripts

7.完成！


另：安装完pip和easy_installer工具后，以后再安装python其他库就方便了



将3列数据变成矩阵
最简单的方法，先把所有column加入元组（无重复），排个序，然后根据kegg作为键，把所有行分类作为列表的元素，然后循环字典的键，查找每个值（column）在元组里的index，然后把nes写到该位置

subset(airquality, Temp > 80, select = c(Ozone, Temp)) #选取Temp>80的列，只在Ozone, Temp 两列里面选取
subset(airquality, Day == 1, select = -Temp) #选取Day == 1的列，不选Temp 
subset(airquality, select = Ozone:wind) #在 Ozone到wind里面的列选
new<-subset(ttoal,Median>5,select=probe_name:Median)


 Use biased variances. When calculating the ranking metrics, as described in Metrics for Ranking Genes, GSEA uses an unbiased variance to calculate standard deviation. Select this option to have GSEA use a biased variance instead. By default, this option is not selected.


 Use biased variances. When calculating the ranking metrics, as described in Metrics for Ranking Genes, GSEA uses an unbiased variance to calculate standard deviation. Select this option to have GSEA use a biased variance instead. By default, this option is not selected.

   Enrichment statistic. To calculate the enrichment score, GSEA first walks down the ranked list of genes increasing a running-sum statistic when a gene is in the gene set and decreasing it when it is not. The enrichment score is the maximum deviation from zero encountered during that walk. This parameter affects the running-sum statistic used for the analysis. The last section of the Gene Set Enrichment Analysis PNAS paper shows the mathematical descriptions of the methods used in GSEA. This option controls the value of p used in the enrichment score calculation shown there:

74          classic: p=0

74          weighted (default): p=1

74          weighted_p2: p=2

74          weighted_p1.5: p=1.5
Metric for ranking genes. GSEA ranks the genes in the expression dataset and then analyzes that ranked list of genes. Use this parameter to select the metric used to score and rank the genes; use the Gene list sorting mode parameter to determine whether to sort the genes using the real (default) or absolute value of the metric score; and use the Gene list ordering mode parameter to determine whether to sort the genes in descending (default) or ascending order. For descriptions of the ranking metrics, see Metrics for Ranking Genes.

Note: The default metric for ranking genes is the signal-to-noise ratio. To use this metric, your phenotype file must define at least two categorical phenotypes and your expression dataset must contain at least three (3) samples for each phenotype. If you are using a continuous phenotype or your expression dataset contains fewer than three samples per phenotype, you must choose a different ranking metric. If your expression dataset contains only one sample, you must rank the genes and use the GSEAPreranked Page to analyze the ranked list; none of the GSEA metrics for ranking genes can be used to rank genes based on a single sample.
f your dataset contains only one sample, GSEA cannot rank the genes; however, you can rank the genes and then use the GSEAPreranked Page to analyze your ranked list of genes.

 Use median instead of mean for metrics. For categorical phenotypes, by default, GSEA calculates differential expression based on the mean expression value for each phenotype.
 For categorical phenotypes, GSEA determines a gene’s mean expression value for each phenotype and then uses one of the following metrics to calculate the gene’s differential expression with respect to the two phenotypes
 

Optionally, use the Cparam_file parameter to specify a parameter file, which can contain any parameter except Cparam_file. If you specify the same parameter on the command line and in the parameter file, the value on the command line takes precedence. A parameter file is a text file that defines one parameter per line. Each line contains a parameter name (without the initial hyphen), a tab (not spaces), and the parameter value. For example: GSEAParameters.txt.
1.       Following is a command line that might appear when you click the Command button in GSEA. To run the command from the command line, you must add the Ccp parameter. In this example, the Cgmx and Cchip parameters reference files on the GSEA ftp site. You must download these files from the GSEA web site (http://www.broadinstitute.org/gsea/downloads.jsp) and update the command line to reference the downloaded files. If necessary, quote file names that include spaces and/or remove hyphens from the file names.

java -Xmx512m xtools.gsea.Gsea 
-res \\Krypton\GSEATest\DataSets\P53_hgu95av2.gct 
-cls \\Krypton\GSEATest\DataSets\P53.cls#MUT_versus_WT 
-gmx ftp.broadinstitute.org://pub/gsea/gene_sets/c1.v2.symbols.gmt 
-chip ftp.broadinstitute.org://pub/gsea/annotations/HG_U95Av2.chip 
-collapse true -mode Max_probe -norm meandiv -nperm 1000 -permute phenotype 
-rnd_type no_balance -scoring_scheme weighted -rpt_label my_analysis 
-metric Signal2Noise -sort real -order descending -include_only_symbols true 
-make_sets true -median false -num 100 -plot_top_x 20 -rnd_seed timestamp 
-save_rnd_lists false -set_max 500 -set_min 15 -zip_report false 
-out C:\Program Files\gsea_home\dec18 -gui false

2.       Following is a command line that assumes that the identifiers in your dataset match those in your gene sets:

java C Xmx1024m -cp /xchip/projects/xtools/gsea2.jar xtools.gsea.Gsea 
-res test.gct -cls test.cls -gmx test.gmx -collapse false

3.       Following is a command line that assumes that your dataset uses HG_U133A probe identifiers and your gene sets use gene symbols, so you want to collapse your dataset:

java -Xmx1024m -cp /xchip/projects/xtools/gsea2.jar xtools.gsea.Gsea 
-res foo.gct -cls foo.cls -gmx foo.gmx 
-chip ftp.broadinstitute.org://pub/gsea/annotations/HG_U133A.chip



gsea命令行
java -Xmx512m xtools.gsea.Gsea -res /Users/lishasha/Desktop/home/new_work/20150801_3000drugs/gsea/20150812talk/10084.gct -cls /Users/lishasha/Desktop/home/new_work/20150801_3000drugs/gsea/20150812talk/10084.cls#Treat_versus_Nontreat -gmx /Users/lishasha/Desktop/home/new_work/20150801_3000drugs/gsea/20150812talk/myc.gmt -collapse false -mode Max_probe -norm meandiv -nperm 1000 -permute phenotype -rnd_type no_balance -scoring_scheme weighted -rpt_label 10084 -metric log2_Ratio_of_Classes -sort real -order descending -include_only_symbols true -make_sets true -median false -num 100 -plot_top_x 20 -rnd_seed timestamp -save_rnd_lists false -set_max 500 -set_min 0 -zip_report false -out /Users/lishasha/gsea_home/output/八月12 -gui false
gsea与我们数据相关的几点
Can I use GSEA to analyze SNP, SAGE, ChIP-Seq or RNA-Seq data?
GSEA requires as input an expression dataset, which contains expression profiles for multiple samples. While the software supports multiple input file formats for these datasets, the tab-delimited GCT format is the most common. The first column of the GCT file contains feature identifiers (genes or probes in the case of data derived from DNA microarray experiments, genes or transcripts in the case of data derived from RNA-Seq experiments). The second column contains a description of the feature; this column is ignored by GSEA. Subsequent columns contain the expression values for each feature, with one sample's expression profile per column.
It is important to note that there are no hard and fast rules regarding how a GCT file's expression values are derived. The important point is that they are comparable to one another across features within a sample and comparable to one another across samples. In the case of expression data derived from RNA-seq experiments, it is not required that the expression values be in units of FPKM. RSEM quantification produces expression estimates in several units:
expected counts
transcripts per million (TPM)
FPKM
Expression data in units of FPKM has been normalized to support comparisons of a feature's expression levels across samples, which is important for GSEA; however, there are other RNA-seq normalization methods (e.g., TMM, geometric mean) that could be applied to the expected counts prior to the incorporation of the expression estimates into a GCT expression data file.
The GSEA algorithm ranks the features listed in a GCT file. It provides a number of alternative statistics that can be used for feature ranking. But in all cases (or at least in the cases where the dataset represents expression profiles for differing categorical phenotypes) the ranking statistics capture some measure of genes' differential expression between a pair of categorical phenotypes. The GSEA team has yet to determine whether any of these ranking statistics, originally selected for their effectiveness when used with expression data derived from DNA Microarray experiments, are appropriate for use with expression data derived from RNA-seq experiments. We hopefully will be able to devote some time to investigating this, but in the mean time, we are recommending use of the GSEAPreranked tool for conducting gene set enrichment analysis of data derived from RNA-seq experiments.
In particular:
Prior to conducting gene set enrichment analysis, conduct your differential expression analysis using any of the tools developed by the bioinformatics community (e.g., cuffdiff, edgeR, DESeq, etc).
Based on your differential expression analysis, rank your features and capture your ranking in an RNK-formatted file. The ranking metric can be whatever measure of differential expression you choose from the output of your selected DE tool. For example, cuffdiff provides the (base 2) log of the fold change.
Run GSEAPreranked, but make sure to select "classic" for your enrichment score (thus, not weighting each gene's contribution to the enrichment score by the value of its ranking metric).
Please note that if you choose to use any of the gene sets available from MSigDB in your analysis, you need to make sure that the features listed in your RNK file are genes, and the genes are identified by their HUGO gene symbols. All gene symbols listed in the RNK file must be unique, and we recommend the values of the ranking metrics be unique.

Can I use GSEA to analyze a dataset that contains a single sample?
Yes.  However, GSEA has no way of ranking the genes in such a dataset. Therefore, you must rank the genes and then use GSEA to analyze the ranked list of genes. For more information, see the GSEA Preranked Page in the GSEA User Guide.
Can I use GSEA to analyze time series data?
Yes. The phenotype labels (.cls) file defines the experimental phenotypes and associates each sample in your dataset with one of those phenotypes. To analyze time course data, use a continuous phenotype label. For more information, see Phenotype Labels in the GSEA User Guide. When you run the GSEA analysis, select Pearson in the Metric for ranking genes parameter. This is the only metric that can be used with time series data.
Can I use GSEA with gene sets that have both up- and down-regulated genes?
The GSEA software does not yet support this, but you can use the enrichment statistic with gene sets that include both up- and down-regulated genes. For one approach, see Lamb, et al 2006.

The rank file is a list of detected genes and a rank metric score. At the top of the list are genes with the "strongest" up-regulation, at the bottom of the list are the genes with the "strongest" down-regulation and the genes not changing are in the middle. The metric score I like to use is the sign of the fold change multiplied by the inverse of the p-value, although there may be better methods out there (link).
#!/bin/bash
DGE=$1
RNK=`echo $DGE | sed 's/.xls/.rnk/'`
sed 1d $DGE \
| sort -k7g \
| cut -d '_' -f2- \
| awk '!arr[$1]++' \
| awk '{OFS="\t"}
{ if ($6>0) printf "%s\t%4.3e\n", $1, 1/$7 ;
else printf "%s\t%4.3e\n", $1, -1/$7 }' \
| sort -k2gr > $RNK

And here's the top and bottom 10 lines of the rank file:

$cat DEB_DESeq.rnk | (head;tail)
AC011899.9 3.352e+64
CAMK1D 1.471e+51
HSD11B1 1.919e+48
GNG4 2.279e+43
CPLX1 3.873e+40
CGNL1 1.200e+39
APCDD1 1.237e+35
MANEAL 9.303e+32
MKX 4.441e+28
HOXA13 1.007e+26
HLA-B -2.908e+132
PRAME -3.063e+165
LYZ -9.701e+185
CPXM1 -1.169e+186
HSH2D -1.658e+186
TGFBI -8.856e+210
APOC2 -9.633e+243
APOC4-APOC2 -1.984e+245
COL1A2 -1.298e+274
SLC12A3 -5.422e+304

How to generate a rank file from gene expression data
Turning a gene expression profile into a ranked list is useful for comparing with other profiling data sets as well as an input for preranked GSEA analysis (example here). In this post, I describe a simple bash script called rnkgen.sh that can take gene expression data from a range of sources, such as edgeR, DESeq, GEO2R, etc., and generate a ranked list of genes from most up-expressed to most down-expressed based on the p-value.

#!/bin/bash
#rnkgen.sh converts a differential gene expression spreadsheet (XLS) into a rank file (RNK)

#Specify the input file
XLS=$1
#Specify the gene ID column
ID=$2
#Specify the fold change value column
FC=$3
#Specify the raw p-value column
P=$4

sed 1d $XLS | tr -d '"' \
| awk -v I=$ID -v F=$FC -v P=$P '{FS="\t"} $I!="" {print $I, $F, $P}' \
| awk '$2!="NA" && $3!="NA"' \
| awk '{s=1} $2<0{s=-1} {print $1"\t"s*-1*log($3)/log(10)}' \
| awk '{print $1,$2,$2*$2}' | sort -k3gr \
| awk '{OFS="\t"} !arr[$1]++ {print $1,$2}' \
| sort -k2gr > ${XLS}.rnk

After you save the script to a file, allow it to be executed


chmod +x rnkgen.sh

The usage of the script is as follows:
./rnkgen.sh /path/to/spreadsheet.xls GeneID_column FoldChange_column P-value_column

So for a spreadsheet which looks like this one generated by Degust previously:
gene c aza FDR C1 C2 C3 A1 A2 A3
LYZ 0 -1.65809730597384 4.70051409587021e-16 17393 14675 16300 4951 5064 4841
IFI27 0 -3.40698193025161 1.01987358298544e-15 5695 5552 4630 475 532 442
SLC12A3 0 -3.46002052417193 6.78905193176251e-14 1129 1051 991 79 109 92
HLA-B 0 -1.41354726616356 6.78905193176251e-14 6678 5597 5567 2167 2281 2042
IFI6 0 -2.76026110717513 7.38087480096726e-14 5884 5044 4171 764 764 624
OAS3 0 -2.64899272132832 1.17759867975735e-13 5196 5812 4621 830 824 753
HLA-C 0 -1.39473306959746 1.18784239161695e-13 3715 3324 3382 1305 1350 1193
IGFBP5 0 -5.1043039727239 1.61487772234777e-13 1706 1282 1643 39 39 52
MX1 0 -2.84129882209861 1.61487772234777e-13 2161 2245 1909 277 339 242

So in this case we run it as follows:
./rnkgen.sh degust.xls 1 3 4 

And have a look at the top and bottom 5 lines of output:
CAMK1D 9.80546
AC011899.9 9.76077
CGNL1 9.56363
GNG4 9.42442
MKX 8.93011

IFI6 -13.1319
HLA-B -13.1682
SLC12A3 -13.1682
IFI27 -14.9915
LYZ -15.3279

We see that the result is just what we're after.


GSEA

Turning a gene expression profile into a ranked list is useful for comparing with other profiling data sets as well as an input for preranked GSEA analysis (example here). In this post, I describe a simple bash script called rnkgen.sh that can take gene expression data from a range of sources, such as edgeR, DESeq, GEO2R, etc., and generate a ranked list of genes from most up-expressed to most down-expressed based on the p-value.



1
 gravatar for lkmklsmn
10 months ago by
lkmklsmn 61 450
United States
The GSEA algorithm is based on the Kolmogorov-Smirnov statistical test. This method test for a shift in ranks between a set of interest and the background. You would basically be asking the question, is this particular set of genes enriched among the top genes in the ranked list of all genes?  

This is fairly simple to do in R. The code would look like this (not run):  

scores<- a numeric vector of your scores (prevalence of variants) of all genes in your dataset  

ranking<-rank(scores)  

ind<- a numeric vector containing the indices of your gene set in scores  

geneset<-ranking[ind]  

background<-ranking[-ind]  

ks.test(geneset,background)  

Actually it does not know which is what phenotype, since we do not provide cls file as one would do during normal GSEA, hence phenotype label is na. But, as you have mentioned, based on log fold changes it assumes those genes with positive fold changes are phenotype 1(na_pos) and those with negatives are phenotype 2 (na_neg). (I think you can change this by reversing the rank order, I am not sure) 

Ranking by significance is better than fold change; just think about those lowly expressed genes with extreme fold changes and high p-values. Checkout this NAR which discusses the difference.

The metric that you selected in the Metric for ranking genes parameter (Signal2Noise or tTest) requires that you have at least three samples for each phenotype. You have too few samples for at least one of the phenotypes selected in the Phenotype labels parameter .

To analyze a categorical phenotype that has fewer than three samples, use one of the following ranking metrics:

    Ratio_of_classes
    log2_Ratio_of_classes
    Diff_of_classes
按行操作，每个dmso被同一个数除
 Filtering based on expression values. For many analytical algorithms, such as clustering, it makes sense to preprocess a dataset. For example, before running hierarchical clustering, you might remove genes that have low variance across the dataset. This prevents flat genes from driving the clustering result and improves processing time by focusing on a smaller number of interesting genes.

The GSEA algorithm does not filter the expression dataset and does not benefit from your filtering of the expression dataset. During the analysis, genes that are poorly expressed or that have low variance across the dataset populate the middle of the ranked gene list and the use of a weighted statistic ensures that they do not contribute to a positive enrichment score. By removing such genes from your dataset, you may actually reduce the power of the statistic. Processing time is rarely a factor; GSEA can easily analyze 22,000 genes with even modest processing power.
Filtering based on expression values. For many analytical algorithms, such as clustering, it makes sense to preprocess a dataset. For example, before running hierarchical clustering, you might remove genes that have low variance across the dataset. This prevents flat genes from driving the clustering result and improves processing time by focusing on a smaller number of interesting genes.

The GSEA algorithm does not filter the expression dataset and does not benefit from your filtering of the expression dataset. During the analysis, genes that are poorly expressed or that have low variance across the dataset populate the middle of the ranked gene list and the use of a weighted statistic ensures that they do not contribute to a positive enrichment score. By removing such genes from your dataset, you may actually reduce the power of the statistic. Processing time is rarely a factor; GSEA can easily analyze 22,000 genes with even modest processing power.


total<-read.table("Lane7_plate1_D7_control.txt",head=T,row.names=1)
control<-total[1:5]
treat<-total[6:384]
for (i in 1:1273){
    control[i,]=log2(treat[i,]/control[i,])

The GSEA PNAS 2005 paper introduced a method where a running sum statistic is incremented by the absolute value of the ranking metric when a gene belongs to the set. This method has proven to be efficient and facilitates intuitive interpretation of ranking metrics that reflect correlation of gene expression with phenotype. In the case of GSEAPreranked, you should make sure that this weighted scoring scheme applies to your choice of ranking statistic. When in doubt, we recommend using a more conservative scoring approach by setting Enrichment statistic to classic. Please refer to the GSEA PNAS 2005 paper for further details.


> control<-read.table("Lane7_plate1_D7_control.txt",head=T,row.names=1)
> treat<-read.table("Lane7_plate1_D7_treat.txt",head=T,row.names=1)
for (i in 1:1273){
    control[i,]=log2(treat[i,]/control[i,])
}
write.table(control,"treatvs_control_log2.txt")

stable<-read.table("stable_1.txt",head=T,row.names=1)
total<-read.table("total_1.txt",head=T,row.names=1)
me<-apply(stable,2,median)
head(me)
for (i in 1:2704){
    total[,i]=100*total[,i]/me[i]
}
write.table(total,"normalized_by_median100.txt")


............*AAAAAAgrep出6A前面的有>15个任意字符的正则表达式

stable<-read.table("compound60.txt",head=T)
total<-read.table("compound60_total.txt",head=T)
me<-apply(stable,2,median)
for (i in 1:64){
    total[,i]=log2(total[,i]/me[i]+1) #再想想加1是不是合适
}
write.table(total,"normalized_by_median_of_stable_genes2.txt")

stable<-read.table("stable.txt",header=T,row.names=1)
head(stable)
total<-read.table("total.txt",header=T,row.names=1)
head(total)
me<-apply(stable,2,median)
me
dim(stable)
dim(total)
for (i in 1:2074){
    total[,i]=total[,i]/me[i]
}
write.table(total,"normalized_by_median_of_stable_genes_no_log.txt")

stable<-read.table("stable.txt",header=T,row.names=1)
total<-read.table("total.txt",header=T,row.names=1)
me<-apply(stable,2,median)
for (i in 1:2074){
    total[,i]=log2(total[,i]/me[i]+1)
}
write.table(total,"normalized_by_median_of_stable_genes_no_log.txt")
gsea报错，因为名字gct,rnk,gmt不一致,
None of the gene sets that you specified passed the size threshold. Check that the selected gene sets:


alias les='less -N'less可以显示行号
统计某文件夹下文件的个数
ls -l |grep "^-"|wc -l

统计某文件夹下目录的个数
ls -l |grep "^ｄ"|wc -l

统计文件夹下文件的个数，包括子文件夹里的
ls -lR|grep "^-"|wc -l

如统计/home/han目录(包含子目录)下的所有js文件则：
ls -lR /home/han|grep js|wc -l 或 ls -l "/home/han"|grep "js"|wc -l

统计当前文件下所有含有lane为前缀的文件个数

ls -lR ./ | grep Lane | wc -l

python SyntaxError: EOL while scanning string literal 字符串末尾忘记加引号
fastx_trimmer -f 1 -l 51 -Q 33 -i {fastq} -o rasl_lab2_AGTGCAT_L005_R1_51nb.fastq

合并文件夹
sudo cp -r fastq_lane7_corrected/* fastq_lane8_corrected/* merged_lane7_lane8/

报错
 ERROR: barcode TGAATTC for lane 7 has length 7: expected barcode lenth (including delimiters) is 14
将I7n 改为I7即可和sample sheet没有关系
configureBclToFastq.pl --input-dir /data/150728_C00126_0206_AC5409ACXX/Data/Intensities/BaseCalls/ --output /data/150728_C00126_0206_AC5409ACXX/unligned_by_barcode8 --sample-sheet /data/150728_C00126_0206_AC5409ACXX/SampleSheet_lane8.csv --no-eamss --use-bases-mask Y101,I7,Y101

SampleSheet_barcode.csv
configureBclToFastq.pl --input-dir /data/150728_C00126_0206_AC5409ACXX/Data/Intensities/BaseCalls/ --output /data/150728_C00126_0206_AC5409ACXX/unligned_by_barcode8 --sample-sheet /data/150728_C00126_0206_AC5409ACXX/SampleSheet_lane8.csv --no-eamss --use-bases-mask Y101,I7n,Y101

configureBclToFastq.pl --input-dir /data/150728_C00126_0206_AC5409ACXX/Data/Intensities/BaseCalls/ --output /data/150728_C00126_0206_AC5409ACXX/unligned_by_barcode3 --sample-sheet /data/150728_C00126_0206_AC5409ACXX/SampleSheet_lane7_8.csv --no-eamss --use-bases-mask Y101,I7n,Y101
cd /data/150728_C00126_0206_AC5409ACXX/unligned_by_barcode
nohup make -j 8

grep -A 3 ':N:0:TGTTGTT' ./lane8_R1.fastq | awk '$1 != "--"' >> ./Lane8_plate1_A1_tmp.fastq
grep ':N:0:TGTTGTT' ./lane8_R1.fastq >> tmp.fasta

mac中去掉^M
在vim中 %s/\r/\r\n/g
               %s/^@//g (^@为control ＋shift＋2)
               
configureBclToFastq.pl --input-dir /data/150728_C00126_0206_AC5409ACXX/Data/Intensities/BaseCalls/ --output /data/150728_C00126_0206_AC5409ACXX/Unaligned --sample-sheet /data/150728_C00126_0206_AC5409ACXX/SampleSheet_undetermined.csv --no-eamss --use-bases-mask Y101,I7n,Y101
cd /data/150728_C00126_0206_AC5409ACXX/Unaligned
nohup make -j 8


setwd('C:\\Documents and Settings\\Administrator\\桌面\\Day5') ;
dat <- read.table("OV.clin.merged.picked.txt",header=T,row.names=1,sep='\t') ;
ov_cl <- t(dat) ;
dat1 <- ov_cl[,c(3:5,14)] ;
toupper(rownames(dat1))-> dat2;
dat2-> rownames(dat1) ;

Vital_status <- as.numeric(dat1[,1]) ;
index1 <- which(Vital_status==1) ;
index2 <- which(Vital_status==0) ;
Time <- rep(0, length(Vital_status)) ;
Time[index1] <- as.numeric(dat1[,2][index1]) ;
Time[index2] <- as.numeric(dat1[,3][index2]) ;

clinical <- data.frame(Time, Vital_status) ;
rownames(clinical) <- rownames(dat1) ;

library(survival) ;
type <- c(rep(1, 290), rep(2, 290)) ;
clinical$type <- type;

surv_curv <- survfit( Surv(Time, Vital_status)~type, data =clinical) ;
plot(surv_curv ,col=c("red", "blue"), main="test") ;

t_results <- survdiff(Surv(Time, Vital_status)~type, data =clinical);
p<-pchisq(t_results$chisq, 1 , lower.tail = F) ;



exp_data <- read.table("OV.uncv2.mRNAseq_RSEM_all.txt",header=T, row.names=1, sep='\t') ;

surv_mode <- coxph(Surv(days, event)~type ,data=data.frame((mer))) ;
summary(surv_mode) ;


ayi
indentical(mutation2[, "Tumor_Seq_Allele1"], mutation2[, "Tumor_Seq_Allele2"])
identical(mutation2[, "Tumor_Seq_Allele1"], mutation2[, "Tumor_Seq_Allele2"])
identical(mutation2[, "Reference_Allele"], mutation2[, "Tumor_Seq_Allele1"])
head(rank(num))
result <- rank(num)
top(result)
barplot(result[1:10])
barplot(result[1:10], pos=2)
barplot(result[1:10], las=2)
barplot(result[1:10])
barplot(result[1:10], las=2)
num <- table(mutation2[,1])
head( table(mutation2[,1]))
num <- table(mutation2[,1])
result <- sort(num, decreasing=T)
head(result)
barplot(result[1:10])
barplot(result[1:10], las=2)
head(order(num))
head(rank(num))
?order
a <- read.table( "BRCA.maf", header=TRUE, sep="\t", as.is=T, quote="" );
mutation_level2 <- a[,c("Hugo_Symbol","Entrez_Gene_Id", "Tumor_Sample_Barcode","Variant_Classification",
 "Reference_Allele",  "Tumor_Seq_Allele1", "Tumor_Seq_Allele2", "Variant_Type")] ;
head(mutation_level2)

diag(m1)<-c(2,2)
可以用来自己定义对角上是2，2
usr。一个形式为c(x1, x2, y1, y2)的向量，表示当前绘图区域的坐标值范围：c(xleft, xright, ybottom, ytop)
segments(x0, y0,x1, y1)从(x0,y0)各点到(x1,y1)各点画线段
x<-rbind(matrix(rnorm(100,sd=0.3),ncol=2),matrix(rnorm(100,mean=1,sd=0.3),ncol=2))
> x
               [,1]         [,2]
  [1,]  0.075809032 -0.404080927
  [2,] -0.169217512  0.024630198
  [3,] -0.326937759 -0.068034088
  [4,] -0.171604128  0.215880739
  [5,]  0.054431972  0.351992275
  [6,] -0.173686287 -0.223952908
  [7,]  0.576472874 -0.219211894
  [8,] -0.067776176  0.492954623
  [9,]  0.277901424 -0.010828356
 [10,]  0.206216625  0.243999254
 [11,] -0.049710020  0.030814346
 [12,] -0.417457200  0.224153542
 [13,] -0.434220743 -0.004177382
 [14,] -0.306631901  0.020381852
 [15,] -0.378454487  0.189841490
 [16,] -0.610371588 -0.239180151
 [17,]  0.233768075  0.074860149
 [18,] -0.024903611 -0.027231788
 [19,] -0.465638102 -0.194418106
 [20,] -0.085820736 -0.331151136
 [21,] -0.189881969  0.051737234
 [22,] -0.315167714 -0.323539376
 [23,]  0.140299351 -0.399533695
 [24,] -0.133015220  0.100039819
 [25,] -0.005198166 -0.053234057
 [26,] -0.098172165 -0.336291389
 [27,]  0.455299180  0.240420169
 [28,] -0.171613843 -0.048417871
 [29,] -0.331062919 -0.064269605
 [30,]  0.085592035 -0.114455008
 [31,]  0.114850089  0.092184222
 [32,]  0.199626606 -0.428192373
 [33,]  0.063885459 -0.333225502
 [34,] -0.420853809  0.319425097
 [35,] -0.224300697 -0.141702796
 [36,]  0.169283536  0.495873724
 [37,] -0.276329627 -0.178151078
 [38,]  0.112015148 -0.026264697
 [39,]  0.019418563  0.223021695
 [40,]  0.242171256  0.109160957
 [41,] -0.149711206  0.092786656
 [42,] -0.286986763 -0.198599716
 [43,]  0.122501635 -0.171359457
 [44,] -0.429046107  0.643727210
 [45,]  0.010416601 -0.163280718
 [46,] -0.168532997  0.379974408
 [47,] -0.046058655 -0.414097070
 [48,]  0.018146006  0.118730073
 [49,] -0.008583462  0.309292921
 [50,]  0.172602507 -0.204626754
 [51,]  0.961742541  0.820614274
 [52,]  1.598841567  1.577888202
 [53,]  0.897445050  1.168759085
 [54,]  1.013799067  1.091085291
 [55,]  0.511657562  1.385365124
 [56,]  0.782030252  0.930881428
 [57,]  1.197374783  1.298786510
 [58,]  0.817749961  1.162679101
 [59,]  0.794933521  0.610055485
 [60,]  0.725761177  1.137923744
 [61,]  0.763168255  0.691686276
 [62,]  1.105607229  0.928403626
 [63,]  1.279447009  1.244126881
 [64,]  1.011831494  1.112167507
 [65,]  1.397522323  0.836428417
 [66,]  0.993731985  0.581916940
 [67,]  1.321884256  0.640129329
 [68,]  1.045486889  1.180896862
 [69,]  1.390668410 -0.169915471
 [70,]  1.226201415  1.090748552
 [71,]  1.407584183  1.441503876
 [72,]  1.270954711  0.701952863
 [73,]  0.721389589  0.972209906
 [74,]  0.738783669  1.283238226
 [75,]  1.012990155  0.664711906
 [76,]  0.711501650  0.079330601
 [77,]  0.708253032  1.480811101
 [78,]  0.515331180  0.257406825
 [79,]  1.128984919  0.577839141
 [80,]  1.249673633  0.848643705
 [81,]  1.103466520  1.400149290
 [82,]  0.650010708  1.079139687
 [83,]  1.008084515  0.962062097
 [84,]  1.384351199  1.332199415
 [85,]  0.788717321  0.710343418
 [86,]  0.817884051  1.123601903
 [87,]  1.418158751  0.923528762
 [88,]  0.466508204  1.414134615
 [89,]  1.202763623  1.317636399
 [90,]  0.874880578  0.588613253
 [91,]  0.778131987  0.798397963
 [92,]  1.191823337  0.854885861
 [93,]  0.715928728  1.596909605
 [94,]  0.854245888  0.611078775
 [95,]  1.525909593  1.530199090
 [96,]  1.136020739  1.085209413
 [97,]  0.902053599  0.859028973
 [98,]  0.738916686  1.050634323
 [99,]  1.325679879  0.906956396
[100,]  0.742656350  1.604796094
> c1<-kmeans(x,2,20)
> c1
K-means clustering with 2 clusters of sizes 48, 52

Cluster means:
         [,1]        [,2]
1  1.01461856 1.021688400
2 -0.04537317 0.001175234

Clustering vector:
  [1] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
 [35] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 [69] 1 1 1 1 1 1 1 2 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1

Within cluster sum of squares by cluster:
[1] 9.329080 7.273998
 (between_SS / total_SS =  76.5 %)

Available components:

[1] "cluster"      "centers"      "totss"        "withinss"    
[5] "tot.withinss" "betweenss"    "size"         "iter"        
[9] "ifault"      
> ??kmeans
> c
function (..., recursive = FALSE)  .Primitive("c")
> c1$cluster
  [1] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
 [35] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 [69] 1 1 1 1 1 1 1 2 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
> plot(x,col=c1$cluster,pch=3,lwd=1)
> c1$centers ＃kmean 算出来的中心
         [,1]        [,2]
1  1.01461856 1.021688400
2 -0.04537317 0.001175234
> c1
K-means clustering with 2 clusters of sizes 48, 52

Cluster means:
         [,1]        [,2]
1  1.01461856 1.021688400
2 -0.04537317 0.001175234

Clustering vector:
  [1] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
 [35] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 [69] 1 1 1 1 1 1 1 2 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1

Within cluster sum of squares by cluster:
[1] 9.329080 7.273998
 (between_SS / total_SS =  76.5 %)

Available components:

[1] "cluster"      "centers"      "totss"        "withinss"    
[5] "tot.withinss" "betweenss"    "size"         "iter"        
[9] "ifault"      
> points(c1$centers,col=1:2,pch=7,lwd=3)
> ??points
segments(x[c1$cluster==1,][,1],x[c1$cluster==1,][,2],c1$centers[1,1],c1$centers[1,2]) #c1$cluster==1,]第一类的分类情况，x[c1$cluster==1,][,1]第一类对应的数x[c1$cluster==1,][,1]第一列 就是将分类1里是true的取出来，然后确定center，然后做center到点的直线，center和实体点都是两个数字决定一个点
segments(x[c1$cluster==2,][,1],x[c1$cluster==2,][,2],c1$centers[2,1],c1$centers[2,2],col=2) # segments(x0, y0,x1, y1)从(x0,y0)各点到(x1,y1)各点画线段
 hcluster 聚类
 > n<-seq(1,50,by=4) ＃可利用这种方式做index
> n
 [1]  1  5  9 13 17 21 25 29 33 37 41 45 49
> x<-USArrests[n,] #然后这样按行取，不用循环，如我60个药,n<-seq(1,60,by=1) total[,n]=log(me[,n]
> x
               Murder Assault UrbanPop Rape
Alabama          13.2     236       58 21.2
California        9.0     276       91 40.6
Florida          15.4     335       80 31.9
Illinois         10.4     249       83 24.0
Kentucky          9.7     109       52 16.3
Massachusetts     4.4     149       85 16.3
Missouri          9.0     178       70 28.2
New Hampshire     2.1      57       56  9.5
North Carolina   13.0     337       45 16.1
Oregon            4.9     159       67 29.3
South Dakota      3.8      86       45 12.8
Vermont           2.2      48       32 11.2
Wisconsin         2.6      53       66 10.8


hc1<-hclust(dist(x),method= "complete")
> dist(x) 可用这种方法算出两个之间的距离
                 Alabama California   Florida  Illinois  Kentucky Massachusetts
California      55.52477                                                       
Florida        102.00162   60.98073                                            
Illinois        28.45488   32.71880  86.55871                                  
Kentucky       127.28417  173.20791 228.33276 143.59937                        
Massachusetts   91.64851  129.52471 187.04374 100.49522  52.12571              
Missouri        59.78829  100.98891 157.49175  72.31597  72.29869      35.05382
New Hampshire  179.73620  224.05539 280.24748 194.60766  53.14132      96.72916
North Carolina 101.96102   80.33212  38.52791  96.21419 228.13139     192.40062
Oregon          78.38686  120.03958 176.81066  91.72971  54.00963      24.35672
South Dakota   151.08911  197.52438 252.43884 167.87495  25.00120      74.71017
Vermont        190.37069  237.43546 292.02008 207.92566  64.83255     114.19654
Wisconsin      183.77573  226.45750 283.42380 197.33241  58.41798      98.03311
                Missouri New Hampshire North Carolina    Oregon South Dakota
California                                                                  
Florida                                                                     
Illinois                                                                    
Kentucky                                                                    
Massachusetts                                                               
Missouri                                                                    
New Hampshire  123.42731                                                    
North Carolina 161.45715     280.50556                                      
Oregon          19.69822     104.52215      180.02180                       
South Dakota    96.71194      31.23748      251.19023  78.01577             
Vermont        136.67202      25.68852      289.53523 117.81723     40.22586
Wisconsin      126.43069      10.86002      285.01447 107.63150     39.18469
                 Vermont
California              
Florida                 
Illinois                
Kentucky                
Massachusetts           
Missouri                
New Hampshire           
North Carolina          
Oregon                  
South Dakota            
Vermont                 
Wisconsin       34.37034
>hc2<-hclust(dist(scale(x)),method= "complete") #scale(x, center = TRUE, scale = TRUE)默认算的是zscore
> hc3<-hclust(dist(x),method= "ave")
> layout(matrix(c(1,1,2,3),nrow=2,byrow=T))
> matrix(c(1,1,2,3),nrow=2,byrow=T)(看出来layout按照矩阵来，查一查怎么按照列聚类)
     [,1] [,2]
[1,]    1    1
[2,]    2    3
代表图的顺序是1，1，2，3
所以根据matrix来改变顺序和安排个数
> plot(hc1)
> plot(hc2)
> plot(hc3)
用cluster包做聚类
> library(cluster)
> clusplot(x,pam(x,2)$clustering)
> pam(x,2)
Medoids:
           ID Murder Assault UrbanPop Rape
California  2    9.0     276       91 40.6
Kentucky    5    9.7     109       52 16.3
Clustering vector:
       Alabama     California        Florida       Illinois       Kentucky  Massachusetts       Missouri 
             1              1              1              1              2              2              2 
 New Hampshire North Carolina         Oregon   South Dakota        Vermont      Wisconsin 
             2              1              2              2              2              2 
Objective function:
   build     swap 
56.58520 46.87565 

Available components:
 [1] "medoids"    "id.med"     "clustering" "objective"  "isolation"  "clusinfo"   "silinfo"    "diss"      
 [9] "call"       "data"      
> pam(x,2)$clustering
       Alabama     California        Florida       Illinois       Kentucky  Massachusetts       Missouri 
             1              1              1              1              2              2              2 
 New Hampshire North Carolina         Oregon   South Dakota        Vermont      Wisconsin 
             2              1              2              2              2              2 
>

points(x, y)添加点(可以使用选项type=)

龙星课程学的几个重要函数
data<-read.csv("BRCA2.maf",sep="\t",quote="")
table(data[,"Variant_Classification"]) # 统计这列里元素的个数
index<-which(data[,"Variant_Classification"]=="Silent")利用等号进行匹配  #which实际是返回索引
index<-which(data[,"Variant_Classification"]%in%c("RNA","Silent"))利用正则表达式匹配"Variant_Classification"这列中为英豪中的内容
data1<-data[-index,]提取出不是INDEX的数据
result<-table(data1[,"Hugo_Symbol"]) # 统计这列里元素的个数
 result2<-sort(result,decreasing=T)讲叙排列
pdf("barplot")
barplot(result2[1:10],las=2)字体是斜体
dev.off()

> a
     [,1] [,2] [,3]
[1,]    1    1    1
[2,]    2    3    4
[3,]    2    3    4
> layout(a)
> table(a[,1])

1 2 
1 2 
> table(a[,3])

1 4 
1 2 

table会生成一个含有因子和因子个数的表格 


stable<-read.table("compound60.txt",head=T)
total<-read.table("compound60_total.txt",head=T)
me<-apply(stable,2,median)
for (i in 1:64){
    total[,i]=log2(total[,i]/(me[i]+1)+1) #再想想加1是不是合适
}
write.table(total,"normalized_by_median_of_stable_genes2.txt")
写法2

stable<-read.table("compound60.txt",head=T)
total<-read.table("compound60_total.txt",head=T)
me<-apply(stable,2,median)
n<-seq(1,64,by=1) 
total[,n]=log2(total[,n]/(me[n]+1)+1)
write.table(total,"normalized_by_median_of_stable_genes2.txt")

m<-cbind(a,b)  
 c1<-kmeans(x,2,20) x是这个matrix，2是2类，20是循环次数
 
plot(x,sin(x),type="o",bg=par("bg"))
with(iris,plot(Sepal.Length,Sepal.Width,pch=as.numeric(Species),cex=1.2)) #pch=as.numeric(Species)这个做法很好，就是将species里面的因子转化为数字，有几类可以成为图形的种类，所以颜色也可以这么表示
legend(6.1,4.4,c("setosa","versicolor","virginica"),cex=1.5,pch=1:3) #　在图上添加描述6.1 x轴上面， 4.4  y最上面，其实就是左上角的点
按颜色标记
> with(iris,plot(Sepal.Length,Sepal.Width,col=as.numeric(Species),cex=1.2))
> legend(6.1,4.4,c("setosa","versicolor","virginica"),cex=1.5,fill=1:3) ＃fill=v,v是一个和legend长度一直的向量，
也可以根据pch=v,来按照符号就行标记
> with(iris,plot(Sepal.Length,Sepal.Width,pch=as.numeric(Species),cex=1.2))
> legend(6.1,4.4,c("setosa","versicolor","virginica"),cex=1.5,pch=1:3)
axis(1,at=year,label=year);axis(2) 设置坐标轴
mtext("UR(%)",4,3,col="red") ＃可在右边又添加一个标题
par(new=T,mar=c(10,4,10,6)+0.1) #mar控制图形边空的有4个值的向量c(bottom, left, top, right), 缺省值 为c(5.1, 4.1, 4.1, 2.1)
axis(4,col="red",col.axis="red") # 两句一起可以又添加一个坐标
 plot(0,main=paste(strwrap("This is a really long title that
+ 2 i can not type it properly",width=50),collapse="\n"))
标题太长可以利strwrap进行自定义段落格式
> barplot(x,col=rev(heat.colors(10)))  c o l = rev ( heat . c o l o r s ( 1 0 ) )还是反着来
> barplot(x,col=gray((1:10)/10))
barplot(x,col=rainbow(10))
text(2.5,2.5,expression(z[i]==sqrt(x[i]^2+y[i]^2)))  #利用expression添加公式
里面的10一定要指定变几种颜色
x<-1:5
> y<-c(1,3,4,2.5,2)
> plot(x,y)
> sp<-spline(x,y,n=50)
> lines(sp) 模拟出链接这几个点的曲线
x<-1:10
y<-runif(10)
symbols(x,y,circles=y/2,inches=F,bg=x)
# This function draws symbols on a plot. One of six symbols; circles, squares, rectangles, stars, thermometers, and boxplots, can be plotted at a specified set of x and y coordinates. Specific aspects of the symbols, such as relative size, can be customized by additional parameters.
symbols(x, y = NULL, circles, squares, rectangles, stars,
        thermometers, boxplots, inches = TRUE, add = FALSE,
        fg = par("col"), bg = NA,
        xlab = NULL, ylab = NULL, main = NULL,
        xlim = NULL, ylim = NULL, ...)
        
x, y	
the x and y co-ordinates for the centres of the symbols. They can be specified in any way which is accepted by xy.coords.

circles	
a vector giving the radii of the circles.

a<-c(1,2,3,4)
hist(a)
op<-par(fig=c(.02,.5,.5,.98),new=TRUE) #利用fig。一个数值向量，形式为c(x1, x2, y1, y2)，用于设定当前图形在绘图设备中所占区域，注意需要满足x1<x2,y1<y2。如果修改参数fig，会自动打开一个新的绘图设备，而若希望在原来的绘图设备中添加新的图形，需要和参数new=TRUE一起使用
boxplot(x)


correlation的图不能取两次log，脚本里是1为底的对数
60个药物作图所在位置
C cluster
/Share/home/wangdong/lss/project/20150430_sw_probes_screen/plot
A集群
./data/sw_20150110/SW_16_Samples_R/
所以根据自己数据的具体情况，初步定义为
log2(m[,i]/q[i]+1) q为含有每一列即每个药stable gene的中位值
find -name all_61samples_3.pdf
利用find -name来查找某个文件所在的位置

apply(m,1,sum,na.rm=TRUE)
R 语言中提供了四类有关统计分布的函数（密度函数，累计分布函数，分位函数，随机数函数）。分别在代表该分布的R函数前加上相应前缀获得 (d，p，q，r)。如正态分布的函数是norm，命令dnorm(0)就可以获得正态分布的密度函数在0处的值(0.3989)(默认为标准正态分 布)。同理pnorm(0)是0.5就是正态分布的累计密度函数在0处的值。而qnorm(0.5)则得到的是0，即标准正态分布在0.5处的分位数是 0（在来个比较常用的：qnorm(0.975)就是那个估计中经常用到的1.96了）。最后一个rnorm(n)则是按正态分布随机产生n个数据。上面 正态分布的参数平均值和方差都是默认的0和１，你可以通过在函数里显示指定这些参数对其进行更改。如dnorm(0,1,2)则得出的是均值为1，标准差 为2的正态分布在0处的概率值。要注意的是()内的顺序不能颠倒。
feelings<-c("sad","afraid","happy","angry")
for (i in feelings)
print(switch(i,happy="i am glad you are happy",afraid="there is nothing to fear",sad="cheer up",angry="calm down"))



bamToBed -i accepted_hits.bam | awk 'BEGIN{FS="\t";OFS="\t"}{if($1=="MT"){print "chrM",$2,$3,$4,$5,$6}else{print "chr"$1,$2,$3,$4,$5,$6}}' > accepted_hits.bed
makeTagDirectory tags2/ accepted_hits.bed -genome /Work1/home/wangdong/ensemble/genome.fa -sspe
removeOutOfBoundsReads.pl tags/ /Work1/home/wangdong/ensemble/genome.fa -chromSizes /Work1/home/wangdong/ensemble/hg19.chrom.sizes     
makeUCSCfile tags2 -o auto -bigWig 	 -norm 1e7 -strand both -fsize 1e20 -fragLength given > tags2/tags.trackInfo2.txt
可视化流程过程(swc2_swc3_swnc_swm1_swm2全按下面流程，在A集群上)
bamToBed -i /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_sh2/accepted_hits.bam > /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_sh2/accepted_hits.bed
makeTagDirectory /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_sh2/tags2/ /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_sh2/accepted_hits.bed -genome /Share/home/lulab1/users/liyang/project/Genome/human_hg19/bowtie2_index/human_genome_hg19.fa -sspe
bamToBed -i /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_sh3/accepted_hits.bam > /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_sh3/accepted_hits.bed
makeTagDirectory /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_sh3/tags/ /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_sh3/accepted_hits.bed -genome /Share/home/lulab1/users/liyang/project/Genome/human_hg19/bowtie2_index/human_genome_hg19.fa -sspe
removeOutOfBoundsReads.pl /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_sh3/tags/ /Share/home/lulab1/users/liyang/project/Genome/human_hg19/bowtie2_index/human_genome_hg19.fa -chromSizes /Share/home/wangdong/lss/hg19/hg19.chrom.sizes     
makeUCSCfile /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_sh3/tags/ -o auto -bigWig /Share/home/wangdong/lss/hg19/hg19.chrom.sizes -norm 1e8 -strand both -fsize 1e20 > /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_sh3/tags/tags.trackInfo.txt
bamToBed -i /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_NC/accepted_hits.bam > /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_NC/accepted_hits.bed
makeTagDirectory /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_NC/tags/ /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_NC/accepted_hits.bed -genome /Share/home/lulab1/users/liyang/project/Genome/human_hg19/bowtie2_index/human_genome_hg19.fa -sspe
removeOutOfBoundsReads.pl /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_NC/tags/ /Share/home/lulab1/users/liyang/project/Genome/human_hg19/bowtie2_index/human_genome_hg19.fa -chromSizes /Share/home/wangdong/lss/hg19/hg19.chrom.sizes     
makeUCSCfile /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_NC/tags/ -o auto -bigWig /Share/home/wangdong/lss/hg19/hg19.chrom.sizes -norm 1e8 -strand both -fsize 1e20 > /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_NC/tags/tags.trackInfo.txt
bamToBed -i /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/wqy_2M/accepted_hits.bam > /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/wqy_2M/accepted_hits.bed
makeTagDirectory /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/wqy_2M/tags/ /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/wqy_2M/accepted_hits.bed -genome /Share/home/lulab1/users/liyang/project/Genome/human_hg19/bowtie2_index/human_genome_hg19.fa -sspe
removeOutOfBoundsReads.pl /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/wqy_2M/tags/ /Share/home/lulab1/users/liyang/project/Genome/human_hg19/bowtie2_index/human_genome_hg19.fa -chromSizes /Share/home/wangdong/lss/hg19/hg19.chrom.sizes     
makeUCSCfile /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/wqy_2M/tags/ -o auto -bigWig /Share/home/wangdong/lss/hg19/hg19.chrom.sizes -norm 1e8 -strand both -fsize 1e20 > /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/wqy_2M/tags/tags.trackInfo.txt


张柳佳rnaseq重新分析
samtools sort -n /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_sh2/accepted_hits.bam /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_sh2/bamsorted
samtools view -h /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_sh2/bamsorted.bam > /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_sh2/pca3_sh2.sam
htseq-count -s no -t gene /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_sh2/pca3_sh2.sam /Share/home/wangdong/data/human_annotation/gencode19.lite.gtf > /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_sh2/pca3_sh2_2.count
samtools sort -n /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_sh3/accepted_hits.bam /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_sh3/bamsorted
samtools view -h /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_sh3/bamsorted.bam > /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_sh3/pca3_sh3.sam
htseq-count -s no -t gene /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_sh3/pca3_sh3.sam /Share/home/wangdong/data/human_annotation/gencode19.lite.gtf > /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_sh3/pca3_sh3.count
samtools sort -n /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_NC/accepted_hits.bam /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_NC/bamsorted
samtools view -h /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_NC/bamsorted.bam > /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_NC/pca3_NC.sam
htseq-count -s no -t gene /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_NC/pca3_NC.sam /Share/home/wangdong/data/human_annotation/gencode19.lite.gtf > /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_NC/pca3_NC.count

htseq-count -s yes /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_sh2/pca3_sh2.sam /Share/home/wangdong/data/human_annotation/gencode19.lite.gff > /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_sh2/pca3_sh2_3.count
htseq-count -s yes /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_sh3/pca3_sh3.sam /Share/home/wangdong/data/human_annotation/gencode19.lite.gff > /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_sh3/pca3_sh3.count
htseq-count -s yes /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_NC/pca3_NC.sam /Share/home/wangdong/data/human_annotation/gencode19.lite.gff > /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_NC/pca3_NC.count

数了后没有结果
正确做法
-s  yes两特异性的 pairend 很关键，-t gene不选择  默认是exon
htseq-count -s yes /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_sh2/pca3_sh2.sam /Share/home/wangdong/data/human_annotation/gencode19.lite.gff > /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_sh2/pca3_sh2_3.count
htseq-count -s yes /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_sh3/pca3_sh3.sam /Share/home/wangdong/data/human_annotation/gencode19.lite.gff > /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_sh3/pca3_sh3.count
htseq-count -s yes /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_NC/pca3_NC.sam /Share/home/wangdong/data/human_annotation/gencode19.lite.gff > /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_NC/pca3_NC.count

统计探针的组成情况
sort worked_termini.fa |uniq -c |sort -g -r -o worked_terminal_result.fa
sort acceptor_2bp_w.fa |uniq -c |sort -g -r -o acceptor_2bp_w_result.fa
sort donor_2bp_w.fa |uniq -c |sort -g -r -o donor_2bp_w_result.fa
sort acceptor_2bp_nw.fa |uniq -c |sort -g -r -o acceptor_2bp_nw_result.fa
sort donor_2bp_nw.fa |uniq -c |sort -g -r -o donor_2bp_nw_result.fa

uniq [选项] 文件

说明：这个命令读取输入文件，并比较相邻的行。在正常情况下，第二个及以后更多个重复行将被删去，行比较是根据所用字符集的排序序列进行的。该命令加工后的结果写到输出文件中。输入文件和输出文件必须不同。如果输入文件用“- ”表示，则从标准输入读取。

该命令各选项含义如下：、

C c 显示输出中，在每行行首加上本行在文件中出现的次数。它可取代- u和- d选项。

C d 只显示重复行。

C u 只显示文件中不重复的各行。


col=c("red","blue"),ylab="The Tm Value of probes",las=1,font.lab=2
作tm值的图
a<-read.table("worked_sw_probe_list_tm.txt",head=T,row.names=1)
a<-read.table("worked_sw_probe_list_tm.txt",head=T)
head(a)
b<-read.table("not_worked_sw_probe_list_tm.txt",head=T)
b<-read.table("not_worked_sw_probe_list_tm.txt",head=T)
b<-read.table("not_worked_sw_probe_list_tm.txt",head=T)
head(b)
pdf("Tm.pdf")
boxplot(a$Tm_a,a$Tm_d,b$Tm_a,b&Tm_d),col=c("red","blue"),ylab="The Tm Value of probes",las=1,font.lab=2))
boxplot(a$Tm_a,a$Tm_d,b$Tm_a,b&Tm_d,col=c("red","blue"),ylab="The Tm Value of probes",las=1,font.lab=2)
boxplot(a$Tm_a,a$Tm_d,b$Tm_a,b$Tm_d,col=c("red","blue"),ylab="The Tm Value of probes",las=1,font.lab=2)
dev.off()
history()

作表达值的图
a<-read.table("a.txt",head=T,row.names=1)
head(a)
b<-read.table("b.txt",head=T,row.names=1)
c<-read.table("c.txt",head=T,row.names=1)
pdf("sw_boxplot.pdf")
boxplot(a$MDA231,a$sw620,a$thp.1,a$hepg2,a$hp7702,b$MDA231,b$sw620,b$thp.1,b$hepg2,b$hp7702,c$MDA231,c$sw620,c$thp.1,c$hepg2,c$hp7702,col=c("mediumturquoise","pink","red"),ylab="normalized Indensity Values",las=1,font.lab=2)
dev.off()
pdf("sw_boxplot2.pdf")
boxplot(a$MDA231,a$sw620,a$thp.1,a$hepg2,a$hp7702,b$MDA231,b$sw620,b$thp.1,b$hepg2,b$hp7702,c$MDA231,c$sw620,c$thp.1,c$hepg2,c$hp7702,col=c("mediumturquoise","pink","red","yellow","blue"),ylab="normalized Indensity Values",las=1,font.lab=2)dev.off()
history()

source("http://bioconductor.org/biocLite.R")
microarry normalization

biocLite("preprocessCore")
library(preprocessCore)

data<-read.table("four_cell_line.txt",header=T,row.names=1)
head(data);dim(data)
data.quantile<-normalize.quantiles(as.matrix(data))

par(mfrow=c(1,2))
boxplot(log2(data))
boxplot(log2(data.quantile))

boxplot(finaldata,col=c("mediumturquoise"),ylab="normalized Indensity Values",las=1,font.lab=2)
pdf("cutoff20_exon.pdf")
boxplot(b$worked,a$not_worked,col=c("mediumturquoise"),ylab="The distribution of gene exon number",las=1,font.lab=2)
boxplot(x1,x2,x3,xlim=c(0,4),ylim=c(0,15),main="白鼠伤寒杆菌实验分析",xlim=c(0,4),ylim=c(0,15),xlab="实验菌种",ylab="平均存活天数",pch=19,col=c("red","green","blue"),xaxs="i",yaxs="i",names=c('菌种一','菌种二','菌种三'))

ttest(array1,array2,2,3)双尾（可比对照大可小），方差不等
A集群提交任务
$bqueues
$bjobs
bkill
$perl bsub.pl bamtobed.sh 1 TINY ./source.txt

.rar解压
安装rarosx-5.2.1.tar.gz

例2：解压缩abc.rar档案中的内容，可以使用e或x命令,假设abc.rar目录中有一个名为file1的文件和一个名为test的目录，test目录中有一个名为file2的文件，
$rar e abc.rar

说明：使用e命令，会将abc.rar中的file1文件连同test目录下的file2文件解压到当前目录。如果想保持abc.rar目录中的目录结构请使用x命令。
$rar x abc.rar


bamToBed -i /Work1/home/wangdong/lss/zlj_rnaseq/zlj_sh2/accepted_hits.bam > /Work1/home/wangdong/lss/zlj_rnaseq/bamtobed/zlj_sh2.bed
bamToBed -i /Work1/home/wangdong/lss/zlj_rnaseq/zlj_sh3/accepted_hits.bam > /Work1/home/wangdong/lss/zlj_rnaseq/bamtobed/zlj_sh3.bed
bamToBed -i /Work1/home/wangdong/lss/zlj_rnaseq/zlj_NC/accepted_hits.bam > /Work1/home/wangdong/lss/zlj_rnaseq/bamtobed/zlj_NC.bed
bamToBed -i /Work1/home/wangdong/lss/zlj_rnaseq/wqy_2M/accepted_hits.bam > /Work1/home/wangdong/lss/zlj_rnaseq/bamtobed/wqy_2M.bed


makeTagDirectory /Work1/home/wangdong/lss/zlj_rnaseq/maketagdir/maketags_zlj_sh2/ /Work1/home/wangdong/lss/zlj_rnaseq/bamtobed/zlj_sh2.bed -genome /Work1/home/wangdong/ensemble/genome.fa -sspe
makeTagDirectory /Work1/home/wangdong/lss/zlj_rnaseq/maketagdir/maketags_zlj_sh3/ /Work1/home/wangdong/lss/zlj_rnaseq/bamtobed/zlj_sh3.bed -genome /Work1/home/wangdong/ensemble/genome.fa -sspe
makeTagDirectory /Work1/home/wangdong/lss/zlj_rnaseq/maketagdir/maketags_zlj_NC/ /Work1/home/wangdong/lss/zlj_rnaseq/bamtobed/zlj_NC.bed -genome /Work1/home/wangdong/ensemble/genome.fa -sspe
makeTagDirectory /Work1/home/wangdong/lss/zlj_rnaseq/maketagdir/maketags_wqy_2M/ /Work1/home/wangdong/lss/zlj_rnaseq/bamtobed/wqy_2M.bed -genome /Work1/home/wangdong/ensemble/genome.fa -sspe

removeOutOfBoundsReads.pl /Work1/home/wangdong/lss/zlj_rnaseq/maketagdir/maketags_zlj_sh2/ /Work1/home/wangdong/ensemble/genome.fa -chromSizes /Work1/home/wangdong/ensemble/hg19.chrom.sizes
removeOutOfBoundsReads.pl /Work1/home/wangdong/lss/zlj_rnaseq/maketagdir/maketags_zlj_sh3/ /Work1/home/wangdong/ensemble/genome.fa -chromSizes /Work1/home/wangdong/ensemble/hg19.chrom.sizes
removeOutOfBoundsReads.pl /Work1/home/wangdong/lss/zlj_rnaseq/maketagdir/maketags_zlj_NC/ /Work1/home/wangdong/ensemble/genome.fa -chromSizes /Work1/home/wangdong/ensemble/hg19.chrom.sizes
removeOutOfBoundsReads.pl /Work1/home/wangdong/lss/zlj_rnaseq/maketagdir/maketags_wqy_2M/ /Work1/home/wangdong/ensemble/genome.fa -chromSizes /Work1/home/wangdong/ensemble/hg19.chrom.sizes

makeUCSCfile /Work1/home/wangdong/lss/zlj_rnaseq/maketagdir/maketags_zlj_sh2/ -o auto -bigWig /Work1/home/wangdong/ensemble/hg19.chrom.sizes -norm 1e7 -strand both -fsize 1e20 > /Work1/home/wangdong/lss/zlj_rnaseq/maketagdir/maketags_zlj_sh2/tags.trackInfo.txt
makeUCSCfile /Work1/home/wangdong/lss/zlj_rnaseq/maketagdir/maketags_zlj_sh3/ -o auto -bigWig /Work1/home/wangdong/ensemble/hg19.chrom.sizes -norm 1e7 -strand both -fsize 1e20 > /Work1/home/wangdong/lss/zlj_rnaseq/maketagdir/maketags_zlj_sh3/tags.trackInfo.txt
makeUCSCfile /Work1/home/wangdong/lss/zlj_rnaseq/maketagdir/maketags_zlj_NC/ -o auto -bigWig /Work1/home/wangdong/ensemble/hg19.chrom.sizes -norm 1e7 -strand both -fsize 1e20 > /Work1/home/wangdong/lss/zlj_rnaseq/maketagdir/maketags_zlj_NC/tags.trackInfo.txt
makeUCSCfile /Work1/home/wangdong/lss/zlj_rnaseq/maketagdir/maketags_wqy_2M/ -o auto -bigWig /Work1/home/wangdong/ensemble/hg19.chrom.sizes -norm 1e7 -strand both -fsize 1e20 > /Work1/home/wangdong/lss/zlj_rnaseq/maketagdir/maketags_wqy_2M/tags.trackInfo.txt

从maketag开始，重新做makeUCSCfile，生成bedgraph,而做可视化是用bigwig
makeTagDirectory /Work1/home/wangdong/lss/zlj_rnaseq/maketagdir/maketags_zlj_sh2_2/ /Work1/home/wangdong/lss/zlj_rnaseq/bamtobed/zlj_sh2.bed -genome /Work1/home/wangdong/ensemble/genome.fa -sspe
makeTagDirectory /Work1/home/wangdong/lss/zlj_rnaseq/maketagdir/maketags_zlj_sh3_2/ /Work1/home/wangdong/lss/zlj_rnaseq/bamtobed/zlj_sh3.bed -genome /Work1/home/wangdong/ensemble/genome.fa -sspe
makeTagDirectory /Work1/home/wangdong/lss/zlj_rnaseq/maketagdir/maketags_zlj_NC_2/ /Work1/home/wangdong/lss/zlj_rnaseq/bamtobed/zlj_NC.bed -genome /Work1/home/wangdong/ensemble/genome.fa -sspe
makeTagDirectory /Work1/home/wangdong/lss/zlj_rnaseq/maketagdir/maketags_wqy_2M_2/ /Work1/home/wangdong/lss/zlj_rnaseq/bamtobed/wqy_2M.bed -genome /Work1/home/wangdong/ensemble/genome.fa -sspe
makeUCSCfile /Work1/home/wangdong/lss/zlj_rnaseq/maketagdir/maketags_zlj_sh2_2/ -fragLength given -o auto
makeUCSCfile /Work1/home/wangdong/lss/zlj_rnaseq/maketagdir/maketags_zlj_sh3_2/ -fragLength given -o auto
makeUCSCfile /Work1/home/wangdong/lss/zlj_rnaseq/maketagdir/maketags_zlj_NC_2/ -fragLength given -o auto
makeUCSCfile /Work1/home/wangdong/lss/zlj_rnaseq/maketagdir/maketags_wqy_2M_2/ -fragLength given -o auto

analyzeRepeats.pl rna hg19 -strand both -condenseGenes -count exons -d /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/result/maketags_zlj_sh2_2 /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/result/maketags_zlj_sh3_2 /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/result/maketags_zlj_NC_2 -rpkm > /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/result/condensed_rpkm.txt
analyzeRepeats.pl rna hg19 -strand both -count exons -condenseGenes -d /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/result/maketags_zlj_sh2_2 /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/result/maketags_zlj_sh3_2 /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/result/maketags_zlj_NC_2 -noadj > /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/result/shRNA1_shRNA2_NC_raw.txt
getDiffExpression.pl /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/result/shRNA1_shRNA2_NC_raw.txt shRNA shRNA NC -repeats > /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/result/shRNA1_shRNA2_NC_diffOutput.txt
[wangdong@cluster ~/lss/project/zlj_pcat1_RNASEQ/result]$getDiffExpression.pl /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/result/shRNA1_shRNA2_NC_raw.txt shRNA shRNA NC -repeats > /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/result/shRNA1_shRNA2_NC_diffOutput.txt

   
        Treating input as file generated by analyzeRepeats.pl (-repeats)
        Using edgeR to calculate differential expression/enrichment...
Loading required package: limma

可以解决batch effect 也可以调用deseq

By default the program calls on EdgeR to perform the differential expression calculations.  In the experiment has no replication, you may want to set the dispersion by using "-dispersion <#>" (default is 0.05).  To use DESeq instead of EdgeR, specify "-DESeq".

If your samples are paired in anyway you may want to try to account for batch effects.  EdgeR allows you to apply a generalized model to try to remove effects caused by analyzing data on a different day or slightly different batch.  For example, lets say you did an experiment with a control and a drug treatment, then did a second experiment two weeks later with a different library preparation protocol, etc.  You can tell the program that the samples are linked by specifying a batch code - the code is like the experiment annotation in that it applies to each experiment in the same order that they were listed when preparing the gene expression matrix:
analyzeRepeats.pl rna hg19 -noadj -d Week1-Ctrl/ Week1-Drug/ Week2-Ctrl/ Week2-Drug/ > output.txt
getDiffExpression.pl output.txt -repeats ctrl drug ctrl drug -batch 1 1 2 2 > diffOutput.txt

到c集群，因为a上没有homer的基因组
下面脚本报错
analyzeRepeats.pl rna hg19 -strand both -condenseGenes -count exons -d /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mktagsdir/maketags_zlj_sh2_2/ /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mktagsdir/maketags_zlj_sh3_2/ /Work1/home/wangdong/lss/zlj_rnaseq/maketagdir/maketags_zlj_NC_2/ -rpkm > /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mktagsdir/result/condensed_rpkm.txt
单个做依旧报错
analyzeRepeats.pl rna hg19 -strand both -condenseGenes -count exons -d /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mktagsdir/maketags_zlj_sh2_2/ /Work1/home/wangdong/lss/zlj_rnaseq/maketagdir/maketags_zlj_NC_2/ -rpkm > /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mktagsdir/result/condensed_rpkm_sh2_nc.txt

analyzeRepeats.pl rna hg19 -strand both -count exons -condenseGenes -d /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mktagsdir/maketags_zlj_sh2_2/ /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mktagsdir/maketags_zlj_sh3_2/ /Work1/home/wangdong/lss/zlj_rnaseq/maketagdir/maketags_zlj_NC_2/ -noadj > ./shRNA1_shRNA2_NC_raw.txt
Use of uninitialized value in addition (+) at /Share/home/wangdong/packages/homer/bin/analyzeRepeats.pl line 612.
Use of uninitialized value in addition (+) at /Share/home/wangdong/packages/homer/bin/analyzeRepeats.pl line 613.
Use of uninitialized value in addition (+) at /Share/home/wangdong/packages/homer/bin/analyzeRepeats.pl line 614.
最终解决
maketagdirectory一步开始重新做，可能原因是这一步像上面用的李洋下面的hg19，而在analyzeRepeats一步却用的homer自己的，若不提供，调用homer自己的
makeTagDirectory /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/result/maketags_zlj_sh2 /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_sh2/accepted_hits.bam -sspe
makeTagDirectory /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/result/maketags_zlj_sh3 /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_sh3/accepted_hits.bam -sspe
makeTagDirectory /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/result/maketags_zlj_NC /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_NC/accepted_hits.bam -sspe
makeUCSCfile /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/result/maketags_zlj_sh2_2 -fragLength given -o auto
makeUCSCfile /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/result/maketags_zlj_sh3_2 -fragLength given -o auto
makeUCSCfile /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/result/maketags_zlj_NC_2 -fragLength given -o auto
analyzeRepeats.pl rna hg19 -strand both -condenseGenes -count exons -d /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/result/maketags_zlj_sh2_2 /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/result/maketags_zlj_sh3_2 /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/result/maketags_zlj_NC_2 -rpkm > /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/result/condensed_rpkm.txt


zip文件大于4g不可以用unzip .gz
Zip file too big (greater than 4294959102 bytes)
if you've got Java on the box, you can use
 -x[file] 从标准输入提取所有文件，或只提取指定的文件。如果省略了file，则提取所有文件；否则只提取指定文件。
 -f 第二个参数指定要处理的jar文件(文件列表中的第一个元素是要创建或访问的存档文件名字)。在-c(创建)情形中，第二个参数指的是要创建的jar文件的名称(不是在标准输出上)。在-t(表(或-x(抽取)这两种情形中，第二个参数指定要列出或抽取的jar文件。
 
利用  jar解压
jar xf test.zip
也可以进行压缩
4: “jar Ccvf m n”    ：将文件打包成jar压缩包

                            m: 要生成jar包的名字

                            n: 要压缩文件的文件名（可以是多个文件或一个目录）

生成的jar文件实际上就是一个普通的zip压缩文件

xxx批量在r中做图脚本
args=commandArgs( T )
pdf( 'tmp.pdf' )
layout( mat=matrix(c( 1:12 ),nrow=6,ncol=2),heights=c(3,1,3,1,3,1),widths=c( 1,1 ) )
Box=read.table( args[ 1 ],sep='\t',fill=T )
Files=strsplit(args[ 2 ],',')[[1]]
for ( Fi in 1:length(Files) ){
    Data=read.table( Files[ Fi ],sep='\t',row.names=1 )
    Mean=colMeans( Data )
    par( mar=c( 1,4,2,1 ) )
    plot( 1:150,Mean,pch='.',col='red',xaxt='n',xlab=NULL,ylab='Mean Methylation Level',main=paste( 'cluster',as.character( Fi-1 ),sep='_' ),ylim=c( 0,0.05 ) )
    lines( 1:150,Mean,col='red',lwd=2 )
    axis( side=1,at=c( 1,26,125,150 ),labels=c( '-2.5k','TSS','TTS','+2.5k' ) )
    abline( v=26,lty=2,col='grey' )
    abline( v=125,lty=2,col='grey' )
    par( mar=c( 2,4,1,1 ) )
    boxplot(t( Box[ Fi, ] ),horizontal=T,notch=T,ylim=c( 0,15 ),ylab='log2RNA')
}
dev.off(  )


python操作excel
import xlrd
data = xlrd.open_workbook('1274_final_table.xlsx')
table = data.sheet_by_index(0)
nrows = table.nrows
ncols = table.ncols
m=table.row_values(1)
n=table.col_values(1)
求一列的和
sum(n[1:])


方式一:try语句:

1使用try和except语句来捕获异常

try:
   block
except [exception,[data…]]:
   block

try:
block
except [exception,[data...]]:
   block
else:
   block

该种异常处理语法的规则是：

・   执行try下的语句，如果引发异常，则执行过程会跳到第一个except语句。

・   如果第一个except中定义的异常与引发的异常匹配，则执行该except中的语句。

・   如果引发的异常不匹配第一个except，则会搜索第二个except，允许编写的except数量没有限制。

・   如果所有的except都不匹配，则异常会传递到下一个调用本代码的最高层try代码中。

・   如果没有发生异常，则执行else块代码。

例:

try:

   f = open(“file.txt”,”r”)
except IOError, e:
   print e

捕获到的IOError错误的详细原因会被放置在对象e中,然后运行该异常的except代码块

捕获所有的异常

try:
   a=b
   b=c
except Exception,ex:
   print Exception,":",ex

使用except子句需要注意的事情，就是多个except子句截获异常时，如果各个异常类之间具有继承关系，则子类应该写在前面，否则父类将会直接截获子类异常。放在后面的子类异常也就不会执行到了。

2 使用try跟finally:

return语句用来从一个函数 返回 即跳出函数。我们也可选从函数 返回一个值 。

python中if __name__ == '__main__': 的解析

当你打开一个.py文件时,经常会在代码的最下面看到if __name__ == '__main__':,现在就来介 绍一下它的作用.

        模块是对象，并且所有的模块都有一个内置属性 __name__。一个模块的 __name__ 的值取决于您如何应用模块。如果 import 一个模块，那么模块__name__ 的值通常为模块文件名，不带路径或者文件扩展名。但是您也可以像一个标准的程序样直接运行模块，在这 种情况下, __name__ 的值将是一个特别缺省"__main__"。

///////////////////////////////////////////////////////////////////////////////////////////////////

在cmd 中直接运行.py文件,则__name__的值是'__main__';

而在import 一个.py文件后,__name__的值就不是'__main__'了;

从而用if __name__ == '__main__'来判断是否是在直接运行该.py文件

如:

#Test.py

class Test:

    def __init(self):pass

    def f(self):print 'Hello, World!'

if __name__ == '__main__':

    Test().f()

#End

 

你在cmd中输入:

C:>python Test.py

Hello, World!

说明:"__name__ == '__main__'"是成立的

 

你再在cmd中输入:

C:>python

>>>import Test

>>>Test.__name__                #Test模块的__name__

'Test'

>>>__name__                       #当前程序的__name__

'__main__'

无论怎样,Test.py中的"__name__ == '__main__'"都不会成立的!

所以,下一行代码永远不会运行到!



python xlrd安装，可读取excel（Mac 终端里）
 513  cd Downloads/
  514  ls
  515  cd xlrd-0.9.3
  516  ls
  517  cd ..
  518  ls
  519  chmod 777 xlrd-0.9.3
  520  cd xlrd-0.9.3
  521  ls
  522  python setup.py install
  523  sudo python setup.py install
  

IndentationError: unexpected indent
python缩进问题
IndentationError: unexpected indent
xlwt安装，python中excel写
安装过程如下自己Mac下安装
  cd setuptools-18.0.1 #可以安装许多python库了
  623  sudo python setup.py install

  634  cd pip-7.0.3/ 安装xlwt需要？也许有setuptools就可以了
  637  sudo python setup.py install
  649  cd xlwt-1.0.0
  652  sudo pip install xlwt （没起作用，直接用 python setup.py install即可以安装了）
  653  sudo python setup.py install
  654  python
  
 在python中写excel表要用到xlwt[1] 模块，大致使用流程如下：
1、导入模块
import xlwt
2、创建workbook（其实就是excel，后来保存一下就行）
workbook = xlwt.Workbook(encoding = 'ascii')
3、创建表
worksheet = workbook.add_sheet('My Worksheet')
4、往单元格内写入内容
worksheet.write(0, 0, label = 'Row 0, Column 0 Value')
5、保存
workbook.save('Excel_Workbook.xls')
以上是最基本的功能。
ef write_excel():
    f = xlwt.Workbook() #创建工作簿

    '''
    创建第一个sheet:
        sheet1
    '''
    sheet1 = f.add_sheet(u'sheet1',cell_overwrite_ok=True) #创建sheet
    row0 = [u'业务',u'状态',u'北京',u'上海',u'广州',u'深圳',u'状态小计',u'合计']
    column0 = [u'机票',u'船票',u'火车票',u'汽车票',u'其它']
    status = [u'预订',u'出票',u'退票',u'业务小计']

    #生成第一行
    for i in range(0,len(row0)):
        sheet1.write(0,i,row0[i],set_style('Times New Roman',220,True))
        

bowtie -n 3 /Share/home/wangdong/lss/project/tmp/yy/first/mapping/ref_plus /Share/home/wangdong/lss/project/tmp/yy/first/Input_WFY-2/WYF-2_CGATGT_L007_R2_001_21nt_plus.fastq -S  /Share/home/wangdong/lss/project/tmp/yy/first/mapping/WYF-2_CGATGT_L007_R2_001_21nt_plus.sam
bowtie -n 3 /Share/home/wangdong/lss/project/tmp/yy/first/mapping/ref_plus /Share/home/wangdong/lss/project/tmp/yy/first/TFH_WFY-4/WYF-4_TGACCA_L007_R2_001_21nt_plus.fastq -S  /Share/home/wangdong/lss/project/tmp/yy/first/mapping/WYF-4_TGACCA_L007_R2_001_21nt_plus.sam
bowtie -n 3 /Share/home/wangdong/lss/project/tmp/yy/first/mapping/ref_plus /Share/home/wangdong/lss/project/tmp/yy/first/TH1_WFY-3/WYF-3_TTAGGC_L007_R2_001_21nt_plus.fastq -S  /Share/home/wangdong/lss/project/tmp/yy/first/mapping/WYF-3_TTAGGC_L007_R2_001_21nt_plus.sam
bowtie -n 3 /Share/home/wangdong/lss/project/tmp/yy/first/mapping/ref_rc /Share/home/wangdong/lss/project/tmp/yy/first/Input_WFY-2/WYF-2_CGATGT_L007_R2_001_21nt_rc.fastq -S  /Share/home/wangdong/lss/project/tmp/yy/first/mapping/WYF-2_CGATGT_L007_R2_001_21nt_rc.sam
bowtie -n 3 /Share/home/wangdong/lss/project/tmp/yy/first/mapping/ref_rc /Share/home/wangdong/lss/project/tmp/yy/first/TFH_WFY-4/WYF-4_TGACCA_L007_R2_001_21nt_rc.fastq -S  /Share/home/wangdong/lss/project/tmp/yy/first/mapping/WYF-4_TGACCA_L007_R2_001_21nt_rc.sam
bowtie -n 3 /Share/home/wangdong/lss/project/tmp/yy/first/mapping/ref_rc /Share/home/wangdong/lss/project/tmp/yy/first/TH1_WFY-3/WYF-3_TTAGGC_L007_R2_001_21nt_rc.fastq -S  /Share/home/wangdong/lss/project/tmp/yy/first/mapping/WYF-3_TTAGGC_L007_R2_001_21nt_rc.sam



cmap 密码和账号
lss
kaxmls0426

rasl自动化流程
0.或者利用extract_sample_with_barcode.py将.R直接提取出来
extract_sample_with_barcode.py
1.解压，合并fastq
unzip_after_extract_for_each_barcode.py
cat_after_extract_for_each_barcode.py
2.mapping
mapping.py
python /Share/home/wangdong/lss/project/compand_60/mapping/tmp/tmp_sub/mapping.py (写到mapping_rasl_test.sh里面，将这个提交即可)
3.sam获得统计结果
count_sam.py
4.将mapping的细节每个探针的值合并为一个大的excel表格
state_rasl_result_final.py
5.获得bowtie mapping ratio，将屏幕上打印出来的结果直接复制，或者将mapping.py放在一个.sh里面提交，即可将结果输出到log里面，利用下面脚本获得想要的mapping情况和raw reads
get_the_mapping_ratio.py（本地运行即可）
或者利用r

tabel_result<-read.table("final_result_table3.txt",head=T,row.names=1)
sum1<-apply(tabel_result,2,sum)
sum2<-t(sum1)
write.table(sum2,"sum.txt")

6.wc -l统计探针检测到值>1的个数
wc -l *.sam_3_result.fa >> result2.fa
所有以上结果均可以进行放在.sh里面提交

6.5去除stable median<5的列（此时的median没有将0全变为1）
stable<-read.table("stable_1.txt",head=T,row.names=1)
算好的各列的median当作一行，放入最后一行，进行转置将化合物名变成行
total<-read.table("total_1.txt",head=T,row.names=1)
ttoal<-t(total)
dim(ttoal)
row.names(ttoal)
ttoal
write.table(ttoal,'total_new_withme.txt')
写出来后进行将转置导致的第一行不是名字的去掉，让探针名和median上前成为title，然后就可以利用Median这一列进行过滤了
new1<-read.table('total_new_withme.txt',head=T,row.names=1)
dim(new1)
new<-subset(new1,Median>5)
dim(new)
tnew=t(new)
write.table(tnew,"new_filter_total.txt")

7.normalizastion R.script
stable<-read.table("stable.txt",head=T,row.names=1)
total<-read.table("total.txt",head=T,row.names=1)
me<-apply(stable,2,median)
for (i in 1:2704){
    total[,i]=log2(total[,i]/(me[i]+1)+1)
}
write.table(total,"normalized_by_median_of_stable_genes2.txt")
或者

stable<-read.table("stable_1.txt",head=T,row.names=1)
total<-read.table("total_1.txt",head=T,row.names=1)
me<-apply(stable,2,median)
head(me)
options(digits =2 )
for (i in 1:2564){
    total[,i]=100*total[,i]/me[i]
}
round(total,digits=2)
write.table(total,"normalized_by_median100_3.txt")

将rasl结果进行统计
 
8.计算每一列fc，但是将4个dmso变化来
最终用方法
treat<-read.table("lane7_plate1_treat.txt",head=T,row.names=1)
control<-read.table("lane7_plate1_dmso.txt",head=T,row.names=1)
treat<-(log2(treat/as.matrix(control[,1]))+log2(treat/as.matrix(control[,2]))+log2(treat/as.matrix(control[,3]))+log2(treat/as.matrix(control[,4]))+log2(treat/as.matrix(control[,5])))/5
write.table(treat,"lane7_plate1_fc.txt")

treat<-read.table("lane7_plate2_treat.txt",head=T,row.names=1)
control<-read.table("lane7_plate2_dmso.txt",head=T,row.names=1)
treat<-(log2(treat/as.matrix(control[,1]))+log2(treat/as.matrix(control[,2]))+log2(treat/as.matrix(control[,3]))+log2(treat/as.matrix(control[,4])))/4
write.table(treat,"lane7_plate2_fc.txt")

treat<-read.table("lane7_plate3_treat.txt",head=T,row.names=1)
control<-read.table("lane7_plate3_dmso.txt",head=T,row.names=1)
treat<-(log2(treat/as.matrix(control[,1]))+log2(treat/as.matrix(control[,2]))+log2(treat/as.matrix(control[,3]))+log2(treat/as.matrix(control[,4]))+log2(treat/as.matrix(control[,5])))/5
write.table(treat,"lane7_plate3_fc.txt")

treat<-read.table("lane7_plate4_treat.txt",head=T,row.names=1)
control<-read.table("lane7_plate4_dmso.txt",head=T,row.names=1)
treat<-(log2(treat/as.matrix(control[,1]))+log2(treat/as.matrix(control[,2]))+log2(treat/as.matrix(control[,3]))+log2(treat/as.matrix(control[,4]))+log2(treat/as.matrix(control[,5])))/5
write.table(treat,"lane7_plate4_fc.txt")

treat<-read.table("lane8_plate2_treat.txt",head=T,row.names=1)
control<-read.table("lane8_plate2_dmso.txt",head=T,row.names=1)
treat<-(log2(treat/as.matrix(control[,1]))+log2(treat/as.matrix(control[,2]))+log2(treat/as.matrix(control[,3]))+log2(treat/as.matrix(control[,4]))+log2(treat/as.matrix(control[,5])))/5
write.table(treat,"lane8_plate2_fc_3.txt")

treat<-read.table("lane8_plate3_treat.txt",head=T,row.names=1)
control<-read.table("lane8_plate3_dmso.txt",head=T,row.names=1)
treat<-(log2(treat/as.matrix(control[,1]))+log2(treat/as.matrix(control[,2]))+log2(treat/as.matrix(control[,3]))+log2(treat/as.matrix(control[,4])))/4
write.table(treat,"lane8_plate3_fc.txt")

treat<-read.table("lane8_plate4_treat.txt",head=T,row.names=1)
control<-read.table("lane8_plate4_dmso.txt",head=T,row.names=1)
treat<-(log2(treat/as.matrix(control[,1]))+log2(treat/as.matrix(control[,2]))+log2(treat/as.matrix(control[,3]))+log2(treat/as.matrix(control[,4])))/4
write.table(treat,"lane8_plate4_fc_4.txt")


total<-read.table("Lane7_plate1_D7_control.txt",head=T,row.names=1)
control<-total[1:5]
treat<-total[6:384]
for (i in 1:1273){
    control[i,]=log2(treat[i,]/control[i,])
write.table(control,"treatvs_control_log2.txt")
新编融合fc为一个

以前方法
total<-read.table("Lane7_plate1_D7_control.txt",head=T,row.names=1)
control<-total[1:5]
for (j in 1:2705){
    treat<-total[,j]
    for (i in 1:1273){
        control[i,]=log2(treat[i,]/control[i,])
        }
    total[,j]<-apply(control,1,mean)
}
write.table(control,"treatvs_control_log2.txt")

9.将大excel表格按药进行拆分得到各自的txt
> a<-read.table("lane7_plate1_fc_test.txt",head=T,row.names=1)
> dim(a)
[1] 1273  375
> write.table(a,"lane7_plate1_fc_test2.txt",sep="\t")
先在r里面改变分隔符, 不要列标题，因为.rnk里直接是以基因开始,可以有标题，中间要有‘\t’隔开
python
split_excel_to_files_by_compound.py
paste.py
（
cut -f 1 total2.txt > name.txt
for i in 
cut -f 3 total2.txt > test_col3.txt
paste test.txt test_col3.txt > col3.txt
cut -f 1 total2.txt > test.txt
cut -f 1 test.txt
paste file1 file2 > file3

 paste [-d] file1 file2
选项与参数：
-d  ：后面可以接分隔字符。默认是以 [tab] 来分隔的！通过交换文件名即可指定哪一列先粘：）

10.安装gsea 命令行的软件，直接下载gsea2-2.2.0.jar，然后告诉路径在哪儿即可，如下面的命令（适合自己的）
例子
java -cp /Users/lishasha/Desktop/home/tools/gsea2-2.2.0.jar -Xmx512m xtools.gsea.GseaPreranked -gmx /Users/lishasha/Desktop/home/new_work/20150801_3000drugs/gsea/lung.gmt -collapse false -mode Max_probe -norm meandiv -nperm 1000 -rnk /Users/lishasha/Desktop/home/new_work/20150801_3000drugs/gsea/fc.rnk -scoring_scheme classic -rpt_label lung -include_only_symbols true -make_sets true -plot_top_x 20 -rnd_seed timestamp -set_max 500 -set_min 0 -zip_report false -out /Users/lishasha/gsea_home/output -gui false
正式写入python的脚本
java -cp /Users/lishasha/Desktop/home/tools/gsea2-2.2.0.jar -Xmx512m xtools.gsea.GseaPreranked -gmx /Users/lishasha/Desktop/home/new_work/20150801_3000drugs/gsea/plan1/again/new/fc/run_gsea/CTNNB1_UP.gmt -collapse false -mode Max_probe -norm meandiv -nperm 1000 -rnk /Users/lishasha/Desktop/home/new_work/20150801_3000drugs/gsea/fc.rnk{} -scoring_scheme classic -rpt_label {*} -include_only_symbols true -make_sets true -plot_top_x 20 -rnd_seed timestamp -set_max 500 -set_min 0 -zip_report false -out /Users/lishasha/gsea_home/output/gsea -gui false
c集群上
java -cp /Share/home/wangdong/local/app/gsea/gsea2-2.2.0.jar -Xmx512m xtools.gsea.GseaPreranked -gmx /Users/lishasha/Desktop/home/new_work/20150801_3000drugs/gsea/plan1/again/new/fc/run_gsea/CTNNB1_UP.gmt -collapse false -mode Max_probe -norm meandiv -nperm 1000 -rnk /Users/lishasha/Desktop/home/new_work/20150801_3000drugs/gsea/fc.rnk{} -scoring_scheme classic -rpt_label {*} -include_only_symbols true -make_sets true -plot_top_x 20 -rnd_seed timestamp -set_max 500 -set_min 0 -zip_report false -out /Users/lishasha/gsea_home/output/gsea -gui false
python /Share/home/wangdong/lss/script/run_gsea.py
AttributeError: 'tuple' object has no attribute 'write'
A集群
perl bsub.pl one.sh 1 TINY ./source.txt
perl bsub.pl f500.sh 1 TINY ./source.txt
perl bsub.pl f1001.sh 1 TINY ./source.txt
perl bsub.pl f1501.sh 1 TINY ./source.txt
perl bsub.pl f2000.sh 1 TINY ./source.txt

11.提取出gsea里的数据，可以分成多份运行
在a集群上可以运行40份没有问题
 for k in {1..40}; do python /Work1/home/wangdong/lss/20150801compoud/run_gsea.py & done
 python /Work1/home/wangdong/lss/20150801compoud/lung/run_gsea_up.py
for k in {1..40}; do python /Work1/home/wangdong/lss/20150801compoud/lung/run_gsea_up.py & done
for k in {1..40}; do python /Work1/home/wangdong/lss/20150801compoud/lung/run_gsea_down.py & done
lung_up.sh
perl bsub.pl lung_up.sh 1 TINY ./source.txt
perl bsub.pl lung_down.sh 1 TINY ./source.txt

python /Work1/home/wangdong/lss/20150801compoud/lung/lung_down_one.py
python /Work1/home/wangdong/lss/20150801compoud/lung/lung_down_500.py
python /Work1/home/wangdong/lss/20150801compoud/lung/lung_down_2000.py
python /Work1/home/wangdong/lss/20150801compoud/lung/lung_down_1500.py
python /Work1/home/wangdong/lss/20150801compoud/lung/lung_down_1000.py
python /Work1/home/wangdong/lss/20150801compoud/lung/lung_up_1000.py
python /Work1/home/wangdong/lss/20150801compoud/lung/lung_up_1500.py
python /Work1/home/wangdong/lss/20150801compoud/lung/lung_up_2000.py
python /Work1/home/wangdong/lss/20150801compoud/lung/lung_up_500.py
python /Work1/home/wangdong/lss/20150801compoud/lung/lung_up_one.py


perl bsub.pl lung_down_one.sh 1 TINY ./source.txt
perl bsub.pl lung_down_500.sh 1 TINY ./source.txt
perl bsub.pl lung_down_2000.sh 1 TINY ./source.txt
perl bsub.pl lung_down_1500.sh 1 TINY ./source.txt
perl bsub.pl lung_down_1000.sh 1 TINY ./source.txt
perl bsub.pl lung_up_1000.sh 1 TINY ./source.txt
perl bsub.pl lung_up_1500.sh 1 TINY ./source.txt
perl bsub.pl lung_up_2000.sh 1 TINY ./source.txt
perl bsub.pl lung_up_500.sh 1 TINY ./source.txt	
perl bsub.pl lung_up_one.sh 1 TINY ./source.txt

FOR capitalbio data
挑选出30个stable gene, 15个样本，挑选>10的
#mapping上reads作图
hist(a$differential,breaks=6,col="gray",xlab="log10(number of aligned reads)",main="The distribution of number of aligned reads")

6.5去除stable median<5的列（此时的median没有将0全变为1）
stable<-read.table("stable.txt",head=T,row.names=1)
me<-apply(stable,2,median)
write.table(me,"me_30_stable_genes.txt")

算好的各列的median当作一行，放入最后一行，进行转置将化合物名变成行
total<-read.table("stable.txt",head=T,row.names=1)
ttoal<-t(total)
dim(ttoal)
row.names(ttoal)
ttoal
write.table(ttoal,'total_new_withme.txt')
写出来后进行将转置导致的第一行不是名字的去掉，让探针名和median上前成为title，然后就可以利用Median这一列进行过滤了
new1<-read.table('total_new_withme.txt',head=T,row.names=1)
dim(new1)
new<-subset(new1,Median>5)
dim(new)
write.table(new,"new_filter_total.txt")

7.normalizastion R.script
stable<-read.table("stable_filtered.txt",head=T,row.names=1)
me<-apply(stable,2,median)
total<-read.table("total_filtered.txt",head=T,row.names=1)
for (i in 1:901){
    total[,i]=log2(total[,i]/me[i])
}
write.table(total,"normalized_by_median_of_stable_genes2.txt")
或者

stable<-read.table("stable_filtered.txt",head=T,row.names=1)
total<-read.table("total_filtered.txt",head=T,row.names=1)
me<-apply(stable,2,median)
head(me)
for (i in 1:901){
    total[,i]=round(100*total[,i]/me[i],digits=2)
}
write.table(total,"normalized_by_median100.txt")

将rasl结果进行统计
 
8.计算每一列fc，但是将4个dmso变化来
先将dmso去一去,其实大可不必将dmso分出来，直接给一个表，然后告诉位置即可

#edit for each plate split

total<-read.table("normalized_by_median100.txt",head=T,row.names=1)
	ttotal<-t(total)
	write.table(ttotal,"normalized_by_median100_t.txt")
#heatmap for dmso

library(pheatmap)
pdf(file="log(dmso)_final.pdf")
data<-read.table("log(dmso).txt",head=T)
data<-data.matrix(data)
pheatmap(data,cluster_row=TRUE,clustering_method="average",cluster_col=FALSE,cellwidth=4,show_rownames=F,fontsize_col=6,las=2)
dev.off()

最终用方法
treat<-read.table("plate1_8_treat.txt",head=T,row.names=1)
control<-read.table("plate1_8_dmso.txt",head=T,row.names=1)
treat<-(log2(treat/as.matrix(control[,1]))+log2(treat/as.matrix(control[,2]))+log2(treat/as.matrix(control[,3]))+log2(treat/as.matrix(control[,4]))+log2(treat/as.matrix(control[,5])))/5
write.table(treat,"plate1_8_fc.txt")

treat<-read.table("plate1_20_treat.txt",head=T,row.names=1)
control<-read.table("plate1_20_dmso.txt",head=T,row.names=1)
treat<-(log2(treat/as.matrix(control[,1]))+log2(treat/as.matrix(control[,2]))+log2(treat/as.matrix(control[,3]))+log2(treat/as.matrix(control[,4]))+log2(treat/as.matrix(control[,5]))+log2(treat/as.matrix(control[,6])))/6
write.table(treat,"plate1_20_fc.txt")

treat<-read.table("plate2_8_treat.txt",head=T,row.names=1)
control<-read.table("plate2_8_dmso.txt",head=T,row.names=1)
treat<-(log2(treat/as.matrix(control[,1]))+log2(treat/as.matrix(control[,2]))+log2(treat/as.matrix(control[,3]))+log2(treat/as.matrix(control[,4]))+log2(treat/as.matrix(control[,5]))+log2(treat/as.matrix(control[,6])))/6
write.table(treat,"plate2_8_fc_2.txt")

treat<-read.table("plate2_20_treat.txt",head=T,row.names=1)
control<-read.table("plate2_20_dmso.txt",head=T,row.names=1)
treat<-(log2(treat/as.matrix(control[,1]))+log2(treat/as.matrix(control[,2]))+log2(treat/as.matrix(control[,3]))+log2(treat/as.matrix(control[,4]))+log2(treat/as.matrix(control[,5]))+log2(treat/as.matrix(control[,6])))/6
write.table(treat,"plate2_20_fc.txt")

treat<-read.table("plate3_8_treat.txt",head=T,row.names=1)
control<-read.table("plate3_8_dmso.txt",head=T,row.names=1)
treat<-(log2(treat/as.matrix(control[,1]))+log2(treat/as.matrix(control[,2]))+log2(treat/as.matrix(control[,3]))+log2(treat/as.matrix(control[,4]))+log2(treat/as.matrix(control[,5]))+log2(treat/as.matrix(control[,6])))/6
write.table(treat,"plate3_8_fc.txt")

treat<-read.table("plate3_20_treat.txt",head=T,row.names=1)
control<-read.table("plate3_20_dmso.txt",head=T,row.names=1)
treat<-(log2(treat/as.matrix(control[,1]))+log2(treat/as.matrix(control[,2]))+log2(treat/as.matrix(control[,3]))+log2(treat/as.matrix(control[,4]))+log2(treat/as.matrix(control[,5]))+log2(treat/as.matrix(control[,6])))/6
write.table(treat,"plate3_20_fc.txt")

treat<-read.table("plate4_8_treat.txt",head=T,row.names=1)
control<-read.table("plate4_8_dmso.txt",head=T,row.names=1)
treat<-(log2(treat/as.matrix(control[,1]))+log2(treat/as.matrix(control[,2]))+log2(treat/as.matrix(control[,3]))+log2(treat/as.matrix(control[,4]))+log2(treat/as.matrix(control[,5]))+log2(treat/as.matrix(control[,6])))/6
write.table(treat,"plate4_8_fc.txt")

treat<-read.table("plate4_20_treat.txt",head=T,row.names=1)
control<-read.table("plate4_20_dmso.txt",head=T,row.names=1)
treat<-(log2(treat/as.matrix(control[,1]))+log2(treat/as.matrix(control[,2]))+log2(treat/as.matrix(control[,3]))+log2(treat/as.matrix(control[,4]))+log2(treat/as.matrix(control[,5]))+log2(treat/as.matrix(control[,6])))/6
write.table(treat,"plate4_20_fc.txt")

total<-read.table("Lane7_plate1_D7_control.txt",head=T,row.names=1)
control<-total[1:5]
treat<-total[6:384]
for (i in 1:1273){
    control[i,]=log2(treat[i,]/control[i,])
write.table(control,"treatvs_control_log2.txt")
新编融合fc为一个

以前方法
total<-read.table("Lane7_plate1_D7_control.txt",head=T,row.names=1)
control<-total[1:5]
for (j in 1:2705){
    treat<-total[,j]
    for (i in 1:1273){
        control[i,]=log2(treat[i,]/control[i,])
        }
    total[,j]<-apply(control,1,mean)
}
write.table(control,"treatvs_control_log2.txt")

8.5. 先保证结果是可信的，过滤低表达的基因
13.另外的途径获得每个药严格的signature
(1)从normalize后的数据开始进行计算
#利用每个板的5dmso的平均值作为一列进行过滤
fc<-read.table("lane7_plate_fc.txt",head=T,row.names=1,sep="\t")
dmso<-read.table("lane7_plate1_dmso_average.txt",head=T,row.names=1,sep="\t")
treat<-read.table("lane7_plate1_treat.txt",head=T,row.names=1,sep="\t")
stable<-read.table("lane7_plate1_stable.txt",head=T,row.names=1,sep="\t")
me<-apply(stable,2,median)
dmso_cutoff<-as.numeric(dmso[,1])
index1<-which(dmso_cutoff<=8.677707)             #8.677707=(5*100/63+5*100/51+5*100/63+5*100/55+5*100/58)/5 即将5个dmso的stable gene的中位值找出来 
Time <- rep(0, length(dmso_cutoff))
fc[index1,]<-as.numeric(Time[index1])

#利用treat自己对fc进行过滤，变成0
for (i in 1:375){
    cutoff<-as.numeric(treat[,i])
    indexi<-which(cutoff<=100*5/me[i])
    Timei <- rep(0, length(cutoff))
    fc[indexi,i]<-as.numeric(Timei[indexi])
}
write.table(fc,"lane7_plate1_fc_filtered_by_median5.txt")  #gsea可能用这个ranked list


9.将大excel表格按药进行拆分得到各自的txt
> a<-read.table("lane7_plate1_fc_test.txt",head=T,row.names=1)
> dim(a)
[1] 1273  375
> write.table(a,"total_r.txt",sep="\t")
先在r里面改变分隔符, 不要列标题，因为.rnk里直接是以基因开始,可以有标题，中间要有‘\t’隔开
python
split_excel_to_files_by_compound.py
paste.py
（
cut -f 1 total2.txt > name.txt
for i in 
cut -f 3 total2.txt > test_col3.txt
paste test.txt test_col3.txt > col3.txt
cut -f 1 total2.txt > test.txt
cut -f 1 test.txt
paste file1 file2 > file3

 paste [-d] file1 file2
选项与参数：
-d  ：后面可以接分隔字符。默认是以 [tab] 来分隔的！通过交换文件名即可指定哪一列先粘：）

10.安装gsea 命令行的软件，直接下载gsea2-2.2.0.jar，然后告诉路径在哪儿即可，如下面的命令（适合自己的）
例子
java -cp /Users/lishasha/Desktop/home/tools/gsea2-2.2.0.jar -Xmx512m xtools.gsea.GseaPreranked -gmx /Users/lishasha/Desktop/home/new_work/20150801_3000drugs/gsea/lung.gmt -collapse false -mode Max_probe -norm meandiv -nperm 1000 -rnk /Users/lishasha/Desktop/home/new_work/20150801_3000drugs/gsea/fc.rnk -scoring_scheme classic -rpt_label lung -include_only_symbols true -make_sets true -plot_top_x 20 -rnd_seed timestamp -set_max 500 -set_min 0 -zip_report false -out /Users/lishasha/gsea_home/output -gui false
正式写入python的脚本
java -cp /Users/lishasha/Desktop/home/tools/gsea2-2.2.0.jar -Xmx512m xtools.gsea.GseaPreranked -gmx /Users/lishasha/Desktop/home/new_work/20150801_3000drugs/gsea/plan1/again/new/fc/run_gsea/CTNNB1_UP.gmt -collapse false -mode Max_probe -norm meandiv -nperm 1000 -rnk /Users/lishasha/Desktop/home/new_work/20150801_3000drugs/gsea/fc.rnk{} -scoring_scheme classic -rpt_label {*} -include_only_symbols true -make_sets true -plot_top_x 20 -rnd_seed timestamp -set_max 500 -set_min 0 -zip_report false -out /Users/lishasha/gsea_home/output/gsea -gui false
c集群上
java -cp /Share/home/wangdong/local/app/gsea/gsea2-2.2.0.jar -Xmx512m xtools.gsea.GseaPreranked -gmx /Users/lishasha/Desktop/home/new_work/20150801_3000drugs/gsea/plan1/again/new/fc/run_gsea/CTNNB1_UP.gmt -collapse false -mode Max_probe -norm meandiv -nperm 1000 -rnk /Users/lishasha/Desktop/home/new_work/20150801_3000drugs/gsea/fc.rnk{} -scoring_scheme classic -rpt_label {*} -include_only_symbols true -make_sets true -plot_top_x 20 -rnd_seed timestamp -set_max 500 -set_min 0 -zip_report false -out /Users/lishasha/gsea_home/output/gsea -gui false
python /Share/home/wangdong/lss/script/run_gsea.py
AttributeError: 'tuple' object has no attribute 'write'
A集群
perl bsub.pl one.sh 1 TINY ./source.txt
perl bsub.pl f500.sh 1 TINY ./source.txt
perl bsub.pl f1001.sh 1 TINY ./source.txt
perl bsub.pl f1501.sh 1 TINY ./source.txt
perl bsub.pl f2000.sh 1 TINY ./source.txt

11.提取出gsea里的数据，可以分成多份运行
在a集群上可以运行40份没有问题
 for k in {1..40}; do python /Work1/home/wangdong/lss/20150801compoud/run_gsea.py & done
 python /Work1/home/wangdong/lss/20150801compoud/lung/run_gsea_up.py
for k in {1..40}; do python /Work1/home/wangdong/lss/20150801compoud/lung/run_gsea_up.py & done
for k in {1..40}; do python /Work1/home/wangdong/lss/20150801compoud/lung/run_gsea_down.py & done
lung_up.sh
perl bsub.pl lung_up.sh 1 TINY ./source.txt
perl bsub.pl lung_down.sh 1 TINY ./source.txt

python /Work1/home/wangdong/lss/20150801compoud/lung/lung_down_one.py
python /Work1/home/wangdong/lss/20150801compoud/lung/lung_down_500.py
python /Work1/home/wangdong/lss/20150801compoud/lung/lung_down_2000.py
python /Work1/home/wangdong/lss/20150801compoud/lung/lung_down_1500.py
python /Work1/home/wangdong/lss/20150801compoud/lung/lung_down_1000.py
python /Work1/home/wangdong/lss/20150801compoud/lung/lung_up_1000.py
python /Work1/home/wangdong/lss/20150801compoud/lung/lung_up_1500.py
python /Work1/home/wangdong/lss/20150801compoud/lung/lung_up_2000.py
python /Work1/home/wangdong/lss/20150801compoud/lung/lung_up_500.py
python /Work1/home/wangdong/lss/20150801compoud/lung/lung_up_one.py


perl bsub.pl lung_down_one.sh 1 TINY ./source.txt
perl bsub.pl lung_down_500.sh 1 TINY ./source.txt
perl bsub.pl lung_down_2000.sh 1 TINY ./source.txt
perl bsub.pl lung_down_1500.sh 1 TINY ./source.txt
perl bsub.pl lung_down_1000.sh 1 TINY ./source.txt
perl bsub.pl lung_up_1000.sh 1 TINY ./source.txt
perl bsub.pl lung_up_1500.sh 1 TINY ./source.txt
perl bsub.pl lung_up_2000.sh 1 TINY ./source.txt
perl bsub.pl lung_up_500.sh 1 TINY ./source.txt	
perl bsub.pl lung_up_one.sh 1 TINY ./source.txt

或者利用一下的方法进行分份
get_the_sh.py
然后在split到100个文件里面，批量添加后缀.sh然后提交2563/50=51个sh,14:57开始,
split -l 50 try.sh gsea
用find和xargs添加后缀名
find . -type f |xargs -t -i mv {} {}.sh
find . -type f |xargs -i mv {} {}.txt

.  当前目录
type  查找某一类型的文件，诸如：
b - 块设备文件。
d - 目录。
c - 字符设备文件。
p - 管道文件。
l - 符号链接文件。
f - 普通文件。
＃
将上述列表作为参数进行传递，一次传递一个。xargs 命令允许你这样做。最后一部分，xargs ls -ltr，用于接收输出并对其执行 ls -ltr 命令，如下所示：
$ ls | xargs -t -i mv {} {}.bak
-i 选项告诉 xargs 用每项的名称替换 {}。-t 选项指示 xargs 先打印命令，然后再执行。


12将gsea数据提取出来
extract_gsea_result.py

13.另外的途径获得每个药严格的signature
博奥自己的计算方式
 #下次这个值直接用公式算,确定好stable中dmso的位置，可以给一个没按照板分的没过滤的stable中dmso的位置信息，不过得将0变成1，每个板子不一样，下一次看看可不可以放在最前面，
所以这个5下次可以定义成Ntotal*8.8%/Gtotal（上次的5平均值摸索为0.088）,如本次为0.09*Ntotal／2033，（上次的5中位值摸索为0.09）0.004426955*Ntotal*100=0.4426955*Ntotal
0.4426955*Ntotal＊1/5*(1/27+1/13.5+1/24+1/20.5+1/18),  最终0.257113821＊0.000885391*Ntotal就是lane6_plate1_8的大约是5的值，而每块版只需要改（1/27+1/13.5+1/24+1/20.5+1/18)和1/5，而Ntotal还需要准备一个没有哦normal的总的值
#本次先用5,原因是对于总reads更小的过滤太不严格了如0.02m 20000／2033=9.8条平均 
这样做有问题，还需要重新过滤，dmso都排到前5去了，且基因太多1000个
fc<-read.table("plate4_20_fc.txt",head=T,row.names=1,sep="\t")
dmso<-read.table("plate4_20_dmso.txt",head=T,row.names=1,sep="\t")
treat<-read.table("plate4_20_treat.txt",head=T,row.names=1,sep="\t")
stable<-read.table("plate4_20_stable.txt",head=T,row.names=1,sep="\t")
dmso1<-apply(dmso,1,mean)
me<-apply(stable,2,median)
dmso_cutoff<-as.numeric(dmso1)
index1<-which(dmso_cutoff<=3.20364165)
Time <- rep(0, length(dmso_cutoff))
fc[index1,]<-as.numeric(Time[index1])
write.table(fc,"test.txt") # 看看dmso过滤情况
for (i in 1:107){
    cutoff<-as.numeric(treat[,i])
    indexi<-which(cutoff<=100*5/me[i])
    Timei <- rep(0, length(cutoff))
    fc[indexi,i]<-as.numeric(Timei[indexi])
}
write.table(fc,"plate4_20_fc_filtered_by_median5.txt")

for (i in 1:107){
    cutoff<-as.numeric(fc[,i])
    indexi<-which(cutoff <1 & cutoff >-1)
    Timei <- rep(0, length(cutoff))
    fc[indexi,i]<-as.numeric(Timei[indexi])
}
write.table(fc,"plate4_20_fc_by_minor1to1.txt")

for (i in 1:107){
    cutoff<-as.numeric(fc[,i])
    indexi<-which(cutoff!=0)
    Timei <- rep(10, length(cutoff))
    fc[indexi,i]<-as.numeric(Timei[indexi])
}
write.table(fc,"plate4_20_fc_modified_final_v2.txt")

a<-matrix(rep(1:107,2), nrow = 2)
for (i in 1:107){
    a[,i]<-table(fc[,i])
}

colnames(a) <- colnames(fc)
write.table(a,"plate4_20_clasification.txt")


(1)从normalize后的数据开始进行计算
#利用每个板的5dmso的平均值作为一列进行过滤
fc<-read.table("lane7_plate1_fc.txt",head=T,row.names=1,sep="\t")
dmso<-read.table("lane7_plate1_dmso_average.txt",head=T,row.names=1,sep="\t")
treat<-read.table("lane7_plate1_treat.txt",head=T,row.names=1,sep="\t")
stable<-read.table("lane7_plate1_stable.txt",head=T,row.names=1,sep="\t")
me<-apply(stable,2,median)
dmso_cutoff<-as.numeric(dmso[,1])
index1<-which(dmso_cutoff<=5.677707)
Time <- rep(0, length(dmso_cutoff))
fc[index1,]<-as.numeric(Time[index1])

#利用treat自己对fc进行过滤，变成0
for (i in 1:375){
    cutoff<-as.numeric(treat[,i])
    indexi<-which(cutoff<=100*5/me[i])
    Timei <- rep(0, length(cutoff))
    fc[indexi,i]<-as.numeric(Timei[indexi])
}
write.table(fc,"lane7_plate1_fc_filtered_by_median5.txt")  #gsea可能用这个ranked list

#将-1<fc<1为0，可以将全部的板子合在一起一起过滤
for (i in 1:375){
    cutoff<-as.numeric(fc[,i])
    indexi<-which(cutoff <1 & cutoff >-1)
    Timei <- rep(0, length(cutoff))
    fc[indexi,i]<-as.numeric(Timei[indexi])
}
write.table(fc,"lane7_plate1_fc_by_minor1to1.txt") #gene signature细节看这个

#将fc变成>1 <-1为一类
for (i in 1:375){
    cutoff<-as.numeric(fc[,i])
    indexi<-which(cutoff!=0)
    Timei <- rep(10, length(cutoff))
    fc[indexi,i]<-as.numeric(Timei[indexi])
}
write.table(fc,"lane7_plate1_fc_modified_final_v2.txt") #gene signature筛选看这个

a<-matrix(rep(1:375,2), nrow = 2)
for (i in 1:375){
    a[,i]<-table(fc[,i])
}

colnames(a) <- colnames(fc)
write.table(a,"lane7_plate1_clasification.txt")
#对于>1 <－1的基因，均变成1，－1，为了heatmap更加明显
fc<-read.table("total_fc.txt",head=T,row.names=1)
for (i in 1:2564){
    cutoff<-as.numeric(fc[,i])
    indexi<-which(cutoff>=1)
    Timei <- rep(1, length(cutoff))
    fc[indexi,i]<-as.numeric(Timei[indexi])
}
write.table(fc,"total_above1.txt")

for (i in 1:2564){
    cutoff<-as.numeric(fc[,i])
    indexi<-which(cutoff<=-1)
    Timei <- rep(-1, length(cutoff))
    fc[indexi,i]<-as.numeric(Timei[indexi])
}
write.table(fc,"total_above1_below1.txt")

#利用fc作heatmap去除所有样品中均是0的基因
a<- matrix(1:12,nrow=3,ncol=4)
keep = rowSums(a)!=26
p = a[keep1,]
#上面实验例子,下面正式运行，利用fc作heatmap去除所有样品中均是0的基因，对于>1 <－1的基因，均变成1，－1，为了heatmap更加明显
fc<-read.table("test.txt",head=T,row.names=1,sep='\t')
keep = rowSums(fc)!=0
fc = fc[keep,]
write.table(fc,"test_remove0.txt")

for (i in 1:107){
    cutoff<-as.numeric(fc[,i])
    indexi<-which(cutoff>=1)
    Timei <- rep(1, length(cutoff))
    fc[indexi,i]<-as.numeric(Timei[indexi])
}
write.table(fc,"total_above1.txt")

for (i in 1:107){
    cutoff<-as.numeric(fc[,i])
    indexi<-which(cutoff<=-1)
    Timei <- rep(-1, length(cutoff))
    fc[indexi,i]<-as.numeric(Timei[indexi])
}
write.table(fc,"total_above1_below1.txt")

#pheatmap进行标注，两个方向
library(pheatmap)

data<-read.table("7702final.txt",head=T,row.names=1,sep='\t')
data<-data.matrix(data)
ann_col<-read.table("ann_col.txt",head=T,row.names=1,sep='\t')
ann_row<-read.table("ann_row.txt",head=T,row.names=1,sep='\t')


annotation_col = data.frame(
time=factor(ann_col$time),yx=factor(ann_col$property)
)

rownames(annotation_col)=rownames(ann_col)

annotation_row = data.frame(
gene_pathwat=factor(ann_row$pathway)
)

rownames(annotation_row) =rownames(ann_row)

pdf(file="test_final2.pdf",height=20,width=20)
pheatmap(data,cluster_row=TRUE,cluster_col=TRUE,show_rownames=F,show_colnames=F,color = colorRampPalette(c("navy", "white", "firebrick3"))(50),cellwidth=0.8,cellheight=0.4,fontsize_col=8,annotation_col = annotation_col,annotation_row = annotation_row)
dev.off()
q()



#作图
a<-read.table("lane6_plate1_20.txt",head=T,row.names=1,sep="\t")
pdf("bar_lane6_plate1_20.pdf",height=10,width=20)
barplot(a$diff_gene,col=as.factor(a$factor))
dev.off()
#将各组进行box plot的比较
a<-read.table("lane6_plate1_8.txt",head=T,row.names=1)
a<-read.table("lane6_plate1_8.txt",head=T,row.names=1)

b<-read.table("not_worked_sw_probe_list_tm.txt",head=T)
b<-read.table("not_worked_sw_probe_list_tm.txt",head=T)
b<-read.table("not_worked_sw_probe_list_tm.txt",head=T)
head(b)
pdf("Tm.pdf")
boxplot(a$Tm_a,a$Tm_d,b$Tm_a,b&Tm_d),col=c("red","blue"),ylab="The Tm Value of probes",las=1,font.lab=2))
boxplot(a$Tm_a,a$Tm_d,b$Tm_a,b&Tm_d,col=c("red","blue"),ylab="The Tm Value of probes",las=1,font.lab=2)
boxplot(a$Tm_a,a$Tm_d,b$Tm_a,b$Tm_d,col=c("red","blue"),ylab="The Tm Value of probes",las=1,font.lab=2)
dev.off()


a<-read.table("plot2.txt",head=T,sep="\t")
> pdf("scatter_plot7.pdf")
plot(a$Lane8_plate2_I1,a$Lane8_plate2_G24,col=as.factor(a$factor),lty=2,pch=20,cex=0.5,xlab="Argatroban",ylab="dmso")
NAME 替换成^t,
plot(a$Lane8_plate2_I1,a$Lane8_plate2_G24,col=as.factor(a$factor),lty=2,pch=20,cex=0.5,xlab="log2(hits of Argatroban)",ylab="log2(hits of dmso)")


a<-read.table("Lane7_plate1_N24_DAPT_3.txt",head=T,sep="\t")
pdf("Lane7_plate1_N24_DAPT_3.pdf")
plot(a$Lane7_plate1_N24_DAPT_3,a$Lane7_plate1_G23,col=as.factor(a$factor),lty=2,pch=20,cex=0.5,xlab="log2(Lane7_plate1_N24_DAPT_3)",ylab="log2(hits of dmso)")
dev.off
a<-read.table("Lane7_plate2_K24_dmso_11.txt",head=T,sep="\t")
pdf("Lane7_plate2_K24_dmso_11.pdf")
plot(a$Lane7_plate2_K24_dmso_11,a$Lane7_plate2_G24,col=as.factor(a$factor),lty=2,pch=20,cex=0.5,xlab="log2(Lane7_plate2_K24_dmso_11)",ylab="log2(hits of dmso)")
dev.off()

a<-read.table("Lane8_plate2_I1.txt",head=T,sep="\t")
pdf("Lane8_plate2_I1_3.pdf")
plot(a$Lane8_plate2_dmso_average,a$Lane8_plate2_I1,col=as.factor(a$factor),lty=2,pch=20,cex=0.5,xlab="log2(hits of dmso)",ylab="log2(Lane8_plate2_I1)")
abline(lm(a$Lane8_plate2_I1~a$Lane8_plate2_dmso_average))
dev.off()

利用r作正太分布曲线
a<-read.table("diff_gene.txt")
a
hist(a$V1,breaks=10,col="red")
hist(a$V1,breaks=5,col="red")
hist(a$V1,breaks=100,col="red")
hist(a$V1,breaks=20,col="red")
hist(a$V1,breaks=18,col="red")
hist(a$V1,breaks=36,col="red")
hist(a$V1,breaks=72,col="red")
420/72
hist(a$V1,breaks=36,col="red")
hist(a$V1,breaks=72,col="red")

a<-read.table("diff_gene7.txt",head=T,sep='\t')
密度的
hist(a$differential.expressed.genes,freq=FALSE,breaks=72,col="red",xlab="number of differential expressed genes",main="Histogram,density curve")
rug(jitter(a$differential.expressed.genes))
lines(density(a$differential.expressed.genes),col="blue",lwd=2)
pdf("hist_density.pdf")
hist(a$differential.expressed.genes,freq=FALSE,breaks=72,col="red",xlab="number of differential expressed genes",main="Histogram,density curve")
rug(jitter(a$differential.expressed.genes))
lines(density(a$differential.expressed.genes),col="blue",lwd=2)
dev.off()
频率的
pdf("hist_freq_try_two_factor_tmp_try2.pdf")
x<-a$DIFFERENTIAL.EXPRESSED.GENES
x
h<-hist(x,,breaks=72,col=as.factor(a$FACTOR),xlab="number of differential expressed genes",main="Histogram with normal curve")
xfit<-seq(min(x),max(x),length=40)
yfit<-dnorm(xfit,mean=mean(x),sd=sd(x))
yfit<-yfit*diff(h$mids[1:2])*length(x)
lines(xfit,yfit,col="blue",lwd=2)
box()
legend(300,100,c("dmso","others"),cex=1.5,fill=1:3)
dev.off()
CTNNB1_UP替换成^t,
#作heatmap
pheatmap 位置
/Share/home/wangdong/lss/project/20150430_sw_probes_screen/plot
library(pheatmap)
pdf(file="capitalbio_bzk_dmso2.pdf")
data<-read.table("dmso_heatmap.txt",head=T,sep='\t',row.names=1)
data1<-log2(data)
data<-data.matrix(data1)
pheatmap(data,cluster_row=TRUE,clustering_method="average",cluster_col=TRUE,cellwidth=8,fontsize_row=2,fontsize_col=8,las=2，color = colorRampPalette(c("navy", "white", "firebrick3"))
dev.off()
q()

pheatmap(test, color = colorRampPalette(c("navy", "white", "firebrick3"))(50))

library(pheatmap)
pdf(file="LXH.pdf")
data<-read.table("brain_no_down_ranked.txt",head=T,sep='\t',row.names=1)
data1<-log2(data)
data<-data.matrix(data1)
pheatmap(data,show_rownames = F,las=2,color = colorRampPalette(c("navy", "white", "firebrick3"))(50))
dev.off()


gsea
java -Xmx512m xtools.gsea.GseaPreranked -gmx gseaftp.broadinstitute.org://pub/gsea/gene_sets/c2.cp.kegg.v5.0.symbols.gmt -collapse false -mode Max_probe -norm meandiv -nperm 1000 -rnk /Users/lishasha/Desktop/home/new_work/20150801_3000drugs/gsea/fc.rnk -scoring_scheme classic -rpt_label my_analysis -include_only_symbols true -make_sets true -plot_top_x 20 -rnd_seed timestamp -set_max 500 -set_min 15 -zip_report false -out /Users/lishasha/gsea_home/output/九月18 -gui false


或者利用一下的方法进行分份
get_the_sh.py
然后在split到100个文件里面，批量添加后缀.sh然后提交2563/50=51个sh,14:57开始,
split -l 50 try.sh gsea
用find和xargs添加后缀名
find . -type f |xargs -t -i mv {} {}.sh
find . -type f |xargs -i mv {} {}.txt

.  当前目录
type  查找某一类型的文件，诸如：
b - 块设备文件。
d - 目录。
c - 字符设备文件。
p - 管道文件。
l - 符号链接文件。
f - 普通文件。
＃
将上述列表作为参数进行传递，一次传递一个。xargs 命令允许你这样做。最后一部分，xargs ls -ltr，用于接收输出并对其执行 ls -ltr 命令，如下所示：
$ ls | xargs -t -i mv {} {}.bak
-i 选项告诉 xargs 用每项的名称替换 {}。-t 选项指示 xargs 先打印命令，然后再执行。


12将gsea数据提取出来
extract_gsea_result.py

NAME 替换成^t,
	
CTNNB1_UP替换成^t,

xargs
大 多数 Linux 命令都会产生输出：文件列表、字符串列表等。但如果要使用其他某个命令并将前一个命令的输出作为参数该怎么办？例如，file 命令显示文件类型（可执行文件、ascii 文本等）；你能处理输出，使其仅显示文件名，目前你希望将这些名称传递给 ls -l 命令以查看时间戳记。xargs 命令就是用来完成此项工作的。他允许你对输出执行其他某些命令。记住下面这个来自于第 1 部分中的语法：
例1：

file -Lz * | grep ASCII | cut -d":" -f1 | xargs ls -ltr
让我们来剖析这个命令字符串。第一个，file -Lz *，用于查找是符号链接或经过压缩的文件。他将输出传递给下一个命令 grep ASCII，该命令在其中搜索 "ASCII" 字符串并产生如下所示的输出：

alert_DBA102.log:        ASCII English text
alert_DBA102.log.Z:      ASCII text (compress’d data 16 bits)
dba102_asmb_12307.trc.Z: ASCII English text (compress’d data 16 bits)
dba102_asmb_20653.trc.Z: ASCII English text (compress’d data 16 bits)
由于我们只对文件名感兴趣，因此我们应用下一个命令 cut -d":" -f1，仅显示第一个字段：

alert_DBA102.log
alert_DBA102.log.Z
dba102_asmb_12307.trc.Z
dba102_asmb_20653.trc.Z
目前，我们希望使用 ls -l 命令，将上述列表作为参数进行传递，一次传递一个。xargs 命令允许你这样做。最后一部分，xargs ls -ltr，用于接收输出并对其执行 ls -ltr 命令，如下所示：
ls -ltr alert_DBA102.log
ls -ltr alert_DBA102.log.Z
ls -ltr dba102_asmb_12307.trc.Z
ls -ltr dba102_asmb_20653.trc.Z
因此，xargs 本身虽然没有多大用处，但在和其他命令相结合时，他的功能非常强大。
下面是另一个示例，我们希望计算这些文件中的行数：
例 2：

$ file * | grep ASCII | cut -d":" -f1 | xargs wc -l
47853 alert_DBA102.log
     19 dba102_cjq0_14493.trc
29053 dba102_mmnl_14497.trc
    154 dba102_reco_14491.trc
     43 dba102_rvwr_14518.trc
77122 total


panel.cor.scale <- function(x, y, digits=2, prefix="R2=", cex.cor) #在之前面加上描述的R2=
{
usr <- par("usr"); on.exit(par(usr))
par(usr = c(0, 1, 0, 1))
r = (cor(x, y,use="pairwise"))
txt <- format(c(r, 0.123456789), digits=digits)[1]
txt <- paste(prefix, txt, sep="")
if(missing(cex.cor)) cex <- 0.8/strwidth(txt)
text(0.5, 0.5, txt, cex = cex * abs(r))

 else {pairs(x,upper.panel=panel.cor,lower.panel=panel.smooth,gap=0,cex.labels=4) #改变中间字体的大小
pdf("well4_cex5_R.pdf",family="sans",height=10,width=10) ＃改变字体，此处为arial
family可以改变字典，在PDF后面，PDF较容易，参见book，p48，另外，先利用下面命令查一下有哪些官方的字体明
 names(pdfFonts())
 
[1] "serif"                "sans"                 "mono"
 [4] "AvantGarde"           "Bookman"              "Courier"
 [7] "Helvetica"            "Helvetica-Narrow"     "NewCenturySchoolbook"
[10] "Palatino"             "Times"                "URWGothic"
[13] "URWBookman"           "NimbusMon"            "NimbusSan"
[16] "URWHelvetica"         "NimbusSanCond"        "CenturySch"
[19] "URWPalladio"          "NimbusRom"            "URWTimes"
[22] "ArialMT"              "Japan1"               "Japan1HeiMin"
[25] "Japan1GothicBBB"      "Japan1Ryumin"         "Korea1"
[28] "Korea1deb"            "CNS1"                 "GB1"

/Work1/home/wangdong/lss/data/sw_20150110/SW_16_Samples_R
#改变字体成arial ,times
 pdf("font_times.pdf",family="serif",height=10,width=10)
Fonts
You can easily set font size and style, but font family is a bit more complicated.
option	description
font	Integer specifying font to use for text. 
1=plain, 2=bold, 3=italic, 4=bold italic, 5=symbol
font.axis	font for axis annotation
font.lab	font for x and y labels
font.main	font for titles
font.sub	font for subtitles
ps	font point size (roughly 1/72 inch)
text size=ps*cex
family	font family for drawing text. Standard values are "serif", "sans", "mono", "symbol". Mapping is device dependent.
In windows, mono is mapped to "TT Courier New", serif is mapped to"TT Times New Roman", sans is mapped to "TT Arial", mono is mapped to "TT Courier New", and symbol is mapped to "TT Symbol" (TT=True Type). You can add your own mappings
下面的想法挺好，但是不能用
C集群上有的字体
> names(pdfFonts())
 [1] "serif"                "sans"                 "mono"
 [4] "AvantGarde"           "Bookman"              "Courier"
 [7] "Helvetica"            "Helvetica-Narrow"     "NewCenturySchoolbook"
[10] "Palatino"             "Times"                "URWGothic"
[13] "URWBookman"           "NimbusMon"            "NimbusSan"
[16] "URWHelvetica"         "NimbusSanCond"        "CenturySch"
[19] "URWPalladio"          "NimbusRom"            "URWTimes"
[22] "ArialMT"              "Japan1"               "Japan1HeiMin"
[25] "Japan1GothicBBB"      "Japan1Ryumin"         "Korea1"
[28] "Korea1deb"            "CNS1"                 "GB1"

# Type family examples - creating new mappings 
plot(1:10,1:10,type="n")
windowsFonts(
 	A=windowsFont("Arial Black"),
  B=windowsFont("Bookman Old Style"),
  C=windowsFont("Comic Sans MS"),
  D=windowsFont("Symbol")
)
text(3,3,"Hello World Default")
text(4,4,family="A","Hello World from Arial Black")
text(5,5,family="B","Hello World from Bookman Old Style")
text(6,6,family="C","Hello World from Comic Sans MS")
text(7,7,family="D", "Hello World from Symbol")
聚类作图方法或软件有什么？类似于散点图成簇的图
楚亚男-宏基因  16:31:13
你指PCA？
袁丽娜-Pangenome  16:33:15
没有那么多成分，就一种数值
袁丽娜-Pangenome  16:37:35
散点 聚类，有吗？
袁丽娜-Pangenome  16:37:53
貌似是要做数值转化？
楚亚男-宏基因  16:39:13
不知了……
康禹-Pangenome  16:41:05
用similarity那个图的聚类方法行吗？
吴浩-细菌复制  16:44:32
PCoA~ R


fastqc -o /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/fastqc /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/barcode6/Project_C6F03ANXX/Sample_zlj_sh2/*.fastq



开始使用Screen
简单来说，Screen是一个可以在多个进程之间多路复用一个物理终端的窗口管理器。Screen中有会话的概念，用户可以在一个screen会话中创建多个screen窗口，在每一个screen窗口中就像操作一个真实的telnet/SSH连接窗口那样。在screen中创建一个新的窗口有这样几种方式：
1．直接在命令行键入screen命令
[root@tivf06 ~]# screen
Screen将创建一个执行shell的全屏窗口。你可以执行任意shell程序，就像在ssh窗口中那样
screen还有更高级的功能。你可以不中断screen窗口中程序的运行而暂时断开（detach）screen会话，并在随后时间重新连接（attach）该会话，重新控制各窗口中运行的程序。例如，我们打开一个screen窗口编辑/tmp/abc文件：
[root@tivf06 ~]# screen vi /tmp/abc
之后我们想暂时退出做点别的事情，比如出去散散步，那么在screen窗口键入C-a d，Screen会给出detached提示：
管理你的远程会话
先来看看如何使用screen解决SIGHUP问题，比如现在我们要ftp传输一个大文件。如果按老的办法，SSH登录到系统，直接ftp命令开始传输，之后。。如果网络速度还可以，恭喜你，不用等太长时间了；如果网络不好，老老实实等着吧，只能传输完毕再断开SSH连接了。让我们使用screen来试试。
SSH登录到系统，在命令行键入screen。
[root@tivf18 root]# screen
在screen shell窗口中输入ftp命令，登录，开始传输。不愿意等了？OK，在窗口中键入C-a d：
让screen来帮你“保存”吧，你只需要打开一个ssh窗口，创建需要的screen窗口，退出的时候C-a d“保存”你的工作，下次登录后直接screen -r <screen_pid>就可以了。
最好能给每个窗口起一个名字，这样好记些。使用C-a A给窗口起名字。使用C-a w可以看到这些窗口名字，可能名字出现的位置不同
进入srceen
运行后暂时退出 control a d 
运行完了后杀掉：
screen -ls  查看编号
screen -r 16582
看看出现什么了，太棒了，一切都在。继续干吧

C-a k	杀掉当前窗口


tophat -p 16 -G /data/reference/human/gencode_v20/gencode_v20_annotation.gtf -o /home/lss/project/zlj/mapping/zlj_NC2 -r 50 --library-type=fr-unstranded /data/reference/human/hg38 /home/lss/project/zlj/fastq/zlj_NC_R1_trime.fastq /home/lss/project/zlj/fastq/zlj_NC_R2_trime.fastq
tophat -p 16 -G /data/reference/human/gencode_v20/gencode_v20_annotation.gtf -o /home/lss/project/zlj/mapping/zlj_sh2 -r 50 --library-type=fr-unstranded /data/reference/human/hg38 /home/lss/project/zlj/fastq/zlj_sh2_R1_trime.fastq /home/lss/project/zlj/fastq/zlj_sh2_R2_trime.fastq
tophat -p 16 -G /data/reference/human/gencode_v20/gencode_v20_annotation.gtf -o /home/lss/project/zlj/mapping/zlj_sh3 -r 50 --library-type=fr-unstranded /data/reference/human/hg38 /home/lss/project/zlj/fastq/zlj_sh3_R1_trime.fastq /home/lss/project/zlj/fastq/zlj_sh3_R2_trime.fastq
tophat -p 16 -G /data/reference/human/gencode_v20/gencode_v20_annotation.gtf -o /home/lss/project/zlj/mapping/wqy_2M -r 50 --library-type=fr-unstranded /data/reference/human/hg38 /home/lss/project/zlj/fastq/wqy_2M_R1_trime.fastq /home/lss/project/zlj/fastq/wqy_2M_R2_trime.fastq



/Share/home/wangdong/local/bin/tophat -p 8 -G /Share/home/wangdong/data/human_annotation/gencode19.lite.gtf -o /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_sh2 -r 50 --library-type=fr-unstranded /Share/home/lulab1/users/liyang/project/Genome/human_hg19/bowtie2_index/human_genome_hg19 /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/barcode6/Project_C6F03ANXX/Sample_zlj_sh2/zlj_sh2_R1_trime.fastq /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/barcode6/Project_C6F03ANXX/Sample_zlj_sh2/zlj_sh2_R2_trime.fastq
/Share/home/wangdong/local/bin/tophat -p 8 -G /Share/home/wangdong/data/human_annotation/gencode19.lite.gtf -o /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_sh3 -r 50 --library-type=fr-unstranded /Share/home/lulab1/users/liyang/project/Genome/human_hg19/bowtie2_index/human_genome_hg19 /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/barcode6/Project_C6F03ANXX/Sample_zlj_sh3/zlj_sh3_R1_trime.fastq /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/barcode6/Project_C6F03ANXX/Sample_zlj_sh3/zlj_sh3_R2_trime.fastq
/Share/home/wangdong/local/bin/tophat -p 8 -G /Share/home/wangdong/data/human_annotation/gencode19.lite.gtf -o /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_NC -r 50 --library-type=fr-unstranded /Share/home/lulab1/users/liyang/project/Genome/human_hg19/bowtie2_index/human_genome_hg19 /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/barcode6/Project_C6F03ANXX/Sample_zlj_NC/zlj_NC_R1_trime.fastq /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/barcode6/Project_C6F03ANXX/Sample_zlj_NC/zlj_NC_R2_trime.fastq
/Share/home/wangdong/local/bin/tophat -p 8 -G /Share/home/wangdong/data/human_annotation/gencode19.lite.gtf -o /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/wqy_2M -r 50 --library-type=fr-unstranded /Share/home/lulab1/users/liyang/project/Genome/human_hg19/bowtie2_index/human_genome_hg19 /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/barcode6/Project_C6F03ANXX/Sample_wqy_2M/wqy_2M_R1_trime.fastq /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/barcode6/Project_C6F03ANXX/Sample_wqy_2M/wqy_2M_R2_trime.fastq



fastx_trimmer -f 16 -l 110 -Q 33 -i /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/barcode6/Project_C6F03ANXX/Sample_zlj_sh2/zlj_sh2_R2.fastq -o /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/barcode6/Project_C6F03ANXX/Sample_zlj_sh2/zlj_sh2_R2_trime.fastq
fastx_trimmer -f 16 -l 110 -Q 33 -i /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/barcode6/Project_C6F03ANXX/Sample_zlj_sh3/zlj_sh3_R1.fastq -o /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/barcode6/Project_C6F03ANXX/Sample_zlj_sh3/zlj_sh3_R1_trime.fastq
fastx_trimmer -f 16 -l 110 -Q 33 -i /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/barcode6/Project_C6F03ANXX/Sample_zlj_sh3/zlj_sh3_R2.fastq -o /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/barcode6/Project_C6F03ANXX/Sample_zlj_sh3/zlj_sh3_R2_trime.fastq
fastx_trimmer -f 16 -l 110 -Q 33 -i /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/barcode6/Project_C6F03ANXX/Sample_zlj_NC/zlj_NC_R1.fastq -o /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/barcode6/Project_C6F03ANXX/Sample_zlj_NC/zlj_NC_R1_trime.fastq
fastx_trimmer -f 16 -l 110 -Q 33 -i /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/barcode6/Project_C6F03ANXX/Sample_zlj_NC/zlj_NC_R2.fastq -o /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/barcode6/Project_C6F03ANXX/Sample_zlj_NC/zlj_NC_R2_trime.fastq
fastx_trimmer -f 16 -l 110 -Q 33 -i /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/barcode6/Project_C6F03ANXX/Sample_wqy_2M/wqy_2M_R1.fastq -o /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/barcode6/Project_C6F03ANXX/Sample_wqy_2M/wqy_2M_R1_trime.fastq
fastx_trimmer -f 16 -l 110 -Q 33 -i /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/barcode6/Project_C6F03ANXX/Sample_wqy_2M/wqy_2M_R2.fastq -o /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/barcode6/Project_C6F03ANXX/Sample_wqy_2M/wqy_2M_R2_trime.fastq

fastx_trimmer -f 1 -l 51 -Q 33 -i rasl_lab2_AGTGCAT_L005_R1_001.fastq -o rasl_lab2_AGTGCAT_L005_R1_51nb.fastq



/Share/home/wangdong/local/bin/tophat -p 8 -G /Share/home/wangdong/data/human_annotation/gencode19.lite.gtf -o /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_sh2 --library-type=fr-unstranded /Share/home/lulab1/users/liyang/project/Genome/human_hg19/bowtie2_index/human_genome_hg19 /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/barcode6/Project_C6F03ANXX/Sample_zlj_sh2/zlj_sh2_R1.fastq /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/barcode6/Project_C6F03ANXX/Sample_zlj_sh2/zlj_sh2_R2.fastq
/Share/home/wangdong/local/bin/tophat -p 8 -G /Share/home/wangdong/data/human_annotation/gencode19.lite.gtf -o /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_sh3 --library-type=fr-unstranded /Share/home/lulab1/users/liyang/project/Genome/human_hg19/bowtie2_index/human_genome_hg19 /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/barcode6/Project_C6F03ANXX/Sample_zlj_sh3/zlj_sh3_R1.fastq /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/barcode6/Project_C6F03ANXX/Sample_zlj_sh3/zlj_sh3_R2.fastq
/Share/home/wangdong/local/bin/tophat -p 8 -G /Share/home/wangdong/data/human_annotation/gencode19.lite.gtf -o /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/zlj_NC --library-type=fr-unstranded /Share/home/lulab1/users/liyang/project/Genome/human_hg19/bowtie2_index/human_genome_hg19 /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/barcode6/Project_C6F03ANXX/Sample_zlj_NC/zlj_NC_R1.fastq /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/barcode6/Project_C6F03ANXX/Sample_zlj_NC/zlj_NC_R2.fastq
/Share/home/wangdong/local/bin/tophat -p 8 -G /Share/home/wangdong/data/human_annotation/gencode19.lite.gtf -o /Share/home/wangdong/lss/project/zlj_pcat1_RNASEQ/mapping/wqy_2M --library-type=fr-unstranded /Share/home/lulab1/users/liyang/project/Genome/human_hg19/bowtie2_index/human_genome_hg19 /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/barcode6/Project_C6F03ANXX/Sample_wqy_2M/wqy_2M_R1.fastq /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/barcode6/Project_C6F03ANXX/Sample_wqy_2M/wqy_2M_R2.fastq




fastqc 
fastqc -o /Share/home/wangdong/lss/project/tmp/test2/ /Share/home/wangdong/lss/project/tmp/Human_exon_1.0ST.fastq


.gzip 解压
unzip .zip -d 目标文件夹

设置bashrc后发现许多命令不能用了，如ls等等，原因如下 
解决办法：

先用：echo $PATH  
查看path是否含有：/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin

如果没有

先用临时环境变量（重启后消失）
#export PATH=$PATH:/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin

然后就可以用那些命令了，进去修改永久环境变量：

1。修改profile文件：（所有用户）

#vi /etc/profile
加入：export PATH=$PATH:/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin
保存退出。

2。修改.bashrc文件：（单独用户）

#vi /~/.bashrc （‘~’代表：$HOME,  .bashrc是每个用户家目录下都有的，ls -all）
加入：export PATH=$PATH:/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin

保存退出。

重启系统，搞定。

也可以不用重启，使用命令：#source .bashrc 
即可使刚刚修改的环境变量生效

自己mac上的解决办法就是加到
~/.bash_profile中

/Share/home/wangdong/local/app/bowtie-1.1.1/bowtie-build -f /Share/home/wangdong/lss/project/20150604_probe_final_test_1411/new_ref_1411_rename.fa /Share/home/wangdong/lss/project/20150604_probe_final_test_1411/new_ref_1411_rename

fastx_trimmer -f 1 -l 51 -Q 33 -i rasl_lab2_AGTGCAT_L005_R1_001.fastq -o rasl_lab2_AGTGCAT_L005_R1_51nb.fastq
fastx_trimmer -f 1 -l 51 -Q 33 -i rasl_lab1_GAATCAT_L005_R1_001.fastq -o rasl_lab1_GAATCAT_L005_R1_51nt.fastq
fastx_trimmer -f 1 -l 51 -Q 33 -i rasl_lab3_GTGGCAT_L005_R1_001.fastq -o rasl_lab3_GTGGCAT_L005_R1_51nt.fastq
bowtie -n 3 /Share/home/wangdong/lss/project/20150604_probe_final_test_1411/new_ref_1411_rename /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/barcode7/Project_C6F03ANXX/Sample_rasl_lab1/rasl_lab1_GAATCAT_L005_R1_51nt.fastq -S /Share/home/wangdong/lss/project/20150604_probe_final_test_1411/mapping_result/rasl_lab1.sam
bowtie -n 3 /Share/home/wangdong/lss/project/20150604_probe_final_test_1411/new_ref_1411_rename /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/barcode7/Project_C6F03ANXX/Sample_rasl_lab2/rasl_lab2_AGTGCAT_L005_R1_51nb.fastq -S /Share/home/wangdong/lss/project/20150604_probe_final_test_1411/mapping_result/rasl_lab2.sam
bowtie -n 3 /Share/home/wangdong/lss/project/20150604_probe_final_test_1411/new_ref_1411_rename /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/barcode7/Project_C6F03ANXX/Sample_rasl_lab3/rasl_lab3_GTGGCAT_L005_R1_51nt.fastq -S /Share/home/wangdong/lss/project/20150604_probe_final_test_1411/mapping_result/rasl_lab3_R1.sam
python /Share/home/wangdong/lss/project/20150430_sw_probes_screen/mapping/split_colume3.py /Share/home/wangdong/lss/project/20150604_probe_final_test_1411/mapping_result/rasl_lab1.sam
python /Share/home/wangdong/lss/project/20150430_sw_probes_screen/mapping/split_colume3.py /Share/home/wangdong/lss/project/20150604_probe_final_test_1411/mapping_result/rasl_lab2.sam
python /Share/home/wangdong/lss/project/20150430_sw_probes_screen/mapping/split_colume3.py /Share/home/wangdong/lss/project/20150604_probe_final_test_1411/mapping_result/rasl_lab3_R1.sam



configureBclToFastq.pl --input-dir /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/Data/Intensities/BaseCalls --output /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/barcode7_2 --sample-sheet /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/SampleSheet_barcode7.csv --no-eamss --use-bases-mask Y126,I7n,Y126
cd /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/barcode7_2
nohup make -j 8
6barcode
configureBclToFastq.pl --input-dir /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/Data/Intensities/BaseCalls --output /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/barcode6 --sample-sheet /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/SampleSheet_barcode6.csv --no-eamss --use-bases-mask Y126,I6n,Y126
cd /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/barcode6
nohup make -j 8

还可以通过下面方法输入文件夹，但是需要进行运行是加引号，运行募夹里的文17
也可以直接input=glob.glob(sys.argv[1])
下面是.py
  1 import sys,os,glob
  2 inputdir = sys.argv[ 1 ]
  3 input = glob.glob( inputdir )#get the list
  4 for f in input:
  5     os.system( 'fastqc -o ./fastqc2/ %s'%f) #%s need format here %f:format like f after %
  6
运行过程如下
 
[wangdong@cluster ~/lss/project/tmp]$python fastqc2.py './test/*'
Started analysis of WHY_HCT116RNA_2.fastq
Approx 5% complete for WHY_HCT116RNA_2.fastq
Approx 10% complete for WHY_HCT116RNA_2.fastq
Approx 15% complete for WHY_HCT116RNA_2.fastq
Approx 20% complete for WHY_HCT116RNA_2.fastq
Approx 25% complete for WHY_HCT116RNA_2.fastq
Approx 30% complete for WHY_HCT116RNA_2.fastq
Approx 35% complete for WHY_HCT116RNA_2.fastq
Approx 40% complete for WHY_HCT116RNA_2.fastq
Approx 45% complete for WHY_HCT116RNA_2.fastq
Approx 50% complete for WHY_HCT116RNA_2.fastq
Approx 55% complete for WHY_HCT116RNA_2.fastq
Approx 60% complete for WHY_HCT116RNA_2.fastq
Approx 65% complete for WHY_HCT116RNA_2.fastq
Approx 70% complete for WHY_HCT116RNA_2.fastq
Approx 75% complete for WHY_HCT116RNA_2.fastq
Approx 80% complete for WHY_HCT116RNA_2.fastq
Approx 85% complete for WHY_HCT116RNA_2.fastq
Approx 90% complete for WHY_HCT116RNA_2.fastq
Approx 95% complete for WHY_HCT116RNA_2.fastq
Analysis complete for WHY_HCT116RNA_2.fastq
Started analysis of Human_exon_1.0ST.fastq
Approx 5% complete for Human_exon_1.0ST.fastq
Approx 10% complete for Human_exon_1.0ST.fastq
Approx 15% complete for Human_exon_1.0ST.fastq
Approx 20% complete for Human_exon_1.0ST.fastq
Approx 25% complete for Human_exon_1.0ST.fastq
Approx 30% complete for Human_exon_1.0ST.fastq
Approx 35% complete for Human_exon_1.0ST.fastq
Approx 40% complete for Human_exon_1.0ST.fastq
Approx 45% complete for Human_exon_1.0ST.fastq
Approx 50% complete for Human_exon_1.0ST.fastq
Approx 55% complete for Human_exon_1.0ST.fastq
Approx 60% complete for Human_exon_1.0ST.fastq
Approx 65% complete for Human_exon_1.0ST.fastq
Approx 70% complete for Human_exon_1.0ST.fastq
Approx 75% complete for Human_exon_1.0ST.fastq
Approx 80% complete for Human_exon_1.0ST.fastq
Approx 85% complete for Human_exon_1.0ST.fastq
Approx 90% complete for Human_exon_1.0ST.fastq
Approx 95% complete for Human_exon_1.0ST.fastq
Analysis complete for Human_exon_1.0ST.fastq

关于format进行字符串格式化 python
自python2.6开始，新增了一种格式化字符串的函数str.format()，可谓威力十足。那么，他跟之前的%型格式化字符串相比，有什么优越的存在呢？让我们来揭开它羞答答的面纱。

语法
它通过{}和:来代替%。

“映射”示例
通过位置

In [1]: '{0},{1}'.format('kzc',18)  
Out[1]: 'kzc,18'  
In [2]: '{},{}'.format('kzc',18)  
Out[2]: 'kzc,18'  
In [3]: '{1},{0},{1}'.format('kzc',18)  
Out[3]: '18,kzc,18'
字符串的format函数可以接受不限个参数，位置可以不按顺序，可以不用或者用多次，不过2.6不能为空{}，2.7才可以。
通过关键字参数

In [5]: '{name},{age}'.format(age=18,name='kzc')  
Out[5]: 'kzc,18'
通过对象属性

class Person:  
    def __init__(self,name,age):  
        self.name,self.age = name,age  
        def __str__(self):  
            return 'This guy is {self.name},is {self.age} old'.format(self=self)  
In [2]: str(Person('kzc',18))  
Out[2]: 'This guy is kzc,is 18 old'
通过下标

In [7]: p=['kzc',18]
In [8]: '{0[0]},{0[1]}'.format(p)
Out[8]: 'kzc,18'
有了这些便捷的“映射”方式，我们就有了偷懒利器。基本的python知识告诉我们，list和tuple可以通过“打散”成普通参数给函数，而dict可以打散成关键字参数给函数（通过和*）。所以可以轻松的传个list/tuple/dict给format函数。非常灵活。
:号后面带填充的字符，只能是一个字符，不指定的话默认是用空格填充
用，号还能用来做金额的千位分隔符。

In [47]: '{:,}'.format(1234567890)
Out[47]: '1,234,567,890'

文件处理
 1 import sys,os,glob
  2 input = glob.glob("/Share/home/wangdong/lss/project/tmp/test/*.fastq")
  3 for f in input:
  4     print(f)
  5     os.system( 'fastqc -o ./fastqc/ %s'%f) ＃提前建立fastqc这个文件夹
运行只需要python fastqc.py
就会输出
/Share/home/wangdong/lss/project/tmp/test/WHY_HCT116RNA_2.fastq
Started analysis of WHY_HCT116RNA_2.fastq
Approx 5% complete for WHY_HCT116RNA_2.fastq
Approx 10% complete for WHY_HCT116RNA_2.fastq
Approx 15% complete for WHY_HCT116RNA_2.fastq
Approx 20% complete for WHY_HCT116RNA_2.fastq
Approx 25% complete for WHY_HCT116RNA_2.fastq
Approx 30% complete for WHY_HCT116RNA_2.fastq
Approx 35% complete for WHY_HCT116RNA_2.fastq
Approx 40% complete for WHY_HCT116RNA_2.fastq
Approx 45% complete for WHY_HCT116RNA_2.fastq
Approx 50% complete for WHY_HCT116RNA_2.fastq
Approx 55% complete for WHY_HCT116RNA_2.fastq
Approx 60% complete for WHY_HCT116RNA_2.fastq
Approx 65% complete for WHY_HCT116RNA_2.fastq
Approx 70% complete for WHY_HCT116RNA_2.fastq
Approx 75% complete for WHY_HCT116RNA_2.fastq
Approx 80% complete for WHY_HCT116RNA_2.fastq
Approx 85% complete for WHY_HCT116RNA_2.fastq
Approx 90% complete for WHY_HCT116RNA_2.fastq
Approx 95% complete for WHY_HCT116RNA_2.fastq
Analysis complete for WHY_HCT116RNA_2.fastq
/Share/home/wangdong/lss/project/tmp/test/Human_exon_1.0ST.fastq
Started analysis of Human_exon_1.0ST.fastq
Approx 5% complete for Human_exon_1.0ST.fastq
Approx 10% complete for Human_exon_1.0ST.fastq
Approx 15% complete for Human_exon_1.0ST.fastq
Approx 20% complete for Human_exon_1.0ST.fastq
Approx 25% complete for Human_exon_1.0ST.fastq
Approx 30% complete for Human_exon_1.0ST.fastq
Approx 35% complete for Human_exon_1.0ST.fastq
Approx 40% complete for Human_exon_1.0ST.fastq
Approx 45% complete for Human_exon_1.0ST.fastq
Approx 50% complete for Human_exon_1.0ST.fastq
Approx 55% complete for Human_exon_1.0ST.fastq
Approx 60% complete for Human_exon_1.0ST.fastq
Approx 65% complete for Human_exon_1.0ST.fastq
Approx 70% complete for Human_exon_1.0ST.fastq
Approx 75% complete for Human_exon_1.0ST.fastq
Approx 80% complete for Human_exon_1.0ST.fastq
Approx 85% complete for Human_exon_1.0ST.fastq
Approx 90% complete for Human_exon_1.0ST.fastq
Approx 95% complete for Human_exon_1.0ST.fastq
Analysis complete for Human_exon_1.0ST.fastq

python glob model

说明：

1、glob是python自己带的一个文件操作相关模块，用它可以查找符合自己目的的文件，就类似于Windows下的文件搜索，支持通配符操作，*,?,[]这三个通配符，*代表0个或多个字符，?代表一个字符，[]匹配指定范围内的字符，如[0-9]匹配数字。

它的主要方法就是glob,该方法返回所有匹配的文件路径列表，该方法需要一个参数用来指定匹配的路径字符串（本字符串可以为绝对路径也可以为相对路径），其返回的文件名只包括当前目录里的文件名，不包括子文件夹里的文件。

比如：

glob.glob(r'c:\*.txt')

我这里就是获得C盘下的所有txt文件

glob.glob(r'E:\pic\*\*.jpg')

获得指定目录下的所有jpg文件

使用相对路径：

glob.glob(r'../*.py')

2、iglob方法：

获取一个可编历对象，使用它可以逐个获取匹配的文件路径名。与glob.glob()的区别是：glob.glob同时获取所有的匹配路径，而 glob.iglob一次只获取一个匹配路径。这有点类似于.NET中操作数据库用到的DataSet与DataReader。下面是一个简单的例子：
 
#父目录中的.py文件
f = glob.iglob(r'../*.py')

print f #<generator object iglob at 0x00B9FF80>

for py in f:
    print py

 

官方说明：
glob.glob(pathname)
Return a possibly-empty list of path names that match pathname, which must be a string containing a path specification. pathname can be either absolute (like /usr/src/Python-1.5/Makefile) or relative (like http://www.cnblogs.com/Tools/*/*.gif), and can contain shell-style wildcards. Broken symlinks are included in the results (as in the shell).
glob.iglob(pathname)
Return an iterator which yields the same values as glob() without actually storing them all simultaneously.

New in version 2.5.

For example, consider a directory containing only the following files: 1.gif, 2.txt, andcard.gif. glob() will produce the following results. Notice how any leading components of the path are preserved.

>>> import glob
>>> glob.glob('./[0-9].*')
['./1.gif', './2.txt']
>>> glob.glob('*.gif')
['1.gif', 'card.gif']
>>> glob.glob('?.gif')
['1.gif']


在许多编程语言中都包含有格式化字符串的功能，比如C和Fortran语言中的格式化输入输出。Python中内置有对字符串进行格式化的操作%。

 

模板

格式化字符串时，Python使用一个字符串作为模板。模板中有格式符，这些格式符为真实值预留位置，并说明真实数值应该呈现的格式。Python用一个tuple将多个值传递给模板，每个值对应一个格式符。

比如下面的例子：

print("I'm %s. I'm %d year old" % ('Vamei', 99))
上面的例子中，

"I'm %s. I'm %d year old" 为我们的模板。%s为第一个格式符，表示一个字符串。%d为第二个格式符，表示一个整数。('Vamei', 99)的两个元素'Vamei'和99为替换%s和%d的真实值。 
在模板和tuple之间，有一个%号分簦它代表了格式化操作17

整个"I'm %s. I'm %d year old" % ('Vamei', 99) 实际上构成一个字符串表达式。我们可以像一个正常的字符串那样，将它赋值给某个变量。比如:

a = "I'm %s. I'm %d year old" % ('Vamei', 99)
print(a)
 

我们还可以用词典来传递真实值。如下：

print("I'm %(name)s. I'm %(age)d year old" % {'name':'Vamei', 'age':99})
可以看到，我们对两个格式符进行了命名。命名使用()括起来。每个命名对应词典的一个key。

 

格式符

格式符为真实值预留位置，并控制显示的格式。格式符可以包含有一个类型码，用以控制显示的类型，如下:

%s    字符串 (采用str()的显示)

%r    字符串 (采用repr()的显示)

%c    单个字符

%b    二进制整数

%d    十进制整数

%i    十进制整数

%o    八进制整数

%x    十六进制整数

%e    指数 (基底写为e)

%E    指数 (基底写为E)

%f    浮点数

%F    浮点数，与上相同

%g    指数(e)或浮点数 (根据显示长度)

%G    指数(E)或浮点数 (根据显示长度)

 

%%    字符"%"

 返回所有匹配的文件路径列表。它只有一个参数pathname，定义了文件路径匹配规则，这里可以是绝对路径，也可以是相对路径。下面是使用glob.glob的例子：

import glob  
  
#获取指定目录下的所有图片  
print glob.glob(r"E:\Picture\*\*.jpg")  
  
#获取上级目录的所有.py文件  
print glob.glob(r'../*.py') #相对路径  

 print(glob.glob(r'/Users/lishasha/Desktop/home/script/*.r'))
['/Users/lishasha/Desktop/home/script/a_cor_cluster.r', '/Users/lishasha/Desktop/home/script/correlation.r', '/Users/lishasha/Desktop/home/script/correlation_again 2.r', '/Users/lishasha/Desktop/home/script/correlation_again.r', '/Users/lishasha/Desktop/home/script/correlation_dot_size.r', '/Users/lishasha/Desktop/home/script/correlation_text_modified 2.r', '/Users/lishasha/Desktop/home/script/correlation_text_modified.r', '/Users/lishasha/Desktop/home/script/corrrelation_modified_by_me.r', '/Users/lishasha/Desktop/home/script/heatmapV5.1.r', '/Users/lishasha/Desktop/home/script/net_work.r', '/Users/lishasha/Desktop/home/script/pheatmapV1 2.r', '/Users/lishasha/Desktop/home/script/pheatmapV1.r', '/Users/lishasha/Desktop/home/script/volcanoplot_corrected.r']


>> import glob
>>> f=glob.glob(r'*.py')
>>> print(f)
['probe_pick_for_well.py', 'test.py', 'test2.py']



亲，如何表示一个数可以被24整除？
逗你玩  20:40:26
if n/／24=？ is true
Nil  21:20:28
if n//24 == n/24:

1   2   3   4   5
6   7   8   9   10
11  12  13  14  15
program:

with open('in.txt') as f:
  lis = [x.split() for x in f]

for x in zip(*lis):
  for y in x:
    print(y+'\t', end='')
  print('\n')
output:

1   6   11  

2   7   12  

3   8   13  

4   9   14  

5   10  15

range的用法，python
>>> range(1,5) #代表从1到5(不包含5)
[1, 2, 3, 4]
>>> range(1,5,2) #代表从1到5，间隔2(不包含5)
[1, 3]
>>> range(5) #代表从0到5(不包含5)
[0, 1, 2, 3, 4]
再看看list的操作:

array = [1, 2, 5, 3, 6, 8, 4]
#其实这里的顺序标识是
[1, 2, 5, 3, 6, 8, 4]
(0，1，2，3，4，5，6)
(-7,-6,-5,-4,-3,-2,-1)
 
>>> array[0:] #列出0以后的
[1, 2, 5, 3, 6, 8, 4]
>>> array[1:] #列出1以后的
[2, 5, 3, 6, 8, 4]
>>> array[:-1] #列出-1之前的
[1, 2, 5, 3, 6, 8]
>>> array[3:-3] #列出3到-3之间的
[3]
 

那么两个[::]会是什么那？

>>> array[::2]
[1, 5, 6, 4]
>>> array[2::]
[5, 3, 6, 8, 4]
>>> array[::3]
[1, 3, 4]
>>> array[::4]
[1, 6] 
如果想让他们颠倒形成reverse函数的效果
>>> array[::-1]
[4, 8, 6, 3, 5, 2, 1]
>>> array[::-2]
[4, 6, 5, 1]
感觉自己懂了吧，那么来个冒泡吧：

array = [1, 2, 5, 3, 6, 8, 4]
for i in range(len(array) - 1, 0, -1):
    print i
    for j in range(0, i):
        print j
        if array[j] > array[j + 1]:
            array[j], array[j + 1] = array[j + 1], array[j]
print array
一行一行的来看：

line 1：array = [1, 2, 5, 3, 6, 8, 4]一个乱序的list没什么好解释的

line 2：for i in range(len(array) - 1, 0, -1):这就是上边给的例子的第二条，我们替换下就成为range(6,1,-1)，意思是从6到1间隔-1,也就是倒叙的range(2,7,1),随后把这些值循环赋给i，那么i的值将会是[6, 5, 4, 3, 2]

line 3：for j in range(0, i):这是一个循环赋值给j，j的值将会是[0, 1, 2, 3, 4, 5][0, 1, 2, 3, 4][0, 1, 2, 3][0, 1, 2][0, 1]
那么上边两个循环嵌套起来将会是

i------------6
j------------0j------------1j------------2j------------3j------------4j------------5

i------------5
j------------0j------------1j------------2j------------3j------------4
i------------4
j------------0j------------1j------------2j------------3
i------------3
j------------0j------------1j------------2
i------------2
j------------0j------------1

line 4：if array[j] > array[j + 1]:

>>> array = [1, 2, 5, 3, 6, 8, 4]
>>> array[0]
1
>>> array[1]
2
>>> array[2]
5
>>> array[3]
3
>>> array[4]
6
>>> array[5]
8
>>> array[6]
4
其实・就是使用这个把这个没有顺序的array = [1, 2, 5, 3, 6, 8, 4]排序

line 5：array[j], array[j + 1] = array[j + 1], array[j] 替换赋值

line 6：打印出来

其实要想省事儿，sort()函数一句就能搞定.......

File /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/Data/Intensities/BaseCalls/L005/C127.1/s_5_1101.bcl does not exist
实际上存在s_5_1101.bcl.gz，只是他自己没有解压，出问题了

针对6个barcode，
configureBclToFastq.pl --input-dir /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/Data/Intensities/BaseCalls --output /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/Data/Intensities/BaseCalls/barcode6 --sample-sheet /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/Data/Intensities/BaseCalls/SampleSheet_barcode6.csv --no-eamss --use-bases-mask Y126,I6n,Y126
对7个barcode
configureBclToFastq.pl --input-dir /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/Data/Intensities/BaseCalls --output /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/Data/Intensities/BaseCalls/barcode7 --sample-sheet /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/Data/Intensities/BaseCalls/SampleSheet_barcode7.csv --no-eamss --use-bases-mask Y126,I7n,Y126

实际上转化还需要其他许多文件
 INFO: Creating directory '/Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/Data/Intensities/BaseCalls/Unaligned'
[2015-06-09 10:38:52]   [configureBclToFastq.pl]        WARNING: Missing /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/Data/Intensities/BaseCalls/../RTAConfiguration.xml
[2015-06-09 10:38:53]   [configureBclToFastq.pl]        INFO: Basecalling software: RTA
[2015-06-09 10:38:53]   [configureBclToFastq.pl]        INFO:              version: 1.18 (build 64)
[2015-06-09 10:38:53]   [configureBclToFastq.pl]        WARNING: Missing /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/Data/Intensities/BaseCalls/../RTAConfiguration.xml
[2015-06-09 10:38:53]   [configureBclToFastq.pl]        WARNING: Missing /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/Data/Intensities/BaseCalls/../RTAConfiguration.xml
[2015-06-09 10:38:53]   [configureBclToFastq.pl]        WARNING: Couldn't find run info in /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/Data/Intensities/BaseCalls/../../../RunInfo.xml
[2015-06-09 10:38:53]   [configureBclToFastq.pl]        WARNING: Couldn't find RunInfo.xml for /Share/home/wangdong/data/rawdata/150604_D00489_0072_AC6F03ANXX/Data/Intensities/BaseCalls
[2015-06-09 10:38:53]   [configureBclToFastq.pl]        INFO: Original use-bases mask: Y126,I7n,Y126
[2015-06-09 10:38:53]   [configureBclToFastq.pl]        INFO: Guessed use-bases mask: Y126,I7n,Y126
[2015-06-09 10:38:53]   [configureBclToFastq.pl]        ERROR: barcode CCGTCC for lane 5 has length 6: expected barcode lenth (including delimiters) is 1
 

grep -A 3 ':N:0:AGGTTA' B1_2.fastq | awk '$1 != "--"' >> /Work1/home/wangdong/lss/20141021/Undetermined_indices/test/B1_2_t.fastq
grep -A 3 ':N:0:GTCTTA' B2_2.fastq | awk '$1 != "--"' >> /Work1/home/wangdong/lss/20141021/Undetermined_indices/test/B2_2_t.fastq
grep -A 3 ':N:0:ACCTTA' B3_2.fastq | awk '$1 != "--"' >> /Work1/home/wangdong/lss/20141021/Undetermined_indices/test/B3_2_t.fastq
grep -A 3 ':N:0:GGATTA' B4_2.fastq | awk '$1 != "--"' >> /Work1/home/wangdong/lss/20141021/Undetermined_indices/test/B4_2_t.fastq
grep -A 3 ':N:0:AAATTA' B5_2.fastq | awk '$1 != "--"' >> /Work1/home/wangdong/lss/20141021/Undetermined_indices/test/B5_2_t.fastq
grep -A 3 ':N:0:GCTGTA' B6_2.fastq | awk '$1 != "--"' >> /Work1/home/wangdong/lss/20141021/Undetermined_indices/test/B6_2_t.fastq
grep -A 3 ':N:0:ATGGTA' B7_2.fastq | awk '$1 != "--"' >> /Work1/home/wangdong/lss/20141021/Undetermined_indices/test/B7_2_t.fastq
grep -A 3 ':N:0:GAGGTA' B8_2.fastq | awk '$1 != "--"' >> /Work1/home/wangdong/lss/20141021/Undetermined_indices/test/B8_2_t.fastq
grep -A 3 ':N:0:GCGCTA' C1_2.fastq | awk '$1 != "--"' >> /Work1/home/wangdong/lss/20141021/Undetermined_indices/test/C1_2_t.fastq
grep -A 3 ':N:0:ATCCTA' C2_2.fastq | awk '$1 != "--"' >> /Work1/home/wangdong/lss/20141021/Undetermined_indices/test/C2_2_t.fastq
grep -A 3 ':N:0:GACCTA' C3_2.fastq | awk '$1 != "--"' >> /Work1/home/wangdong/lss/20141021/Undetermined_indices/test/C3_2_t.fastq
grep -A 3 ':N:0:AGACTA' C4_2.fastq | awk '$1 != "--"' >> /Work1/home/wangdong/lss/20141021/Undetermined_indices/test/C4_2_t.fastq
grep -A 3 ':N:0:GTTATA' C5_2.fastq | awk '$1 != "--"' >> /Work1/home/wangdong/lss/20141021/Undetermined_indices/test/C5_2_t.fastq
grep -A 3 ':N:0:ACTATA' C6_2.fastq | awk '$1 != "--"' >> /Work1/home/wangdong/lss/20141021/Undetermined_indices/test/C6_2_t.fastq
grep -A 3 ':N:0:GGGCAC' SWS1_2.fastq | awk '$1 != "--"' >> /Work1/home/wangdong/lss/20141021/Undetermined_indices/test/SWS1_2_t.fastq
grep -A 3 ':N:0:ATCTCG' SWS2_2.fastq | awk '$1 != "--"' >> /Work1/home/wangdong/lss/20141021/Undetermined_indices/test/SWS2_2_t.fastq
grep -A 3 ':N:0:GAACAC' SWT4_2.fastq | awk '$1 != "--"' >> /Work1/home/wangdong/lss/20141021/Undetermined_indices/test/SWT4_2_t.fastq

sam种reads在第10个域，提取方法如下
cut -f 10 SW_Competition1_100_2.sam > test4.fa

怎么看C集群所有人的任务我也想知道怎么看所有用户。。现在就知道－u加指定用户，－f看所有节点状态。。
tiangeng-郎继东  10:29:28
qstat -f -u '*'

microarray数据包
combat做normalize，limma做差异表达
a<- read.table("plot_com1_10.csv",head=TRUE,sep=";")
pdf("COM100_2.pdf")
plot(a$old_value,a$Competition1_10_2,lty=2,pch=20,cex=0.5,col=a$clour,xlab="before competition(log10(hits+1))",ylab="1/10 competition(log10(hits+1))")
dev.off()

复制重复时要命名为不一样的

02Unix系统里，每行结尾只有“<换行>”，即“\n”；Windows系统里面，每行结尾是“<换行><回车>”，即“\n\r”；Mac系统里，每行结尾是“<回车>”。一个直接后果是，Unix/Mac系统下的文件在Windows里打开的话，所有文字会变成一行；而Windows里的文件在Unix/Mac下打开的话，在每行的结尾可能会多出一个^M符号。

生成反向互补序列的诸多方法python
lt='CATGCATCGT'
def func1(liststr):
	t=list(liststr);d=[]
	dct={'A':'T','T':'A','C':'G','G':'C'}
	for x in range(len(t)):
	    d.append(dct[t.pop()])
	return d 
其他的都简单，1.直接字符串反向处理，再逐一翻译；2.用正负数来处理，这个对于大量的任务可以提高效率；3.两遍处理，True、False开关；4.列表内替换，然后反向；5.成对换位，不过效率低下； 6.还有就是直接的字符串替换，然后一个切片s[::-1]就OK了 ； 
lt='CATGCATCGT'
lt=lt.replace('A','{A}').replace('T','{T}').replace('C','{C}').replace('G','{G}')
result=lt.format(A='T',T='A',C='G',G='C')[::-1]

在探针用 Tm值进行过滤时，由于minor会出现长度不一的现象，所以，会报错, 可能原因还是均要转化为小写，因为苹果里面python需要区分大小写
另外探针最后一条少1bp的原因是由于没有去掉每行的换行符，所以占用了一个字符each=each.strip('\n')
Traceback (most recent call last):
  File "/Users/lishasha/Desktop/home/old_computer/E/lss/work/oligo_design/1/chr/extracted/all_minor/oligo-filtered_by_Tm.py", line 27, in <module>
    Tm_r=4*(nc+ng)+2*(na+nt)
NameError: name 'nc' is not defined


快捷键 Shift+Command+G就可以去自己想去的文件夹，mac下
基因集富集分析 (Gene Set Enrichment Analysis, GSEA) 的基本思想是使用预定义的基因集，通常来自功能注释或先前实验的结果，将基因按照在两类样本中的差异表达程度排序，然后检验预先设定的基因集合是否在这个排序表的顶端或者底端富集。基因集合富集分析检测基因集合而不是单个基因的表达变化，因此可以包含这些细微的表达变化，预期得到更为理想的结果。


configureBclToFastq.pl --input-dir /Share/home/wangdong/data/rawdata/150520_C00126_0190_AHFNKYADXX/Data/Intensities/BaseCalls --output /Share/home/wangdong/data/rawdata/150520_C00126_0190_AHFNKYADXX/Unaligned --sample-sheet /Share/home/wangdong/data/rawdata/150520_C00126_0190_AHFNKYADXX/SampleSheet_undetermined.csv --no-eamss
cd /Share/home/wangdong/data/rawdata/150520_C00126_0190_AHFNKYADXX/Unaligned
nohup make -j 8
Permutation test 置换检验是Fisher于20世纪30年代提出的一种基于大量计算（computationally intensive），利用样本数据的全（或随机）排列，进行统计推断的方法，
因其对总体分布自由，应用较为广泛，特别适糜谧芴宸植嘉粗的小样本资料，以及某些难以用常规方法分析资料的假设检验问题。在具体使用上它和Bootstrap Methods类似，
通过对样本进行顺序上的置换，重新计算统计检验量，构造经验分布，然后在此基础上求出P-value进行推断。

plot（）不同点不同颜色，更多参考寝室实验记录
a<- read.table("icg001_10um_dmso.txt",head=TRUE,sep="\t")
pdf("icg001_dmso3.pdf")
plot(a$RNA_12h_icg001_10um_clc,a$RNA_12h_dmso_clc,lty=2,pch=20,cex=0.5,col=a$clour,main="expression difference between icg001 and dmso",xlab="hits(icg001)",ylab="hots(dmso)")
dev.off()

改变dot的大小
自己是这样解决的
 panel.points<-function(x,y)
 42 {
 43   points(x,y,cex=0.5)
 44 }
 pairs(x,lower.panel=panel.points,upper.panel=panel.cor)
 
I managed to make a plot with the r function "pairs" (the value of the Kendall tau's in the lower panel and pairs-plots in upper panel). Still i have one problem. How do I change the size of the dots in my upper panel? This is my code.

panel.Kendall <- function(x,y,digits=2, prefix="", cex.cor)
{
usr <- par("usr"); on.exit(par(usr))
par(usr = c(0, 1, 0, 1))
r <- Kendall(x, y)$tau
txt <- format(c(r, 0.123456789), digits=digits)[1]
txt <- paste(prefix, txt, sep="")
if(missing(cex.cor)) cex <- 0.8/strwidth(txt)
text(0.5, 0.5, txt, cex = cex * 0.5)
}
png("Kendall1.jpg",width=600,height=600,res=100)
pairs(all[c(2,6,8,9,10,11,14,15)],lower.panel=panel.Kendall)
dev.off() 
You can define own function also for the upper.panel= (for example, panel.points) where you set points size with cex=

panel.points<-function(x,y)
{
  points(x,y,cex=3)
}

pairs(iris[,1:4],lower.panel=panel.Kendall,upper.panel=panel.points)

> pairs(iris[1:4], main = "Anderson's Iris Data -- 3 species", pch = 21, bg = c("red", "green3", "blue")[unclass(iris$Species)], lower.panel=NULL, labels=c("SL","SW","PL","PW"), font.labels=2, cex.labels=4.5) 
#可改变中间txt的笮17

pairs {graphics}	R Documentation
Scatterplot Matrices

Description

A matrix of scatterplots is produced.

Usage

pairs(x, ...)

## S3 method for class 'formula'
pairs(formula, data = NULL, ..., subset,
      na.action = stats::na.pass)

## Default S3 method:
pairs(x, labels, panel = points, ...,
      horInd = 1:nc, verInd = 1:nc,
      lower.panel = panel, upper.panel = panel,
      diag.panel = NULL, text.panel = textPanel,
      label.pos = 0.5 + has.diag/3, line.main = 3,
      cex.labels = NULL, font.labels = 1,
      row1attop = TRUE, gap = 1, log = "")
Arguments

x	
the coordinates of points given as numeric columns of a matrix or data frame. Logical and factor columns are converted to numeric in the same way that data.matrix does.

formula	
a formula, such as ~ x + y + z. Each term will give a separate variable in the pairs plot, so terms should be numeric vectors. (A response will be interpreted as another variable, but not treated specially, so it is confusing to use one.)

data	
a data.frame (or list) from which the variables in formula should be taken.

subset	
an optional vector specifying a subset of observations to be used for plotting.

na.action	
a function which indicates what should happen when the data contain NAs. The default is to pass missing values on to the panel functions, but na.action = na.omit will cause cases with missing values in any of the variables to be omitted entirely.

labels	
the names of the variables.

panel	
function(x, y, ...) which is used to plot the contents of each panel of the display.

...	
arguments to be passed to or from methods.

Also, graphical parameters can be given as can arguments to plot such as main. par("oma") will be set appropriately unless specified.

horInd, verInd	
The (numerical) indices of the variables to be plotted on the horizontal and vertical axes respectively.

lower.panel, upper.panel	
separate panel functions (or NULL) to be used below and above the diagonal respectively.

diag.panel	
optional function(x, ...) to be applied on the diagonals.

text.panel	
optional function(x, y, labels, cex, font, ...) to be applied on the diagonals.

label.pos	
y position of labels in the text panel.

line.main	
if main is specified, line.main gives the line argument to mtext() which draws the title. You may want to specify oma when changing line.main.

cex.labels, font.labels	
graphics parameters for the text panel.

row1attop	
logical. Should the layout be matrix-like with row 1 at the top, or graph-like with row 1 at the bottom?

gap	
distance between subplots, in margin lines.

log	
a character string indicating if logarithmic axes are to be used: see plot.default. log = "xy" specifies logarithmic axes for all variables.

Details

The ijth scatterplot contains x[,i] plotted against x[,j]. The scatterplot can be customised by setting panel functions to appear as something completely different. The off-diagonal panel functions are passed the appropriate columns of x as x and y: the diagonal panel function (if any) is passed a single column, and the text.panel function is passed a single (x, y) location and the column name. Setting some of these panel functions to NULL is equivalent to not drawing anything there.

The graphical parameters pch and col can be used to specify a vector of plotting symbols and colors to be used in the plots.

The graphical parameter oma will be set by pairs.default unless supplied as an argument.

A panel function should not attempt to start a new plot, but just plot within a given coordinate system: thus plot and boxplot are not panel functions.

By default, missing values are passed to the panel functions and will often be ignored within a panel. However, for the formula method and na.action = na.omit, all cases which contain a missing values for any of the variables are omitted completely (including when the scales are selected).

Arguments horInd and verInd were introduced in R 3.2.0. If given the same value they can be used to select or re-order variables: with different ranges of consecutive values they can be used to plot rectangular windows of a full pairs plot; in the latter case ‘diagonal’ refers to the diagonal of the full plot.

Author(s)

Enhancements for R 1.0.0 contributed by Dr. Jens Oehlschlaegel-Akiyoshi and R-core members.

References

Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988) The New S Language. Wadsworth & Brooks/Cole.

Examples

pairs(iris[1:4], main = "Anderson's Iris Data -- 3 species",
      pch = 21, bg = c("red", "green3", "blue")[unclass(iris$Species)])  # 可给散点图加颜色/Work1/home/wangdong/lss/data/sw_20150110/SW_16_Samples_R／Rplots.pdf，空心散点，实际上那个correlation.r就是用pairs来做图的,按照species里面的几个因素

## formula method
pairs(~ Fertility + Education + Catholic, data = swiss,
      subset = Education < 20, main = "Swiss data, Education < 20") 

pairs(USJudgeRatings)
## show only lower triangle (and suppress labeling for whatever reason):
pairs(USJudgeRatings, text.panel = NULL, upper.panel = NULL)

## put histograms on the diagonal
panel.hist <- function(x, ...)
{
    usr <- par("usr"); on.exit(par(usr))
    par(usr = c(usr[1:2], 0, 1.5) )
    h <- hist(x, plot = FALSE)
    breaks <- h$breaks; nB <- length(breaks)
    y <- h$counts; y <- y/max(y)
    rect(breaks[-nB], 0, breaks[-1], y, col = "cyan", ...)
}
pairs(USJudgeRatings[1:5], panel = panel.smooth,	
      cex = 1.5, pch = 24, bg = "light blue",
      diag.panel = panel.hist, cex.labels = 2, font.labels = 2)  ＃这儿可以改变字体，颜色大小等等
## put (absolute) correlations on the upper panels,
## with size proportional to the correlations.
panel.cor <- function(x, y, digits = 2, prefix = "", cex.cor, ...)
{
    usr <- par("usr"); on.exit(par(usr))
    par(usr = c(0, 1, 0, 1))
    r <- abs(cor(x, y))
    txt <- format(c(r, 0.123456789), digits = digits)[1]
    txt <- paste0(prefix, txt)
    if(missing(cex.cor)) cex.cor <- 0.8/strwidth(txt)
    text(0.5, 0.5, txt, cex = cex.cor * r)
}
pairs(USJudgeRatings, lower.panel = panel.smooth, upper.panel = panel.cor)

pairs(iris[-5], log = "xy") # plot all variables on log scale
pairs(iris, log = 1:4, # log the first four
      main = "Lengths and Widths in [log]", line.main=1.5, oma=c(2,2,3,2))


结束

      


pairs.panels {psych}

SPLOM, histograms and correlations for a data matrix
Package:  psych
Version:  1.4.2.3
Description
Adapted from the help page for pairs, pairs.panels shows a scatter plot of matrices (SPLOM), with bivariate scatter plots below the diagonal, histograms on the diagonal, and the Pearson correlation above the diagonal. Useful for descriptive statistics of small data sets. If lm=TRUE, linear regression fits are shown for both y by x and x by y. Correlation ellipses are also shown. Points may be given different colors depending upon some grouping variable.

Usage
 
## S3 method for class 'panels':
pairs((x, smooth = TRUE, scale = FALSE, density=TRUE,ellipses=TRUE,
     digits = 2,method="pearson", pch = 20,lm=FALSE, 
     cor=TRUE,jiggle=FALSE,factor=2,hist.col="cyan",show.points=TRUE,rug=TRUE, ...))

Arguments
x
a data.frame or matrix
smooth
TRUE draws loess smooths
scale
TRUE scales the correlation font by the size of the absolute correlation.
density
TRUE shows the density plots as well as histograms
ellipses
TRUE draws correlation ellipses
lm
Plot the linear fit rather than the LOESS smoothed fits.
digits
the number of digits to show
method
method parameter for the correlation ("pearson","spearman","kendall")
pch
The plot character (defaults to 20 which is a '.').
cor
If plotting regressions, should correlations be reported?
jiggle
Should the points be jittered before plotting?
factor
factor for jittering (1-5)
hist.col
What color should the histogram on the diagonal be?
show.points
If FALSE, do not show the data points, just the data ellipses and smoothed functions
rug
if TRUE (default) draw a rug under the histogram, if FALSE, don't draw the rug
...
other options for pairs
Details
Shamelessly adapted from the pairs help page. Uses panel.cor, panel.cor.scale, and panel.hist, all taken from the help pages for pairs. Also adapts the ellipse function from John Fox's car package.

pairs.panels is most useful when the number of variables to plot is less than about 6-10. It is particularly useful for an initial overview of the data.

To show different groups with different colors, use a plot character (pch) between 21 and 25 and then set the background color to vary by group. (See the second example).

When plotting more than about 10 variables, it is useful to set the gap parameter to something less than 1 (e.g., 0). Alternatively, consider using cor.plot

In addition, when plotting more than about 100-200 cases, it is useful to set the plotting character to be a point. (pch=".")

Sometimes it useful to draw the correlation ellipses and best fitting lowess without the points. (points.false=TRUE).

Values
A scatter plot matrix (SPLOM) is drawn in the graphic window. The lower off diagonal draws scatter plots, the diagonal histograms, the upper off diagonal reports the Pearson correlation (with pairwise deletion).

If lm=TRUE, then the scatter plots are drawn above and below the diagonal, each with a linear regression fit. Useful to show the difference between regression lines.

See Also
pairs which is the base from which pairs.panels is derived, cor.plot to do a heat map of correlations, and scatter.hist to draw a single correlation plot with histograms and best fitted lines.

Examples
pairs.panels(attitude)   #see the graphics window
data(iris)
pairs.panels(iris[1:4],bg=c("red","yellow","blue")[iris$Species],
        pch=21,main="Fisher Iris data by Species") #to show color grouping
 
pairs.panels(iris[1:4],bg=c("red","yellow","blue")[iris$Species],
   pch=21,main="Fisher Iris data by Species",hist.col="red") 
   #to show changing the diagonal
 
#demonstrate not showing the data points
data(sat.act)
pairs.panels(sat.act,show.points=FALSE)
#better yet is to show the points as a period
pairs.panels(sat.act,pch=".")
#show many variables with 0 gap between scatterplots
# data(bfi)
# pairs.panels(bfi,show.points=FALSE,gap=0)

sort above_six_A.fa | uniq -u >> above_6A_unique.fa

sw相关图所在路径（ibm-a）
/Work1/home/wangdong/lss/data/sw_20150110/SW_16_Samples_R

Error bars are a graphical representation of the variability of data and are used on graphs to indicate the error, or uncertainty in a reported measurement. They give a general idea of how precise a measurement is, or conversely, how far from the reported value the true (error free) value might be. Error bars often represent one standard deviation of uncertainty, one standard error, or a certain confidence interval (e.g., a 95% interval). These quantities are not the same and so the measure selected should be stated explicitly in the graph or supporting text.

Error bars can be used to compare visually two quantities if various other conditions hold. This can determine whether differences are statistically significant. Error bars can also suggest goodness of fit of a given function, i.e., how well the function describes the data

标准分数（Standard Score，又称z-score，中文称为Z-分数或标准化值）在统计学中是一种无因次值，是借由从单一（原始）分数中减去母体的平均值，再依照母体（母集合）的标准差分割成不同的差距。

目录  [隐藏] 
1 概念
2 数理统计学中的标准化
3 应用
4 外部链接
5 参见
概念[编辑]
标准分数与使用在高速筛选分析中的“Z-因数”（z-factor）不同，甚至有时两者会互相混淆。

其约化过程被称为“标准化”（standardizing）。

标准分数可借由以下公式求出：

 z = {x - \mu \over \sigma}
其中 \sigma \ne 0。

其中

x\, 是需要被标准化的原始分数
\mu\, 是母体的平均值
\sigma\, 是母体的标准差
Z值的量代表着原始分数和母体平均值之间的距离，是以标准差为单位计算。在原始分数低于平均值时Z则为负数，反之则为正数。

关键点是，计算Z值时需要“母体”的平均值和标准差，而不是“样本”的平均值和标准差。因此需要了解母体的统计数据资料。

但是要确实了解母体真正的标准差往往是不切实际的，除非是在“标准化测验”（Standardized testing）之类的情形中，整个母体都是经过测量的。在其他情况中，几乎不可能测量母体的每一个组成单位，因此通常会使用随机的样本来评估标准差。例如：“有吸烟习惯的总人数”就不是经过一个一个测量而得出的。

当母体为正态分布时，其百分位数可能是由标准分数和普通表格所决定的。

数理统计学中的标准化[编辑]
在数理统计学中，随机变数“X”是使用理论（母体）的平均值和标准差所标准化的结果：

Z = {X - \mu \over \sigma}
其中 μ = E(X) 为平均值、σ05 = Var(X) X的概率分布之方差

若随机变数无法确定时，则为算术平均数：

\bar{X}={1 \over n} \sum_{i=1}^n X_i
因此经过标准化的结果为：

Z={\bar{X}-\mu\over\sigma/\sqrt{n}}.
应用[编辑]

 reads processed: 2021918
# reads with at least one reported alignment: 336945 (16.66%)
# reads that failed to align: 1684973 (83.34%)
Reported 336945 alignments to 1 output stream(s)


1.Linux下

增加一下三行：

给你的ls一点颜色

[html] view plaincopy

    alias ls='/bin/ls --color=auto'  


给你的grep一点颜色

[html] view plaincopy

    alias grep='grep --color'  


给你的vi一点颜色

[html] view plaincopy

    alias vi='vim'  


2.Mac下

增加一下三行：

给你的ls一点颜色

[html] view plaincopy

    alias ls='/bin/ls -G'  


给你的grep一点颜色

[html] view plaincopy

    alias grep='grep --color'  


给你的vi一点颜色

[html] view plaincopy

    alias vi='vim'  


3.修改颜色

修改/etc/profile
加入

[html] view plaincopy

    export CLICOLOR=1  
      
    export LSCOLORS=gxfxaxdxcxegedabagacad  

CLICOLOR是用来设置是否进行颜色的显示。CLI是Command Line Interface的缩写。

LSCOLORS是用来设置当CLICOLOR被启用后，各种文件类型的颜色。LSCOLORS的值中每两个字母为一组，分别设置某个文件类型的文字颜色和背景颜色。LSCOLORS中一共11组颜色设置，按照先后顺序，分别对以下的文件类型进行设置：

    [html] view plaincopy
        directory  
        symbolic link  
        socket  
        pipe  
        executable  
        block special  
        character special  
        executable with setuid bit set  
        executable with setgid bit set  
        directory writable to others, with sticky bit  
        directory writable to others, without sticky bit  

LSCOLORS中，字母代表的颜色如下：

    [html] view plaincopy
        a 黑色  
        b 红色  
        c 绿色  
        d 棕色  
        e 蓝色  
        f 洋红色  
        g 青色  
        h 浅灰色  
        A 黑色粗体  
        B 红色粗体  
        C 绿色粗体  
        D 棕色粗体  
        E 蓝色粗体  
        F 洋红色粗体  
        G 青色粗体  
        H 浅灰色粗体  
        

Vim

Vim 的配色最好和终端的配色保持一致，不然在 Terminal/iTerm2 里使用命令行 Vim 会很别扭：

$ cd solarized
$ cd vim-colors-solarized/colors
$ mkdir -p ~/.vim/colors
$ cp solarized.vim ~/.vim/colors/

$ vi ~/.vimrc
syntax enable
set background=dark
colorscheme solarized
ls

vimrc是vim的配置文件，是启动vim的时候自动读取的，不需要也不能够在终端里面运行或者像bashrc那样导入，他们不是一样的东西。vimrc是不能source的，source是对bash的相关设置用的，对vim不会起作用。

Mac OS X 是基于 FreeBSD 的，所以一些工具 ls, top 等都是 BSD 那一套，ls 不是 GNU ls，所以即使 Terminal/iTerm2 配置了颜色，但是在 Mac 上敲入 ls 命令也不会显示高亮，可以通过安装 coreutils 来解决（brew install coreutils），不过如果对 ls 颜色不挑剔的话有个简单办法就是在 .bash_profile 里输出 CLICOLOR=1：

$ vi ~/.bash_profile
export CLICOLOR=1



mkdir的-p选项允许你一次性创建多层次的目录，而不是一次只创建单独的目录。例如，我们要在当前目录创建目录Projects/a/src，使用命令

mkdir -p Project/a/src

git clone git://github.com/altercation/solarized.git /Users/lishasha/Desktop/home/tools
fatal: destination path '/Users/lishasha/Desktop/home/tools' already exists and is not an empty directory.
git clone  可以指定下载所在文件夹，但是要是一个空的文件夹

bowtie -n 3 /Share/home/wangdong/lss/project/20150430_capitalbio_AR/misligation/capital_bio_misligation /Share/home/wangdong/lss/project/20150430_capitalbio_AR/misligation/unmatched_reads2.fastq -S /Share/home/wangdong/lss/project/20150430_capitalbio_AR/misligation/unmatched_reads.sam
bowtie -n 3 /Share/home/wangdong/lss/project/20150430_sw_probes_screen/ref_1411 /Share/home/wangdong/data/rawdata/150429_C00126_0187_AHFMG2ADXX/Project_HFMG2ADXX/fastq/Sample_SW_our_lab_probe.fastq -S /Share/home/wangdong/lss/project/20150430_sw_probes_screen/mapping/Sample_SW_our_lab_probe_tmp.sam

/Share/home/wangdong/local/app/bowtie-1.1.1/bowtie-build -f /Share/home/wangdong/lss/project/20150430_capitalbio_AR/misligation/capital_bio_misligation.fa /Share/home/wangdong/lss/project/20150430_capitalbio_AR/misligation/capital_bio_misligation
/Share/home/wangdong/local/app/bowtie-1.1.1/bowtie-build -f /Share/home/wangdong/lss/project/20150430_capitalbio_AR/misligation/capital_bio_misligation.fa /Share/home/wangdong/lss/project/20150430_capitalbio_AR/misligation/capital_bio_misligation2
VIM如何将空格替换为换行，比如有时需要把很长的一行按空格分为多行，这时就可以用如下命令实现:

:%s/ +/\r/gc

参数解释：

    %s ：在整个文件范围查找替换
    / ： 分隔符
    + ：匹配空格，其中“ ”表示空格，+表示重复1次或多次，加在一起表示一个或多个空格。
    \r ：换行符
    g ：全局替换
    c ：替换前确认
    
touch unmatched_reads.fastq # --un参数输出没有match的reads,需要建一个空文件unmatched_reads.fastq 
bowtie -n 3 --un /Share/home/wangdong/lss/project/20150430_capitalbio_AR/mapping/unmatched_reads2.fastq /Share/home/wangdong/lss/project/20150430_capitalbio_AR/ref_2920_biocapital /Share/home/wangdong/data/rawdata/150429_C00126_0187_AHFMG2ADXX/Project_HFMG2ADXX/fastq/Sample_SW_captialbio_probe.fastq -S /Share/home/wangdong/lss/project/20150430_capitalbio_AR/mapping/captialbio_TEST.sam

pheatmap
/Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/plot
pheatmap	R Documentation
A function to draw clustered heatmaps.
Description
A function to draw clustered heatmaps where one has better control over some graphical parameters such as cell size, etc.
Usage
  pheatmap(mat,
    color = colorRampPalette(rev(c("#D73027", "#FC8D59", "#FEE090", "#FFFFBF", "#E0F3F8", "#91BFDB", "#4575B4")))(100),
    kmeans_k = NA, breaks = NA, border_color = "grey60",
    cellwidth = NA, cellheight = NA, scale = "none",
    cluster_rows = TRUE, cluster_cols = TRUE,
    clustering_distance_rows = "euclidean",
    clustering_distance_cols = "euclidean",
    clustering_method = "complete",
    treeheight_row = ifelse(cluster_rows, 50, 0),
    treeheight_col = ifelse(cluster_cols, 50, 0),
    legend = TRUE, legend_breaks = NA, legend_labels = NA,
    annotation = NA, annotation_colors = NA,
    annotation_legend = TRUE, drop_levels = TRUE,
    show_rownames = T, show_colnames = T, main = NA,
    fontsize = 10, fontsize_row = fontsize,
    fontsize_col = fontsize, display_numbers = F,
    number_format = "%.2f",
    fontsize_number = 0.8 * fontsize, filename = NA,
    width = NA, height = NA, ...)
Arguments
de>matde>	
numeric matrix of the values to be plotted.
de>colorde>	
vector of colors used in heatmap.
de>kmeans_kde>	
the number of kmeans clusters to make, if we want to agggregate the rows before drawing heatmap. If NA then the rows are not aggregated.
de>breaksde>	
a sequence of numbers that covers the range of values in mat and is one element longer than color vector. Used for mapping values to colors. Useful, if needed to map certain values to certain colors, to certain values. If value is NA then the breaks are calculated automatically.
de>border_colorde>	
color of cell borders on heatmap, use NA if no border should be drawn.
de>cellwidthde>	
inpidual cell width in points. If left as NA, then the values depend on the size of plotting window.
de>cellheightde>	
inpidual cell height in points. If left as NA, then the values depend on the size of plotting window.
de>scalede>	
character indicating if the values should be centered and scaled in either the row direction or the column direction, or none. Corresponding values are de>"row"de>, de>"column"de> and de>"none"de>
de>cluster_rowsde>	
boolean values determining if rows should be clustered,
de>cluster_colsde>	
boolean values determining if columns should be clustered.
de>clustering_distance_rowsde>	
distance measure used in clustering rows. Possible values are de>"correlation"de> for Pearson correlation and all the distances supported by de>distde>, such as de>"euclidean"de>, etc. If the value is none of the above it is assumed that a distance matrix is provided.
de>clustering_distance_colsde>	
distance measure used in clustering columns. Possible values the same as for clustering_distance_rows.
de>clustering_methodde>	
clustering method used. Accepts the same values as de>hclustde>.
de>treeheight_rowde>	
the height of a tree for rows, if these are clustered. Default value 50 points.
de>treeheight_colde>	
the height of a tree for columns, if these are clustered. Default value 50 points.
de>legendde>	
logical to determine if legend should be drawn or not.
de>legend_breaksde>	
vector of breakpoints for the legend.
de>legend_labelsde>	
vector of labels for the de>legend_breaksde>.
de>annotationde>	
data frame that specifies the annotations shown on top of the columns. Each row defines the features for a specific column. The columns in the data and rows in the annotation are matched using corresponding row and column names. Note that color schemes takes into account if variable is continuous or discrete.
de>annotation_colorsde>	
list for specifying annotation track colors manually. It is possible to define the colors for only some of the features. Check examples for details.
de>annotation_legendde>	
boolean value showing if the legend for annotation tracks should be drawn.
de>drop_levelsde>	
logical to determine if unused levels are also shown in the legend
de>show_rownamesde>	
boolean specifying if column names are be shown.
de>show_colnamesde>	
boolean specifying if column names are be shown.
de>mainde>	
the title of the plot
de>fontsizede>	
base fontsize for the plot
de>fontsize_rowde>	
fontsize for rownames (Default: fontsize)
de>fontsize_colde>	
fontsize for colnames (Default: fontsize)
de>display_numbersde>	
logical determining if the numeric values are also printed to the cells.
de>number_formatde>	
format strings (C printf style) of the numbers shown in cells. For example "de>%.2fde>" shows 2 decimal places and "de>%.1ede>" shows exponential notation (see more in de>sprintfde>).
de>fontsize_numberde>	
fontsize of the numbers displayed in cells
de>filenamede>	
file path where to save the picture. Filetype is decided by the extension in the path. Currently following formats are supported: png, pdf, tiff, bmp, jpeg. Even if the plot does not fit into the plotting window, the file size is calculated so that the plot would fit there, unless specified otherwise.
de>widthde>	
manual option for determining the output file width in
de>heightde>	
manual option for determining the output file height in inches.
de>...de>	
graphical parameters for the text used in plot. Parameters passed to de>grid.textde>, see de>gparde>.
Details
The function also allows to aggregate the rows using kmeans clustering. This is advisable if number of rows is so big that R cannot handle their hierarchical clustering anymore, roughly more than 1000. Instead of showing all the rows separately one can cluster the rows in advance and show only the cluster centers. The number of clusters can be tuned with parameter kmeans_k.
Value
Invisibly a list of components
de>tree_rowde> the clustering of rows as de>hclustde> object
de>tree_colde> the clustering of columns as de>hclustde> object
de>kmeansde> the kmeans clustering of rows if parameter de>kmeans_kde> was specified
Author(s)
Raivo Kolde <rkolde@gmail.com>
Examples
# Generate some data
test = matrix(rnorm(200), 20, 10)
test[1:10, seq(1, 10, 2)] = test[1:10, seq(1, 10, 2)] + 3
test[11:20, seq(2, 10, 2)] = test[11:20, seq(2, 10, 2)] + 2
test[15:20, seq(2, 10, 2)] = test[15:20, seq(2, 10, 2)] + 4
colnames(test) = paste("Test", 1:10, sep = "")
rownames(test) = paste("Gene", 1:20, sep = "")

# Draw heatmaps
pheatmap(test)
pheatmap(test, kmeans_k = 2)
pheatmap(test, scale = "row", clustering_distance_rows = "correlation")
pheatmap(test, color = colorRampPalette(c("navy", "white", "firebrick3"))(50))
pheatmap(test, cluster_row = FALSE)
pheatmap(test, legend = FALSE)
pheatmap(test, display_numbers = TRUE)
pheatmap(test, display_numbers = TRUE, number_format = "%.1e")
pheatmap(test, cluster_row = FALSE, legend_breaks = -1:4, legend_labels = c("0",
"1e-4", "1e-3", "1e-2", "1e-1", "1"))
pheatmap(test, cellwidth = 15, cellheight = 12, main = "Example heatmap")
pheatmap(test, cellwidth = 15, cellheight = 12, fontsize = 8, filename = "test.pdf")


# Generate column annotations
annotation = data.frame(Var1 = factor(1:10 %% 2 == 0, labels = c("Class1", "Class2")), Var2 = 1:10)
annotation$Var1 = factor(annotation$Var1, levels = c("Class1", "Class2", "Class3"))
rownames(annotation) = paste("Test", 1:10, sep = "")

pheatmap(test, annotation = annotation)
pheatmap(test, annotation = annotation, annotation_legend = FALSE)
pheatmap(test, annotation = annotation, annotation_legend = FALSE, drop_levels = FALSE)

# Specify colors
Var1 = c("navy", "darkgreen")
names(Var1) = c("Class1", "Class2")
Var2 = c("lightgreen", "navy")

ann_colors = list(Var1 = Var1, Var2 = Var2)

pheatmap(test, annotation = annotation, annotation_colors = ann_colors, main = "Example with all the features")

# Specifying clustering from distance matrix
drows = dist(test, method = "minkowski")
dcols = dist(t(test), method = "minkowski")
pheatmap(test, clustering_distance_rows = drows, clustering_distance_cols = dcols)


R Graphical Manual

Source Code Search
Blog (test)
Help (test)
Browse All
Last data update: 2013.05.20
Data Source
R Release (2.15.3) 
CranContrib 
BioConductor 
All 
Data Type
Packages 
Functions 
Images 
Data set 
pheatmap	R Documentation
A function to draw clustered heatmaps.
Description
A function to draw clustered heatmaps where one has better control over some graphical parameters such as cell size, etc.
Usage
  pheatmap(mat,
    color = colorRampPalette(rev(c("#D73027", "#FC8D59", "#FEE090", "#FFFFBF", "#E0F3F8", "#91BFDB", "#4575B4")))(100),
    kmeans_k = NA, breaks = NA, border_color = "grey60",
    cellwidth = NA, cellheight = NA, scale = "none",
    cluster_rows = TRUE, cluster_cols = TRUE,
    clustering_distance_rows = "euclidean",
    clustering_distance_cols = "euclidean",
    clustering_method = "complete",
    treeheight_row = ifelse(cluster_rows, 50, 0),
    treeheight_col = ifelse(cluster_cols, 50, 0),
    legend = TRUE, legend_breaks = NA, legend_labels = NA,
    annotation = NA, annotation_colors = NA,
    annotation_legend = TRUE, drop_levels = TRUE,
    show_rownames = T, show_colnames = T, main = NA,
    fontsize = 10, fontsize_row = fontsize,
    fontsize_col = fontsize, display_numbers = F,
    number_format = "%.2f",
    fontsize_number = 0.8 * fontsize, filename = NA,
    width = NA, height = NA, ...)
Arguments
de>matde>	
numeric matrix of the values to be plotted.
de>colorde>	
vector of colors used in heatmap.
de>kmeans_kde>	
the number of kmeans clusters to make, if we want to agggregate the rows before drawing heatmap. If NA then the rows are not aggregated.
de>breaksde>	
a sequence of numbers that covers the range of values in mat and is one element longer than color vector. Used for mapping values to colors. Useful, if needed to map certain values to certain colors, to certain values. If value is NA then the breaks are calculated automatically.
de>border_colorde>	
color of cell borders on heatmap, use NA if no border should be drawn.
de>cellwidthde>	
inpidual cell width in points. If left as NA, then the values depend on the size of plotting window.
de>cellheightde>	
inpidual cell height in points. If left as NA, then the values depend on the size of plotting window.
de>scalede>	
character indicating if the values should be centered and scaled in either the row direction or the column direction, or none. Corresponding values are de>"row"de>, de>"column"de> and de>"none"de>
de>cluster_rowsde>	
boolean values determining if rows should be clustered,
de>cluster_colsde>	
boolean values determining if columns should be clustered.
de>clustering_distance_rowsde>	
distance measure used in clustering rows. Possible values are de>"correlation"de> for Pearson correlation and all the distances supported by de>distde>, such as de>"euclidean"de>, etc. If the value is none of the above it is assumed that a distance matrix is provided.
de>clustering_distance_colsde>	
distance measure used in clustering columns. Possible values the same as for clustering_distance_rows.
de>clustering_methodde>	
clustering method used. Accepts the same values as de>hclustde>.
de>treeheight_rowde>	
the height of a tree for rows, if these are clustered. Default value 50 points.
de>treeheight_colde>	
the height of a tree for columns, if these are clustered. Default value 50 points.
de>legendde>	
logical to determine if legend should be drawn or not.
de>legend_breaksde>	
vector of breakpoints for the legend.
de>legend_labelsde>	
vector of labels for the de>legend_breaksde>.
de>annotationde>	
data frame that specifies the annotations shown on top of the columns. Each row defines the features for a specific column. The columns in the data and rows in the annotation are matched using corresponding row and column names. Note that color schemes takes into account if variable is continuous or discrete.
de>annotation_colorsde>	
list for specifying annotation track colors manually. It is possible to define the colors for only some of the features. Check examples for details.
de>annotation_legendde>	
boolean value showing if the legend for annotation tracks should be drawn.
de>drop_levelsde>	
logical to determine if unused levels are also shown in the legend
de>show_rownamesde>	
boolean specifying if column names are be shown.
de>show_colnamesde>	
boolean specifying if column names are be shown.
de>mainde>	
the title of the plot
de>fontsizede>	
base fontsize for the plot
de>fontsize_rowde>	
fontsize for rownames (Default: fontsize)
de>fontsize_colde>	
fontsize for colnames (Default: fontsize)
de>display_numbersde>	
logical determining if the numeric values are also printed to the cells.
de>number_formatde>	
format strings (C printf style) of the numbers shown in cells. For example "de>%.2fde>" shows 2 decimal places and "de>%.1ede>" shows exponential notation (see more in de>sprintfde>).
de>fontsize_numberde>	
fontsize of the numbers displayed in cells
de>filenamede>	
file path where to save the picture. Filetype is decided by the extension in the path. Currently following formats are supported: png, pdf, tiff, bmp, jpeg. Even if the plot does not fit into the plotting window, the file size is calculated so that the plot would fit there, unless specified otherwise.
de>widthde>	
manual option for determining the output file width in
de>heightde>	
manual option for determining the output file height in inches.
de>...de>	
graphical parameters for the text used in plot. Parameters passed to de>grid.textde>, see de>gparde>.
Details
The function also allows to aggregate the rows using kmeans clustering. This is advisable if number of rows is so big that R cannot handle their hierarchical clustering anymore, roughly more than 1000. Instead of showing all the rows separately one can cluster the rows in advance and show only the cluster centers. The number of clusters can be tuned with parameter kmeans_k.
Value
Invisibly a list of components
de>tree_rowde> the clustering of rows as de>hclustde> object
de>tree_colde> the clustering of columns as de>hclustde> object
de>kmeansde> the kmeans clustering of rows if parameter de>kmeans_kde> was specified
Author(s)
Raivo Kolde <rkolde@gmail.com>
Examples
# Generate some data
test = matrix(rnorm(200), 20, 10)
test[1:10, seq(1, 10, 2)] = test[1:10, seq(1, 10, 2)] + 3
test[11:20, seq(2, 10, 2)] = test[11:20, seq(2, 10, 2)] + 2
test[15:20, seq(2, 10, 2)] = test[15:20, seq(2, 10, 2)] + 4
colnames(test) = paste("Test", 1:10, sep = "")
rownames(test) = paste("Gene", 1:20, sep = "")

# Draw heatmaps
pheatmap(test)
pheatmap(test, kmeans_k = 2)
pheatmap(test, scale = "row", clustering_distance_rows = "correlation")
pheatmap(test, color = colorRampPalette(c("navy", "white", "firebrick3"))(50))
pheatmap(test, cluster_row = FALSE)
pheatmap(test, legend = FALSE)
pheatmap(test, display_numbers = TRUE)
pheatmap(test, display_numbers = TRUE, number_format = "%.1e")
pheatmap(test, cluster_row = FALSE, legend_breaks = -1:4, legend_labels = c("0",
"1e-4", "1e-3", "1e-2", "1e-1", "1"))
pheatmap(test, cellwidth = 15, cellheight = 12, main = "Example heatmap")
pheatmap(test, cellwidth = 15, cellheight = 12, fontsize = 8, filename = "test.pdf")


# Generate column annotations
annotation = data.frame(Var1 = factor(1:10 %% 2 == 0, labels = c("Class1", "Class2")), Var2 = 1:10)
annotation$Var1 = factor(annotation$Var1, levels = c("Class1", "Class2", "Class3"))
rownames(annotation) = paste("Test", 1:10, sep = "")

pheatmap(test, annotation = annotation)
pheatmap(test, annotation = annotation, annotation_legend = FALSE)
pheatmap(test, annotation = annotation, annotation_legend = FALSE, drop_levels = FALSE)

# Specify colors
Var1 = c("navy", "darkgreen")
names(Var1) = c("Class1", "Class2")
Var2 = c("lightgreen", "navy")

ann_colors = list(Var1 = Var1, Var2 = Var2)

pheatmap(test, annotation = annotation, annotation_colors = ann_colors, main = "Example with all the features")

# Specifying clustering from distance matrix
drows = dist(test, method = "minkowski")
dcols = dist(t(test), method = "minkowski")
pheatmap(test, clustering_distance_rows = drows, clustering_distance_cols = dcols)
Results

draw clustered heatmaps_pheatmap(热图) - 云之南 - 云之南 draw clustered heatmaps_pheatmap(热图) - 云之南 - 云之南 draw clustered heatmaps_pheatmap(热图) - 云之南 - 云之南 draw clustered heatmaps_pheatmap(热图) - 云之南 - 云之南 draw clustered heatmaps_pheatmap(热图) - 云之南 - 云之南 draw clustered heatmaps_pheatmap(热图) - 云之南 - 云之南 draw clustered heatmaps_pheatmap(韧) - 云之南 - 云之南 draw clustered heatmaps_pheatmap(热图) - 云之南 - 云之南 draw clustered heatmaps_pheatmap(热图) - 云之南 - 云之南 draw clustered heatmaps_pheatmap(热图) - 云之南 - 云之南 draw clustered heatmaps_pheatmap(热图) - 云之南 - 云之南 draw clustered heatmaps_pheatmap(热图) - 云之南 - 云之南 draw clustered heatmaps_pheatmap(热图) - 云之南 - 云之南 draw clustered heatmaps_pheatmap(热图) - 云之南 - 云之南 draw clustered heatmaps_pheatmap(热图) - 云之南 - 云之南 draw clustered heatmaps_pheatmap(热图) - 云之南 - 云之南

Created & Maintained by Osamu Ogasawara (osamu.ogasawara@gmail.com)

library("pheatmap")
pheatmap(c,
border_color="#FFFAEA",
color=colorRampPalette(rev(c("red","gray90","green")))(102),
cellwidth=8,cellheight=8,
fontsize=7,
cluster_rows=TRUE,cluster_cols=FALSE,
breaks=c(seq(0,0.9,length.out=51),seq(1.1,6,length.out=51))

)
library(grid)
library(VennDiagram)
mydata1<-read.table("/Users/lishasha/Desktop/home/new_work/biocapital_2700probe/expression_data/value_four_cellline_3000genes/PLOT/MDA231.txt",head=T)
mydata2<-read.table("/Users/lishasha/Desktop/home/new_work/biocapital_2700probe/expression_data/value_four_cellline_3000genes/PLOT/SW620.txt",head=T)
mydata3<-read.table("/Users/lishasha/Desktop/home/new_work/biocapital_2700probe/expression_data/value_four_cellline_3000genes/PLOT/THP1.txt",head=T)
mydata4<-read.table("/Users/lishasha/Desktop/home/new_work/biocapital_2700probe/expression_data/value_four_cellline_3000genes/PLOT/hepg2.txt",head=T)
mydata5<-read.table("/Users/lishasha/Desktop/home/new_work/biocapital_2700probe/expression_data/value_four_cellline_3000genes/PLOT/hp7702.txt",head=T)
venn.diagram(list("MDA231"=mydata1$MDA231,"SW620"=mydata2$SW620,"PMA_THP1"=mydata3$PMA_THP1,"HEPG2"=mydata4$HEPG2,"HP_7702"=mydata5$HP_7702),fill=c("red","blue","yellow","green","pink"),"/Users/lishasha/Desktop/home/new_work/biocapital_2700probe/expression_data/value_four_cellline_3000genes/PLOT/O6_cex_lables.png",main="overlap in 5 cell lines",main.col = "black",main.cex =1.5,cex.labels=0.6, font.labels=4)


library(grid)
library(VennDiagram)
mydata1<-read.table("/Users/lishasha/Desktop/home/new_work/biocapital_2700probe/expression_data/value_four_cellline_3000genes/PLOT/MDA231.txt",head=T)
mydata2<-read.table("/Users/lishasha/Desktop/home/new_work/biocapital_2700probe/expression_data/value_four_cellline_3000genes/PLOT/SW620.txt",head=T)
mydata3<-read.table("/Users/lishasha/Desktop/home/new_work/biocapital_2700probe/expression_data/value_four_cellline_3000genes/PLOT/THP1.txt",head=T)
mydata4<-read.table("/Users/lishasha/Desktop/home/new_work/biocapital_2700probe/expression_data/value_four_cellline_3000genes/PLOT/hepg2.txt",head=T)
mydata5<-read.table("/Users/lishasha/Desktop/home/new_work/biocapital_2700probe/expression_data/value_four_cellline_3000genes/PLOT/hp7702.txt",head=T)
venn.diagram(list("MDA231"=mydata1$MDA231,"SW620"=mydata2$SW620,"PMA_THP1"=mydata3$PMA_THP1,"HEPG2"=mydata4$HEPG2,"HP_7702"=mydata5$HP_7702), height = 3500, width = 3500,resolution = 500,fill=c("red","blue","yellow","green","pink"),"/Users/lishasha/Desktop/home/new_work/biocapital_2700probe/expression_data/value_four_cellline_3000genes/PLOT/O6_height_res.png",main="overlap in 5 cell lines",main.col = "black",main.cex =1.5)

Make a Venn Diagram

Description:

     This function takes a list and creates a publication-quality TIFF
     Venn Diagram

Usage:

     venn.diagram(x, filename, height = 3000, width = 3000, resolution = 500, 
     imagetype = "tiff", units = "px", compression = "lzw", na = "stop", main = "", sub = "", 
     main.pos = c(0.5, 1.05), main.fontface = "plain", main.fontfamily = "serif", 
     main.col = "black", main.cex = 1, main.just = c(0.5, 1), sub.pos = c(0.5, 1.05), 
     sub.fontface = "plain", sub.fontfamily = "serif", sub.col = "black", sub.cex = 1, 
     sub.just = c(0.5, 1), category.names = names(x), force.unique = TRUE, ...);
     



http://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE5230

python /Share/home/wangdong/lss/project/20150430_capitalbio_AR/split_colume3.py /Share/home/wangdong/lss/project/20150430_capitalbio_AR/mapping/captialbio.sam
python /Share/home/wangdong/lss/project/20150430_capitalbio_AR/split_colume3.py /Share/home/wangdong/lss/project/20150430_capitalbio_AR/mapping/AR.sam

bowtie -n 3 /Share/home/wangdong/lss/project/20150430_capitalbio_AR/ref_2920_biocapital /Share/home/wangdong/data/rawdata/150429_C00126_0187_AHFMG2ADXX/Project_HFMG2ADXX/fastq/Sample_SW_captialbio_probe.fastq -S /Share/home/wangdong/lss/project/20150430_capitalbio_AR/mapping/captialbio.sam 
bowtie -n 3 /Share/home/wangdong/lss/project/20150430_capitalbio_AR/ref_89 /Share/home/wangdong/data/rawdata/150429_C00126_0187_AHFMG2ADXX/Project_HFMG2ADXX/fastq/Sample_SW_AR_probe.fastq -S /Share/home/wangdong/lss/project/20150430_capitalbio_AR/mapping/AR.sam

/Share/home/wangdong/local/app/bowtie-1.1.1/bowtie-build -f ref_2920_biocapital.fa ref_2920_biocapital
/Share/home/wangdong/local/app/bowtie-1.1.1/bowtie-build -f ref_89.fa ref_89

Normalizing microarray 3data in R
1) Place the .CEL file(s) in your working directory. By default, the working directory is the R folder; if you want to use any other directory, do so as follows:
a. >setwd(“c:\\My Documents\MyCelFiles”)
2) Open R.
3) > library(affy)
a. This opens the “affy” library in Bioconductor, which enables you to process Affymetrix data.
4) >memory.limit(size=3000)
a. You want to set the memory as high as possible; 3000 Mb (3 Gb) is the most Windows is capable of allocating to R. It is possible that it will not like that, and you will have to do “size=2000”. R will probably respond with the word “NULL”. (The newest build of R seems to have gotten around some of the Windows size problems.)
5) >rawdata <- ReadAffy()
a. You are choosing a name for an object (“rawdata” is sensible, but you could call it anything you like), and defining “rawdata” as the temporary file that holds the output of the “ReadAffy()” command. “ReadAffy()” reads all of the .CEL files in the R folder into “rawdata”; if you only wanted to work with, say, 24 hour replicates, you would say
i. >rawdata <- ReadAffy(“24H_rep1.CEL”, “24H_rep2.CEL”, “24H_rep3.CEL”)
6) >eset <- rma(rawdata)
a. You are creating an object (named “eset” by convention; again, it could be whatever you like it to be), and defining it as the outcome of running “rma” on your previously-named “rawdata”. “rma” does three things: “convolution background correction, quantile normalization, and a summarization based on a multi-array model fit robustly using the median polish algorithm” (BM Bolstad, RA Irizarry, L Gautier & Z Wu (2005) “Preprocessing high-density oligonucleotide arrays”, pp. 13-32 in Bioinformatics and Computational Biology Solutions Using R and Bioconductor, Gentleman, et al., eds. New York, Springer.)
7) >write.exprs(eset, file=”NormalizedDate.txt”)
a. You are exporting your normalized data, stored in “eset”, to a text file with the name of your choosing. I recommend “NormalizedJuly2006.txt”, or some other such thing to help identify precisely what generation it is.
8) Open Excel. Open your new normalized file and go through the steps to turn a text file into a proper Excel file.
9) Insert one cell in the upper left-hand corner of the Excel file, as all of the column headings are shifted one to the left of where they should be. Note the order in which R arranges your probe sets; it’s generally in alphabetical order, but can vary a bit between versions of R/Bioconductor. You may or may not be given a column with the numbers 1-x, x being the total number of probe sets on your GeneChip. If you do not have such a column, make one; your first probe (as ordered in the normalized file) is number 1 (and corresponds with the 2nd row on
your Excel file); your xth probe is number x (and corresponds with row x+1). This is important, because this is how R thinks of your probe sets; probe names (fg00505_at) mean nothing to R, and cannot be used in doing more sophisticated analyses.
10) Quit R: >q() 

将microarray数据.cel使用起来的pipeline
> source("http://www.bioconductor.org/biocLite.R")
> biocLite("affy")
> biocLite("affycoretools")
> library(affy)
> library(affycoretools)
> mydata <- ReadAffy("sw620_GSM1312999_CTS_69_SW620_0h.CEL")
> mydata
trying URL 'http://bioconductor.org/packages/3.0/data/annotation/src/contrib/hugene10stv1cdf_2.15.0.tar.gz'
Content type 'application/x-gzip' length 3010436 bytes (2.9 Mb)
opened URL
==================================================
downloaded 2.9 Mb

* installing *source* package 'hugene10stv1cdf' ...
** R
** data
** preparing package for lazy loading
** help
*** installing help indices
** building package indices
** testing if installed package can be loaded
* DONE (hugene10stv1cdf)

The downloaded source packages are in
        '/tmp/RtmppE94Ng/downloaded_packages'
Updating HTML index of packages in '.Library'
Making 'packages.html' ... donemakeTagDirectorymakeTagDirectory


AffyBatch object
size of arrays=1050x1050 features (17 kb)
cdf=HuGene-1_0-st-v1 (32321 affyids)
number of samples=1
number of genes=32321
annotation=hugene10stv1
notes=
> pdf("620hist_2.pdf")
> hist(mydata)
> pdf("620boxplot.pdf")
> boxplot(mydata)
> pdf("620image.pdf")
> image(mydata[,1])
#In affy package there are several methodologies available to correct background, normalize and summarize expression values per each probe set of the dataset.
#Here we will use the RMA and MAS5 methods that are implemented into ready made functions.
#In order to apply the RMA method, we can type:
>eset <- rma(mydata) #(normalization)
Background correcting
Normalizing
Calculating Expression 
> eset
ExpressionSet (storageMode: lockedEnvironment)
assayData: 32321 features, 1 samples 
  element names: exprs
protocolData
  sampleNames: sw620_GSM1312999_CTS_69_SW620_0h.CEL
  varLabels: ScanDate
  varMetadata: labelDescription
phenoData
  sampleNames: sw620_GSM1312999_CTS_69_SW620_0h.CEL
  varLabels: sample
  varMetadata: labelDescription
featureData: none
experimentData: use 'experimentData(object)'
Annotation: hugene10stv1
> eset1 <- mas5(mydata)
background correction: mas
PM/MM correction : mas
expression values: mas
background correcting...done.
32321 ids to be processed
|                    |
|####################|
> eset1
ExpressionSet (storageMode: lockedEnvironment)
assayData: 32321 features, 1 samples
  element names: exprs, se.exprs
protocolData
  sampleNames: sw620_GSM1312999_CTS_69_SW620_0h.CEL
  varLabels: ScanDate
  varMetadata: labelDescription
phenoData
  sampleNames: sw620_GSM1312999_CTS_69_SW620_0h.CEL
  varLabels: sample
  varMetadata: labelDescription
featureData: none
experimentData: use 'experimentData(object)'
Annotation: hugene10stv1
>pdf("exploratory_boxplot.pdf")
>boxplot(exprs(eset))
>dev.off()
>source("http://www.bioconductor.org/biocLite.R")
>biocLite("hgu133a.db")
>biocLite("annotate")
> biocLite("R2HTML")
> library(hgu133a.db)
> library(annotate)
Loading required package: XML

Attaching package: 'annotate'

The following object is masked from 'package:GenomeInfoDb':

    organism
> library(R2HTML)
We are now going to extract the feature names from eset2 that contains the selected genes of interest:
> ID <- featureNames(eset2)
And look up the Gene Symbol, Name, and Ensembl Gene ID for each of those IDs:
> Symbol <- getSYMBOL(ID, "hgu133a.db")
> Name <- as.character(lookUp(ID, "hgu133a.db", "GENENAME"))
> Ensembl <- as.character(lookUp(ID, "hgu133a.db", "ENSEMBL"))
For each Ensembl ID (if we have it), we will now create a hyperlink that goes to the Ensembl genome browser:
> Ensembl <- ifelse(Ensembl=="NA", NA,
paste("<a href='http://useast.ensembl.org/Homo_sapiens/Gene/Summary?g=",
Ensembl, "'>", Ensembl, "</a>", sep=""))
And make a temporary data frame with all those identifiers:
> tmp <- data.frame(ID=ID, Symbol=Symbol, Name=Name, Ensembl=Ensembl, stringsAsFactors=F) > tmp[tmp=="NA"] <- NA # The stringsAsFactors makes "NA" characters. This fixes that problem.
We are now going to write out an HTML file with clickable links to the Ensembl Genome Browser, and .txt file with gene list:
> HTML(tmp, "out.html", append=F)
> write.table(tmp ,file="target.txt",row.names=F, sep="\t")


 


boxplot等将坐标标签斜起来
correlation中间text用这两个参数即可
cex.labels=0.8, font.labels=4,可以直接用
 #panel.hist <- function(x, ...)
 36 #{
 37 #usr <- par("usr"); on.exit(par(usr))
 38 #par(usr = c(usr[1:2], 0, 4) )
 39 #h <- hist(x, plot = FALSE)
 40 #breaks <- h$breaks; nB <- length(breaks)
 41 #y <- h$counts; y <- y/max(y)
 42 #rect(breaks[-nB], 0, breaks[-1], y, col="cyan", ...)
 43 #}
 
 panel.boxplot        More univariate panel plots可以直接加入在pairs()里面来画 boxplot 
panel.boxplot        More univariate panel plots
panel.cor        More panel plots
panel.density        More univariate panel plots
panel.ellipse        More panel plots
panel.hist        More univariate panel plots
panel.qqnorm        More univariate panel plots
下面这个图是拿来作直方图的
    panel.hist <- function(x, ...)
    {
        usr <- par("usr"); on.exit(par(usr))
        par(usr = c(usr[1:2], 0, 1.5) )
        h <- hist(x, plot = FALSE)
        breaks <- h$breaks; nB <- length(breaks)
        y <- h$counts; y <- y/max(y)
        rect(breaks[-nB], 0, breaks[-1], y, col="cyan", ...)
    }
    pairs(USJudgeRatings[1:5], panel=panel.smooth,
          cex = 1.5, pch = 24, bg="light blue",
          diag.panel=panel.hist, cex.labels = 2, font.labels=2)
          
还是得这样才能改掉想线图的颜色
airs(iris[,1:4], upper.panel = panel.cor, pch = 21, bg = c("lightblue"))

 
text(1:6, par("usr")[3]-0.25, srt =30, adj = 1,labels = labels, xpd = TRUE)
王会丽cor invivo in vitro normalization(10000000*hits/total of reads)

fastqc -o /Share/home/wangdong/lss/project/20150402rasl_sw_again_why_hepg2/fastqc/ R3_2015_03_25.fastq R3-mops_AGTGCCC.fastq mops_R1.fastq mops_C3.fastq rasl_cell.fastq rasl_ha.fastq rasl_hg2_15.fastq rasl_nc_negative.fastq rasl_nc_positive.fastq rasl_rna.fastq rasl_shrna2.fastq rasl_shrna3.fastq

> mydata  <-read.table("cor_whl.txt",head=T)
> cor(mydata)
              C1         C2         C3          C4         C5       EXP1
C1   1.000000000 0.03754071 0.01653239 0.038140525 0.03209038 0.03957553
C2   0.037540715 1.00000000 0.03471711 0.012206128 0.02818689 0.05267082
C3   0.016532385 0.03471711 1.00000000 0.009407780 0.01378644 0.06162066
C4   0.038140525 0.01220613 0.00940778 1.000000000 0.35795586 0.01885744
C5   0.032090377 0.02818689 0.01378644 0.357955856 1.00000000 0.03392869
EXP1 0.039575535 0.05267082 0.06162066 0.018857443 0.03392869 1.00000000
EXP2 0.021015294 0.03108098 0.04494751 0.018359859 0.02638896 0.04890143
EXP3 0.098674955 0.13395507 0.03435392 0.015572696 0.02308882 0.06746699
EXP4 0.050923726 0.05901322 0.03716548 0.018256494 0.01982727 0.06727874
EXP5 0.006787069 0.01166773 0.00752868 0.003704685 0.01292783 0.08213378
BI   0.013649467 0.02008112 0.01501878 0.009698009 0.00888524 0.03777409
           EXP2        EXP3       EXP4        EXP5          BI
C1   0.02101529 0.098674955 0.05092373 0.006787069 0.013649467
C2   0.03108098 0.133955066 0.05901322 0.011667734 0.020081123
C3   0.04494751 0.034353924 0.03716548 0.007528680 0.015018776
C4   0.01835986 0.015572696 0.01825649 0.003704685 0.009698009
C5   0.02638896 0.023088824 0.01982727 0.012927833 0.008885240
EXP1 0.04890143 0.067466987 0.06727874 0.082133776 0.037774089
EXP2 1.00000000 0.149879010 0.04363418 0.038647002 0.019827310
EXP3 0.14987901 1.000000000 0.06573363 0.009921881 0.016585631
EXP4 0.04363418 0.065733633 1.00000000 0.051256056 0.026198685
EXP5 0.03864700 0.009921881 0.05125606 1.000000000 0.010872146
BI   0.01982731 0.016585631 0.02619869 0.010872146 1.000000000


Ultraedit mac版本破解方法
下载软件地址
http://www.bubuko.com/infodetail-581168.html
1. 首先下载安装文件：下载，提取码：b003

2. 网盘中两个文件下载完以后，安装.dmg文件，安装完后先别打开软件，用PATCH那个压缩包解压出来的文件替换掉 /Applications/UltraEdit.app/Contents/MacOS下同名文件 即可


 a=read.csv("above10.csv",head=T,sep=";")
> b=read.csv("below10.csv",head=T,sep=";")
> pdf("boxplot_probe_above10.pdf")
> boxplot(a$array,b$array,col=c("pink"),ylab="The micrparray indensity of genes",las=1,font.lab=2)
>stripchart()#增加上面的点，另外读入数据
stripchart(B,vertical=TRUE,method="jitter",pch=20,cex=0.01,col=c("grey"),bg="bisque",add=TRUE)

 dev.off()
null device
          1
> a=read.csv("above10_log.csv",head=T,sep=";")
> b=read.csv("below10_log.csv",head=T,sep=";")
> pdf("boxplot_probe_above10_log10.pdf")
> boxplot(a$array,b$array,col=c("pink"),ylab="The micrparray indensity of genes",las=1,font.lab=2)
> dev.off()
null device
          1
> pdf("boxplot_probe_above10_log10_below.pdf")
> boxplot(b$array,col=c("pink"),ylab="The micrparray indensity of genes",las=1,font.lab=2)
> dev.off()
null device
          1
> pdf("boxplot_probe_above10_log10_above.pdf")
> boxplot(a$array,col=c("pink"),ylab="The micrparray indensity of genes",las=1,font.lab=2)
Error in boxplot.default(a$array, col = c("pink"), ylab = "The micrparray indensity of genes",  :
  adding class "factor" to an invalid object
> q()
a=read.csv("25minus.csv",head=T,sep=";")
b=read.csv("25_10.csv",head=T,sep=";")
c=read.csv("below10minus.csv",head=T,sep=";")
 #抬头开始不能有数字
finaldata<-data.frame(a$above_25_a,a$above_25_d,b$from_10_25_a,b$from_10t_25_d,c$below_10_a,c$below_10_d)
 pdf("boxplot_probe_minus3.pdf")
 boxplot(a$Tm_acceptor_minus_Tm_donor,b$Tm_acceptor_minus_Tm_donor,c$Tm_acceptor_minus_Tm_donor,col=c("pink"),ylab="The Dvalue of Acceptor Tm and donor Tm",las=1,font.lab=2)
 boxplot(b$from_10_25_a,b$from_10t_25_d,col=c("pink"),ylab="normalized Indensity Values",las=1,font.lab=2)
 pdf("boxplot_probe_below10.pdf")
  boxplot(c$below_10_a,c$below_10_d,col=c("yellow"),ylab="normalized Indensity Values",las=1,font.lab=2)
above_25_a	above_25_d	10to25_a	10to25_d	below_10_a	below_10_d

#制作comvert.sh，qsub提交即可
configureBclToFastq.pl --input-dir /Share/home/wangdong/data/rawdata/150401_C00126_0182_AHGT55ADXX/Data/Intensities/BaseCalls --output ./Unaligned --sample-sheet /Share/home/wangdong/data/rawdata/150401_C00126_0182_AHGT55ADXX/SampleSheet_undetermined.csv --no-eamss
cd /Share/home/wangdong/data/rawdata/150401_C00126_0182_AHGT55ADXX/Unaligned
nohup make -j 8


corRaw <- cor(Raw)
library(spatstat) # "im" function
plot(im(corRaw[nrow(corRaw):1,]), main="Correlation Matrix Map")
#Here's the R code that creates a distance matrix from 1-Correlation as the dissimilarity index:
#Several methods of "dissimilarity" were explored to see if one method is "better" in some way over the other methods:

    Dissimilarity = 1 - Correlation
    Dissimilarity = (1 - Correlation)/2
    Dissimilarity = 1 - Abs(Correlation)
    Dissimilarity = Sqrt(1 - Correlation2)

dissimilarity <- 1 - cor(Raw)
distance <- as.dist(dissimilarity)
#Note the as.dist function is used here to assign the correlation values to be "distances".  (In some cases, you may want to use the dist function to compute distances using a variety of distance metrics instead.) 

#The hierarchical clustering function, hclust, expects a dissimilarity matrix.  The plot function knows how to plot a dendrogram from hclust's result

    plot(hclust(distance),
         main="Dissimilarity = 1 - Correlation", xlab="")


转换成dendrogram可以有更多选项
    > hc = hclust(dist(mtcars))
    > hcd = as.dendrogram(hc)
    > plot(hcd)
    > plot(cut(hcd, h=75)$lower[ [2] ])
    
    
    
    
corrgram

mydata  <-read.table("test.txt",head=T)
cor(mydata)
library(corrgram)
> mydata
     a    b    c    d     e
a 1.00 0.85 0.85 0.70 0.681
b 0.85 1.00 0.90 0.83 0.700
c 0.85 0.90 1.00 0.79 0.710
d 0.78 0.83 0.79 1.00 0.449
e 0.68 0.70 0.71 0.45 1.000

> corrgram(mydata, order=TRUE, lower.panel=panel.shade,upper.panel=NULL)
> dev.off()
null device
          1
> pdf("test.pdf")
> corrgram(mydata, order=TRUE, lower.panel=panel.shade,up
update                update.formula        update.packageStatus  upper.tri
update.default        update.packages       upgrade               upper.panel=
> corrgram(mydata, order=TRUE, lower.panel=panel.shade,upper.panel=p
Display all 184 possibilities? (y or n)
> corrgram(mydata, order=TRUE, lower.panel=panel.shade,upper.panel=panel.pie)
> dev.off()
null device
          1
> pdf("test2.pdf")
> corrgram(mydata, order=TRUE, lower.panel=panel.shade,upper.panel=panel.shade)
> dev.off()
null device
          1
> pdf("test3.pdf")
> corrgram(mydata, order=TRUE, lower.panel=panel.shade,upper.panel=panel.pie,text.panel=NULL)
> dev.off()
null device

# Changing Colors in a Correlogram
library(corrgram)
col.corrgram <- function(ncol){   
  colorRampPalette(c("darkgoldenrod4", "burlywood1",
  "darkkhaki", "darkgreen"))(ncol)}
corrgram(mtcars, order=TRUE, lower.panel=panel.shade,
   upper.panel=panel.pie, text.panel=panel.txt,
   main="Correlogram of Car Mileage Data (PC2/PC1 Order)")
   

 #R#barplot
分类： R 2013-01-30 09:30 490人阅读 评论(0) 收藏 举报
 （1）建一个测试的DATASET：
> a<-seq(2,10,2)
> b<-seq(10,100,20)
> c<-as.matrix(a,b)
> c
     [,1]
[1,]    2
[2,]    4
[3,]    6
[4,]    8
[5,]   10
> c<-data.frame(a,b)
> c<-as.matrix(c)
> c
      a  b
[1,]  2 10
[2,]  4 30
[3,]  6 50
[4,]  8 70
[5,] 10 90

（2）根据测试DATASET，来画图
>barplot(c,main="Test",xlab="Product",ylab="xiaohao",col=c("red","blue","yellow","black","green"),legend=rownames(c))



韦恩图自己成功运行
install.packages("VennDiagram")
library(VennDiagram)
library(grid)
mydata1<-read.table("/Users/lishasha/Desktop/exp1_2oligos.txt",head=T)
mydata2<-read.table("/Users/lishasha/Desktop/exp2_2oligos.txt",head=T)
mydata3<-read.table("/Users/lishasha/Desktop/exp3_2oligos.txt",head=T)
mydata4<-read.table("/Users/lishasha/Desktop/exp4_2oligos.txt",head=T)
mydata5<-read.table("/Users/lishasha/Desktop/exp5_2oligos.txt",head=T)
venn.diagram(list("exp1"=mydata1$exp1,"exp2"=mydata2$exp2,"exp3"=mydata3$exp3,"exp4"=mydata4$exp4,"exp5"=mydata5$exp5),fill=c("red","blue","yellow","green","orange"),"/Users/lishasha/Desktop/OVERLAP3.png",main="overlap gene in 5 experiment group",main.col = "black", main.cex =1)


临客圈传脚本


library(VennDiagram)
A<-c("a","b")
B<-c("b","c")
C<-c("c","d")
D<-c("d","e")
E<-c("e","a")
venn.diagram(list("lncapablonly"=A,"lncaponly"=B,"C"=C,"D"=D),fill=c("red","blue","yellow","green"),"G:/test.png",main="H3K27Ac Peak Numbers",main.col = "black", main.cex =3)




网上下载一些其他相关韦恩图
R中提供了多个可用于绘制韦恩图的软件包，本文主要是介绍的是VennDiagram包。
安装VennDiagram包：
install.packages("VennDiagram")
首先加载相应的软件包：
library(VennDiagram)
生成几个集合并计算各个集合及其相互交集的大小：
A = 1:150
B = c(121:170,300:320)
C = c(20:40,141:200)
Length_A<-length(A)
Length_B<-length(B)
Length_C<-length(C)
Length_AB<-length(intersect(A,B))
Length_BC<-length(intersect(B,C))
Length_AC<-length(intersect(A,C))
Length_ABC<-length(intersect(intersect(A,B),C))
利用通用函数venn.diagram绘制两个集合的韦恩图：
T<-venn.diagram(list(A=A,B=B),filename=NULL
,lwd=1,lty=2
,col=c("red","green"),fill=c("red","green")
,cat.col=c("red","green")
,rotation.degree=90)
grid.draw(T)
其中，参数filename指定用于保存图形文件的文件名，如果希望在当前的图形窗口中看到绘制的韦恩图，则filename必须为空；若希望将绘制的图 形直接保存为某文件，则直接使用venn.diagram(...,filename="*")即可完成。参数fill表示各个集合对应的圆的填充颜 色,col表示对应的圆周的颜色，而cat.col则表示集合名称的显示颜色。lwd用于设定圆弧的宽度，lty用于设定圆弧的线型。参数 rotation.degree则可用于调整图形的旋转角度。
利用函数venn.diagram绘制三个集合的韦恩图：
T<-venn.diagram(list(A=A,B=B,C=C),filename=NULL
,lwd=1,lty=2,col=c("red","green","blue")
,fill=c("red","green","blue")
,cat.col=c("red","green","blue")
,reverse=TRUE)
grid.draw(T)
从上面的两个例子可以看出函数venn.diagram是利用集合作为参数绘制韦恩图的，但是有时候我们并不知道各个集合都包含什么元素，而只知道集合及 相互之间交集的大小，这个时候如何绘制韦恩图呢？包VennDiagram还给我们提供了另外几个函数：绘制两个集合的韦恩图的 draw.pairwise.venn，三个集合的draw.triple.venn，四个、五个集合的draw.quad.venn、 draw.quintuple.venn。我们此处只介绍前两个函数的用法。
利用函数draw.pairwise.venn绘制两个集合的韦恩图：
draw.pairwise.venn(area1=Length_A,area2=Length_B,cross.area=Length_AB
,category=c("A","B"),lwd=rep(1,1),lty=rep(2,2)
,col=c("red","green"),fill=c("red","green")
,cat.col=c("red","green")
,rotation.degree=90)
其中area1指第一个集合的大小，area2指第二个集合的大小，而cross.area则指交集的大小。参数category用于指定集合名称。其余参数与venn.diagram相同。
利用函数draw.triple.venn绘制三个集合的韦恩图：
draw.triple.venn(area1=Length_A, area2=Length_B, area3=Length_C
,n12=Length_AB, n23=Length_BC, n13=Length_AC, n123=Length_ABC
,category = c("A","B","C")
,col=c("red","green","blue"),fill=c("red","green","blue")
,cat.col=c("red","green","blue")
,reverse = FALSE)
同draw.pairwise.venn类似，area1、area2、area3分别指第一个、第二个、第三个集合的大小。n12表示第一个与第二个集合的交集大小，n23、n13也是类似，n123指三个集合的交集大小。reverse则指是否对图形进行反转。


重新生成将rpkm根据不同转录本合成一个基因
analyzeRepeats.pl rna hg19 -strand both -condenseGenes -count exons -d /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/c2_2/ /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/c3_2/ /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/nc_2/ -rpkm > condensed_rpkm.txt
[wangdong@cluster ~/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/mktags]$analyzeRepeats.pl rna hg19 -strand both -condenseGenes -count exons -d /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/c2_2/ /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/c3_2/ /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/nc_2/ -rpkm > condensed_rpkm.txt
        Tag Directories:
                /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/c2_2/
                /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/c3_2/
                /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/nc_2/
        Input file format: homerRmsk
        Filtering based on repeat parameters: kept 51794 of 51794
        Calculating read coverage for /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/c2_2/
        Calculating read coverage for /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/c3_2/
        Calculating read coverage for /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/nc_2/
                Normalizing...
                Normalizing...
                Normalizing...
        Printing output
                Printed 25690 of 48118 repeats (expression >= -1e+20)
                
google 小丽给网址


http://173.194.121.48/

1#trim_reads to 22bp of antisense

fastx_trimmer -f 25 -l 46 -Q 33 -i before_injection.fastq -o bi_antisense.fastq
fastx_trimmer -f 25 -l 46 -Q 33 -i c1.fastq -o c1_antisense.fastq
fastx_trimmer -f 25 -l 46 -Q 33 -i c2.fastq -o c2_antisense.fastq
fastx_trimmer -f 25 -l 46 -Q 33 -i c3.fastq -o c3_antisense.fastq
fastx_trimmer -f 25 -l 46 -Q 33 -i c4.fastq -o c4_antisense.fastq
fastx_trimmer -f 25 -l 46 -Q 33 -i c5.fastq -o c5_antisense.fastq
fastx_trimmer -f 25 -l 46 -Q 33 -i d0.fastq -o d0_antisense.fastq
fastx_trimmer -f 25 -l 46 -Q 33 -i d10_negative.fastq -o d10plus_antisense.fastq
fastx_trimmer -f 25 -l 46 -Q 33 -i d10_positive.fastq -o d10minor_antisense.fastq
fastx_trimmer -f 25 -l 46 -Q 33 -i d2_negative.fastq -o d2plus_antisense.fastq
fastx_trimmer -f 25 -l 46 -Q 33 -i d2_positive.fastq -o d2minor_antisense.fastq
fastx_trimmer -f 25 -l 46 -Q 33 -i Exp1.fastq -o Exp1_antisense.fastq
fastx_trimmer -f 25 -l 46 -Q 33 -i Exp2.fastq -o Exp2_antisense.fastq
fastx_trimmer -f 25 -l 46 -Q 33 -i Exp3.fastq -o Exp3_antisense.fastq
fastx_trimmer -f 25 -l 46 -Q 33 -i Exp4.fastq -o Exp4_antisense.fastq
fastx_trimmer -f 25 -l 46 -Q 33 -i Exp5.fastq -o Exp5_antisense.fastq

2#prepare ref sequence big pool
bowtie2-build -f ref_22_antisense.fasta ref_22_antisense
ref_51.fasta
bowtie2-build -f ref_51.fasta ref_51_antisense
3#mapping(bowtie2),count(python)
bowtie2 -x /Share/home/wangdong/lss/project/whl_invivo_screen_big_pool/ref_51/ref_51_antisense /Share/home/wangdong/lss/project/whl_invivo_screen_big_pool/bi_antisense.fastq -S /Share/home/wangdong/lss/project/whl_invivo_screen_big_pool/ref_51/bi_51.sam
f

bowtie2-build -f ref.fasta ref_whl
bowtie2 -N 3 -x ref_whl WHL_C1_R1_antisense.fastq -S C1_3.sam
fastx_trimmer -f 25 -l 46 -Q 33 -i WHL_C1_R1.fastq -o WHL_C1_R1_antisense.fastq
bedtools安装




pheatmap用于作heatmap用
~/lkq/projects/cluster_for_whl/pheatmap

boxplot

a=read.csv("probe_plot.csv",head=T,sep=";")
finaldata<-data.frame(a$X1N,a$X2N,a$X3N,a$X4N,a$X5N,a$X8N,a$X9N,a$X10N,a$X11N,a$X12N,a$X13N,a$X15N,a$X16N,a$X17N,a$X1T,a$X2T,a$X3T,a$X4T,a$X5T,a$X8T,a$X9T,a$X10T,a$X11T,a$X12T,a$X13T,a$X15T,a$X16T,a$X17T)
 pdf("boxplot_mRNA_28.pdf")
 boxplot(finaldata,col=c("mediumturquoise"),ylab="normalized Indensity Values",las=1,font.lab=2)
 


maping 用的～／lkq/script/produce.pls
bin/bash\n
/Share/home/wangdong/local/bin/tophat -p 8 -G /Share/home/wangdong/data/human_annotation/gencode19.lite.gtf -o $out/$sample --library-type=fr-unstranded /Share/home/lulab1/users/liyang/project/Genome/human_hg19/bowtie2_index/human_genome_hg19 $_

bamToBed -i accepted_hits.bam >> c2.bed
bamToBed -i /Share/home/wangdong/lss/sw_ctnnb_RNASeq_reanalysis/mapping/SWC3_1_t/accepted_hits.bam >> /Share/home/wangdong/lss/sw_ctnnb_RNASeq_reanalysis/mapping/SWC3_1_t/c3.bed
bamToBed -i /Share/home/wangdong/lss/sw_ctnnb_RNASeq_reanalysis/mapping/SWNC_1_t/accepted_hits.bam >> /Share/home/wangdong/lss/sw_ctnnb_RNASeq_reanalysis/mapping/SWNC_1_t/NC.bed


makeTagDirectory /Share/home/wangdong/lss/sw_ctnnb_RNASeq_reanalysis/maketags/c2_2/ /Share/home/wangdong/lss/sw_ctnnb_RNASeq_reanalysis/mapping/SWC2_1_t/c2.bed -genome /Share/home/lulab1/users/liyang/project/Genome/human_hg19/bowtie2_index/human_genome_hg19.fa -sspe
makeTagDirectory /Share/home/wangdong/lss/sw_ctnnb_RNASeq_reanalysis/maketags/c3_2/ /Share/home/wangdong/lss/sw_ctnnb_RNASeq_reanalysis/mapping/SWC3_1_t/c3.bed -genome /Share/home/lulab1/users/liyang/project/Genome/human_hg19/bowtie2_index/human_genome_hg19.fa -sspe
makeTagDirectory /Share/home/wangdong/lss/sw_ctnnb_RNASeq_reanalysis/maketags/nc_2/ /Share/home/wangdong/lss/sw_ctnnb_RNASeq_reanalysis/mapping/SWNC_1_t/NC.bed -genome /Share/home/lulab1/users/liyang/project/Genome/human_hg19/bowtie2_index/human_genome_hg19.fa -sspe

removeOutOfBoundsReads.pl ~/lss/sw_ctnnb_RNASeq_reanalysis/maketags/c2/ /Share/home/lulab1/users/liyang/project/Genome/human_hg19/bowtie2_index/human_genome_hg19.fa -chromSizes /Share/home/wangdong/data/human/hg19.chrom.sizes
removeOutOfBoundsReads.pl ~/lss/sw_ctnnb_RNASeq_reanalysis/maketags/c3/ /Share/home/lulab1/users/liyang/project/Genome/human_hg19/bowtie2_index/human_genome_hg19.fa -chromSizes /Share/home/wangdong/data/human/hg19.chrom.sizes
removeOutOfBoundsReads.pl ~/lss/sw_ctnnb_RNASeq_reanalysis/maketags/nc/ /Share/home/lulab1/users/liyang/project/Genome/human_hg19/bowtie2_index/human_genome_hg19.fa -chromSizes /Share/home/wangdong/data/human/hg19.chrom.sizes


makeUCSCfile /Work1/home/wangdong/lss/data/GQ_CHIP/maketags_H3K4ME1/ -o auto -bigWig /Work1/home/wangdong/ensemble/hg19.chrom.sizes -norm 1e7 -strand both -fsize 1e20 > /Work1/home/wangdong/lss/data/GQ_CHIP/maketags_H3K4ME1/tags.trackInfo2.txt
makeUCSCfile /Work1/home/wangdong/lss/data/GQ_CHIP/maketags_h3k4me2/ -o auto -bigWig /Work1/home/wangdong/ensemble/hg19.chrom.sizes -norm 1e7 -strand both -fsize 1e20 > /Work1/home/wangdong/lss/data/GQ_CHIP/maketags_h3k4me2/tags.trackInfo2.txt
makeUCSCfile /Work1/home/wangdong/lss/data/GQ_CHIP/maketags_H3K4ME1/ -o auto -bigWig /Work1/home/wangdong/ensemble/hg19.chrom.sizes -norm 1e7 -strand both -fsize 1e20 > /Work1/home/wangdong/lss/data/GQ_CHIP/maketags_H3K4ME1/tags.trackInfo2.txt

analyzeRepeats.pl rna /Share/home/lulab1/users/liyang/project/Genome/human_hg19/bowtie2_index/human_genome_hg19.fa -strand both -condenseGenes -d ./c2 ./c3 ./nc -rpkm > ./shRNA1_shRNA2_nc_rpkm.txt

报错
makeUCSCfile /Share/home/wangdong/lss/sw_ctnnb_RNASeq_reanalysis/maketags/c2_2/ -fragLength given -o auto
makeUCSCfile /Share/home/wangdong/lss/sw_ctnnb_RNASeq_reanalysis/maketags/c3_2/ -fragLength given -o auto
makeUCSCfile /Share/home/wangdong/lss/sw_ctnnb_RNASeq_reanalysis/maketags/nc_2/ -fragLength given -o auto     
      
analyzeRepeats.pl rna hg19 -strand both -count exons -d /Share/home/wangdong/lss/sw_ctnnb_RNASeq_reanalysis/maketags/c2_2/ /Share/home/wangdong/lss/sw_ctnnb_RNASeq_reanalysis/maketags/c3_2/ /Share/home/wangdong/lss/sw_ctnnb_RNASeq_reanalysis/maketags/nc_2/ -rpkm > rpkm.txt

analyzeRepeats.pl rna hg19 -strand both -count exons -d /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/c2_2/ /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/c3_2/ /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/nc_2/ -rpkm > rpkm.txt
新文件夹中再运行
[wangdong@cluster ~/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/mktags]$analyzeRepeats.pl rna hg19 -strand both -count exons -d /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/c2_2/ /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/c3_2/ /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/nc_2/ -rpkm > rpkm.txt
        Tag Directories:
                /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/c2_2/
                /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/c3_2/
                /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/nc_2/
        Input file format: homerRmsk
        Filtering based on repeat parameters: kept 51794 of 51794
        Calculating read coverage for /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/c2_2/
        Calculating read coverage for /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/c3_2/
        Calculating read coverage for /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/nc_2/
                Normalizing...
                Normalizing...
                Normalizing...
        Printing output
                Printed 48118 of 48118 repeats (expression >= -1e+20)

analyzeRepeats.pl rna hg19 -strand both -count exons -d /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/c2_2/ /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/nc_2/ -rpkm > shRNA1_NC_rpkm.txt
[wangdong@cluster ~/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/mktags]$analyzeRepeats.pl rna hg19 -strand both -count exons -d /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/c2_2/ /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/nc_2/ -rpkm > shRNA1_NC_rpkm.txt
        Tag Directories:
                /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/c2_2/
                /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/nc_2/
        Input file format: homerRmsk
        Filtering based on repeat parameters: kept 51794 of 51794
        Calculating read coverage for /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/c2_2/
        Calculating read coverage for /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/nc_2/
                Normalizing...
                Normalizing...
        Printing output
                Printed 48118 of 48118 repeats (expression >= -1e+20)
分开做结果和三个合在一起一样
analyzeRepeats.pl rna hg19 -strand both -count exons -d /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/c3_2/ /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/nc_2/ -rpkm > shRNA2_NC_rpkm.txt
[wangdong@cluster ~/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/mktags]$analyzeRepeats.pl rna hg19 -strand both -count exons -d /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/c3_2/ /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/nc_2/ -rpkm > shRNA2_NC_rpkm.txt
        Tag Directories:
                /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/c3_2/
                /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/nc_2/
        Input file format: homerRmsk
        Filtering based on repeat parameters: kept 51794 of 51794
        Calculating read coverage for /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/c3_2/
        Calculating read coverage for /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/nc_2/
                Normalizing...
                Normalizing...
        Printing output
                Printed 48118 of 48118 repeats (expression >= -1e+20)

 Tag Directories:
                /Share/home/wangdong/lss/sw_ctnnb_RNASeq_reanalysis/maketags/c2_2/
                /Share/home/wangdong/lss/sw_ctnnb_RNASeq_reanalysis/maketags/c3_2/
                /Share/home/wangdong/lss/sw_ctnnb_RNASeq_reanalysis/maketags/nc_2/
        Input file format: homerRmsk
        Filtering based on repeat parameters: kept 51794 of 51794
        Calculating read coverage for /Share/home/wangdong/lss/sw_ctnnb_RNASeq_reanalysis/maketags/c2_2/
        Calculating read coverage for /Share/home/wangdong/lss/sw_ctnnb_RNASeq_reanalysis/maketags/c3_2/
        Calculating read coverage for /Share/home/wangdong/lss/sw_ctnnb_RNASeq_reanalysis/maketags/nc_2/
                Normalizing...
                Normalizing...
                Normalizing...
        Printing output
                Printed 48118 of 48118 repeats (expression >= -1e+20)
                
                
analyzeRepeats.pl rna hg19 -strand both -count exons -d Exp1r1/ Exp1r2 Exp2r1/ Exp2r2/ -rpkm > rpkm.txt
analyzeRepeats.pl rna hg19 -strand both -count exons -d Exp1r1/ Exp1r2 Exp2r1/ Exp2r2/ -rpkm > rpkm.txt      

4. Quantify gene expression as integer counts for differential expression (-noadj)
# May also wish to use "-condenseGenes" if you don't want multiple isoforms per gene
analyzeRepeats.pl rna hg19 -strand both -count exons -condenseGenes -d /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/c2_2/ /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/c3_2/ /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/nc_2/ -noadj > shRNA1_shRNA2_NC_raw.txt
[wangdong@cluster ~/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/mktags]$analyzeRepeats.pl rna hg19 -strand both -count exons -condenseGenes -d /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/c2_2/ /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/c3_2/ /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/nc_2/ -noadj > shRNA1_shRNA2_NC_raw.txt
        Tag Directories:
                /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/c2_2/
                /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/c3_2/
                /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/nc_2/
        Input file format: homerRmsk
        Filtering based on repeat parameters: kept 51794 of 51794
        Calculating read coverage for /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/c2_2/
        Calculating read coverage for /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/c3_2/
        Calculating read coverage for /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/nc_2/
        Printing output
                Printed 25690 of 48118 repeats (expression >= -1e+20)  #由于加了-condenseGenes，所以只输出25690
                
analyzeRepeats.pl rna hg19 -strand both -count exons -condenseGenes -d /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/c2_2/ /Share/home/wangdong/lss/project/sw_ctnnb_RNASeq_reanalysis/maketags/nc_2/ -noadj > shRNA1_NC_raw.txt


Calculate differential expression (requires R/Bioconductor and the package edgeR to be installed!!)
# Name the samples in the same order as they were used in the analyzeRepeats.pl command.
# Replicates should have the same name (i.e. cond1)
getDiffExpression.pl raw.txt cond1 cond1 cond2 cond2 > diffExp.output.txt

getDiffExpression.pl shRNA1_shRNA2_NC_raw.txt c2_2 c3_2 nc_2 > shRNA1_shRNA2_NC_diffExp.txt
getDiffExpression.pl shRNA1_NC_raw.txt c2_2 nc_2 > shRNA1_NC_diffExp.txt

getDiffExpression.pl shRNA2_NC_raw.txt c3_2 nc_2 > shRNA2_NC_diffExp.txt

getDiffExpression.pl shRNA1_shRNA2_NC_raw.txt shRNA shRNA NC -repeats > shRNA1_shRNA2_NC_diffOutput.txt



# Check out the results in Excel - there will be 4 columns per comparison: log2 fold, log2 intensity cpm, p-value, and FDR/q-value.


下面是临客圈脚本里关于这几步，主要是用里面的参考序列和注释文件
maketags
Share/home/wangdong/packages/homer/bin/makeTagDirectory $out/$sample $_ -unique -genome /Share/home/lulab1/users/liyang/project/Genome/human_hg19/bowtie2_index/human_genome_hg19.fa -checkGC
makeucscfiles
/Share/home/wangdong/packages/homer/bin/removeOutOfBoundsReads.pl $_ /Share/home/wangdong/data/human/human_genome_hg19.fa -chromSizes /Share/home/wangdong/data/human/hg19.chrom.sizes &&\n/Share/home/wangdong/packages/homer/bin/makeUCSCfile $_ -o auto -bigWig /Share/home/wangdong/data/human/hg19.chrom.sizes -strand both -fsize 1e20 >$out/$sample/$sample.trackInfo.txt

孟凡琳的homer rna-seq脚本
1、Make tag directories for each experiment
cd /Share/home/wangdong/mfl/result/OvCa_result/2HOMER
makeTagDirectory /Share/home/wangdong/mfl/result/OvCa_result/2HOMER/OvCa_1T /Share/home/wangdong/mfl/result/OvCa_result/2HOMER/Tophat_accepted_hits/Ovarian_1T.bam -sspe
makeTagDirectory /Share/home/wangdong/mfl/result/OvCa_result/2HOMER/OvCa_1M /Share/home/wangdong/mfl/result/OvCa_result/2HOMER/Tophat_accepted_hits/Ovarian_1M.bam -sspe

makeTagDirectory /Share/home/wangdong/mfl/result/OvCa_result/2HOMER/OvCa_3T /Share/home/wangdong/mfl/result/OvCa_result/2HOMER/Tophat_accepted_hits/Ovarian_3T.bam -sspe
makeTagDirectory /Share/home/wangdong/mfl/result/OvCa_result/2HOMER/OvCa_3M /Share/home/wangdong/mfl/result/OvCa_result/2HOMER/Tophat_accepted_hits/Ovarian_3M.bam -sspe

makeTagDirectory /Share/home/wangdong/mfl/result/OvCa_result/2HOMER/OvCa_4T /Share/home/wangdong/mfl/result/OvCa_result/2HOMER/Tophat_accepted_hits/Ovarian_4T.bam -sspe
makeTagDirectory /Share/home/wangdong/mfl/result/OvCa_result/2HOMER/OvCa_4M /Share/home/wangdong/mfl/result/OvCa_result/2HOMER/Tophat_accepted_hits/Ovarian_4M.bam -sspe

2、Make bedGraph visualization files for each tag directory

4、Quantify gene expression as integer counts for differential expression (-noadj)
analyzeRepeats.pl rna hg19 -count exons -d ./OvCa_1T ./OvCa_1M -noadj > OvCa_1T_1M_count.txt
analyzeRepeats.pl rna hg19 -count exons -d ./OvCa_3T ./OvCa_3M -noadj > OvCa_3T_3M_count.txt
analyzeRepeats.pl rna hg19 -count exons -d ./OvCa_4T ./OvCa_4M -noadj > OvCa_4T_4M_count.txt


5、Calculate differential expression (requires R/Bioconductor and the package edgeR to be installed!!)
getDiffExpression.pl OvCa_1T_1M_count.txt tumor metastasis > OvCa_1T_1M.diffExp_count.txt
getDiffExpression.pl OvCa_3T_3M_count.txt tumor metastasis > OvCa_3T_3M.diffExp_count.txt
getDiffExpression.pl OvCa_4T_4M_count.txt tumor metastasis > OvCa_4T_4M.diffExp_count.txt



configureBclToFastq.pl --input-dir /Share/home/wangdong/data/rawdata/150315_C00126_0172_AHFNJ5ADXX/Data/Intensities/BaseCalls --output ./Unaligned2 --sample-sheet ./SampleSheet_undetermined.csv --no-eamss


我觉得可能可以了，你试一下，把这个命令写在一个bash脚本里，使用绝对路径，然后qsub这个脚本
qsub -V foo.sh -o foo.log
LKQ  服务器所在位置
/Share/home/wangdong/lkq/Rscipt/htmap/heatmapV5.1.r

configureBclToFastq.pl --input-dir /Share/home/wangdong/data/rawdata/150311_C00126_0168_AHFNLMADXX/Data/Intensities/BaseCalls --output ./Unaligned2 --sample-sheet ./SampleSheet_undetermined.csv --no-eamss
换成苹果得到的sample sheet由于分隔符不同而报错

configureBclToFastq.pl --input-dir /Share/home/wangdong/data/rawdata/150210_C00126_0166_AC6EYWANXX/Data/Intensities/BaseCalls --output ./Unaligned3_undetermined --sample-sheet ./SampleSheet_undetermined.csv --no-eamss

 最终调节版，数据自己变过后，里面就不用再取了
library(ggplot2)
tiff("new5.tiff")
a<- read.csv("mRNA_right.csv",head=TRUE,sep=";")
P.Value <- c(a$PValue)
FC <- c(a$FC)
df <- data.frame(P.Value, FC)
df$threshold = as.factor(((df$FC > 1) |(df$FC < -1) ) & df$P.Value > 1.30103)
g = ggplot(data=df, aes(x=FC, y=P.Value, colour=threshold)) +
 geom_point(alpha=0.4, size=1.75) +
  theme(legend.position = "none") +  
  xlim(c(-5, 5)) + ylim(c(0, 5)) +
  xlab("log2 fold change") + ylab("-log10 p-value")
g
dev.off()

library(ggplot2)
tiff("new2.tiff")
a<- read.csv("mRNA2.csv",head=TRUE,sep=";")
P.Value <- c(a$PValue)
FC <- c(a$FC)
df <- data.frame(P.Value, FC)
df$threshold = as.factor(abs(df$FC) > 1 & df$P.Value < 0.05) 这几部数据必须行数一致，检查一下空格，以及是不是P.Value，FC均有，不能在抬头里面有P_Value 中文下划线
 
##Construct the plot object
g = ggplot(data=df, aes(x=FC, y=-log10(P.Value), colour=threshold)) +
 geom_point(alpha=0.4, size=1.75) +
  theme(legend.position = "none") +        #错误: Use 'theme' instead. (Defunct; last used in version 0.9.1), 将原来的opts换成theme
  xlim(c(-5, 5)) + ylim(c(0, 5)) +
  xlab("log2 fold change") + ylab("-log10 p-value")
g
dev.off()

 Mac其实所有移动硬盘都在/Volumes/目录中
$cd /
$cd /Volumes

 R画volcano plot from baidu
分类： R 2013-03-22 23:10 1461人阅读 评论(0) 收藏 举报
require(ggplot2)
png("")
##Highlight genes that have an absolute fold change > 2 and a p-value < Bonferroni cut-off
a <- read.table("",header=TRUE,sep="\t",)
P.Value <- c(a$R画volcano plot)
FC <- c(a$FC)
df <- data.frame(P.Value, FC)
df$threshold = as.factor(abs(df$FC) > 1 & df$P.Value < 0.05)
 
##Construct the plot object
g = ggplot(data=df, aes(x=FC, y=-log10(P.Value), colour=threshold)) +
 geom_point(alpha=0.4, size=1.75) +
  opts(legend.position = "none") +
  xlim(c(-5, 5)) + ylim(c(0, 5)) +
  xlab("log2 fold change") + ylab("-log10 p-value")
g
dev.off()



homer李洋在A集群上安装位置
/Share/home/lulab1/users/liyang/apps/homer/bin

自己摸索chip分析流程（homer几步）
1，mapping,bowtie2
bowtie2 -q -N 1 -x /Work1/home/wangdong/ensemble/genome -1 /Work1/home/wangdong/lss/data/GQ_CHIP/GQ-1_R1.fastq -2 /Work1/home/wangdong/lss/data/GQ_CHIP/GQ-1_R2.fastq -S /Work1/home/wangdong/lss/data/GQ_CHIP/mapping/GQ_H3K4ME.sam
bowtie2 -q -N 1 -x /Work1/home/wangdong/ensemble/genome -1 /Work1/home/wangdong/lss/data/GQ_CHIP/GQ-2_R1.fastq -2 /Work1/home/wangdong/lss/data/GQ_CHIP/GQ-2_R2.fastq -S /Work1/home/wangdong/lss/data/GQ_CHIP/mapping/GQ_H3K4ME2.sam

###选用的mapping为bowtie2，homer上推荐用bowtie，所以bowtie2参数说明如下
bowtie2 [options]* -x <bt2-idx> {-1 <m1> -2 <m2> | -U <r>} -S [<hit>]

必须参数：

    -x <bt2-idx> 由bowtie2-build所生成的索引文件的前缀。首先 在当前目录搜寻，然后
    在环境变量 BOWTIE2_INDEXES 中制定的文件夹中搜寻。
    -1 <m1> 双末端测寻对应的文件1。可以为多个文件，并用逗号分开；多个文件必须和 -2 
    <m2> 中制定的文件一一对应。比如:"-1 flyA_1.fq,flyB_1.fq -2 flyA_2.fq,flyB
    _2.fq". 测序文件中的reads的长度可以不一样。
    -2 <m2> 双末端测寻对应的文件2.
    -U <r> 非双末端测寻对应的文件。可以为多个文件，并用逗号分开。测序文件中的reads的
    长度可以不一样。
    -S <hit> 所生成的SAM格式的文件前缀。默认是输入到标准输出。


2提交任务
perl bsub.pl mapping_gq_chip_bowtie2.sh 1 TINY ./source.txt
samtobam
samtools view -bS /Work1/home/wangdong/lss/data/GQ_CHIP/mapping/GQ_H3K4ME.sam > /Work1/home/wangdong/lss/data/GQ_CHIP/mapping/GQ_H3K4ME1.bam

提交任务
GQ_samtobam1.sh
samtobam.sh
perl bsub.pl samtobam.sh 1 TINY ./source.txt

bamToBed -i /Work1/home/wangdong/lss/data/GQ_CHIP/mapping/GQ_H3K4ME1.bam | awk 'BEGIN{FS="\t";OFS="\t"}{if($1=="MT"){print "chrM",$2,$3,$4,$5,$6}else{print "chr"$1,$2,$3,$4,$5,$6}}' > /Work1/home/wangdong/lss/data/GQ_CHIP/mapping/GQ_H3K4ME1.bed
bamToBed -i /Work1/home/wangdong/lss/data/GQ_CHIP/mapping/GQ_H3K4ME2.bam | awk 'BEGIN{FS="\t";OFS="\t"}{if($1=="MT"){print "chrM",$2,$3,$4,$5,$6}else{prznt "chr"$1,$2,$3,$4,$5,$6}}' > /Work1/home/wangdong/lss/data/GQ_CHIP/mapping/GQ_H3K4ME2.bed

bamtobed.sh
perl bsub.pl bamtobed.sh 1 TINY ./source.txt

makeTagDirectory /Work1/home/wangdong/lss/data/GQ_CHIP/maketags_H3K4ME1/ /Work1/home/wangdong/lss/data/GQ_CHIP/mapping/GQ_H3K4ME1.bed -genome /Work1/home/wangdong/ensemble/genome.fa -sspe
makeTagDirectory /Work1/home/wangdong/lss/data/GQ_CHIP/maketags_h3k4me2/ /Work1/home/wangdong/lss/data/GQ_CHIP/mapping/GQ_H3K4ME2.bed -genome /Work1/home/wangdong/ensemble/genome.fa -sspe

maketagdirectory.sh
perl bsub.pl maketagdirectory.sh 1 TINY ./source.txt

removeOutOfBoundsReads.pl /Work1/home/wangdong/lss/data/GQ_CHIP/maketags_H3K4ME1/ /Work1/home/wangdong/ensemble/genome.fa -chromSizes /Work1/home/wangdong/ensemble/hg19.chrom.sizes
removeOutOfBoundsReads.pl /Work1/home/wangdong/lss/data/GQ_CHIP/maketags_h3k4me2/ /Work1/home/wangdong/ensemble/genome.fa -chromSizes /Work1/home/wangdong/ensemble/hg19.chrom.sizes

makeUCSCfile /Work1/home/wangdong/lss/data/GQ_CHIP/maketags_H3K4ME1/ -o auto -bigWig /Work1/home/wangdong/ensemble/hg19.chrom.sizes -norm 1e7 -strand both -fsize 1e20 > /Work1/home/wangdong/lss/data/GQ_CHIP/maketags_H3K4ME1/tags.trackInfo2.txt
makeUCSCfile /Work1/home/wangdong/lss/data/GQ_CHIP/maketags_h3k4me2/ -o auto -bigWig /Work1/home/wangdong/ensemble/hg19.chrom.sizes -norm 1e7 -strand both -fsize 1e20 > /Work1/home/wangdong/lss/data/GQ_CHIP/maketags_h3k4me2/tags.trackInfo2.txt

perl bsub.pl makeUCSCfile.sh 1 TINY ./source.txt

findPeaks <tag directory> -style <factor|histone|groseq> -o auto -i <control tag directory>

findPeaks /Work1/home/wangdong/lss/data/GQ_CHIP/maketags_H3K4ME1/ -style histone -o auto
findPeaks /Work1/home/wangdong/lss/data/GQ_CHIP/maketags_h3k4me2/ -style histone -o auto

perl bsub.pl findpeak.sh 1 TINY ./source.txt

pos2bed.pl regions.txt > H3K4ME2.regions.bed
$pos2bed.pl regions.txt > H3K4ME1.regions.bed

        Converted 107445 peaks total
pos2bed.pl regions.txt > H3K4ME2.regions.bed

        Converted 89935 peaks total


#李洋方法
findPeaks $path0/2.tags/$i -i $input480 -o auto -style histone -F 4 -L 4 -size 1000 -fdr 0.001 -nfr
pos2bed.pl $path0/2.tags/$i/regions.txt > $path0/2.tags/$i/$i.regions.bed

chip李洋所在位置
/Share/home/lulab1/users/liyang/project/wanglab/bin


chip分析小玄推荐
先maketag
findPeaks <tag directory> -style <factor|histone|groseq> -o auto -i <control tag directory>

李洋推荐
没有一个工具是可以从头到尾的，可以查查MACS和MEME
现在的结果是homer
我用过macs，meme，感觉他们的结果会更好一些

尝试利用-fragLength given(?)去将依旧还是有一些峰的intron隐藏，似乎不行？
bamToBed -i accepted_hits.bam | awk 'BEGIN{FS="\t";OFS="\t"}{if($1=="MT"){print "chrM",$2,$3,$4,$5,$6}else{print "chr"$1,$2,$3,$4,$5,$6}}' > accepted_hits.bed
makeTagDirectory tags2/ accepted_hits.bed -genome /Work1/home/wangdong/ensemble/genome.fa -sspe
removeOutOfBoundsReads.pl tags/ /Work1/home/wangdong/ensemble/genome.fa -chromSizes /Work1/home/wangdong/ensemble/hg19.chrom.sizes     
makeUCSCfile tags2 -o auto -bigWig 	 -norm 1e7 -strand both -fsize 1e20 -fragLength given > tags2/tags.trackInfo2.txt

可视化流程过程(swc2_swc3_swnc_swm1_swm2全按下面流程，在A集群上)
bamToBed -i accepted_hits.bam | awk 'BEGIN{FS="\t";OFS="\t"}{if($1=="MT"){print "chrM",$2,$3,$4,$5,$6}else{print "chr"$1,$2,$3,$4,$5,$6}}' > accepted_hits.bed
makeTagDirectory tags/ accepted_hits.bed -genome /Work1/home/wangdong/ensemble/genome.fa -sspe
removeOutOfBoundsReads.pl tags/ /Work1/home/wangdong/ensemble/genome.fa -chromSizes /Work1/home/wangdong/ensemble/hg19.chrom.sizes     
makeUCSCfile tags -o auto -bigWig /Work1/home/wangdong/ensemble/hg19.chrom.sizes -norm 1e7 -strand both -fsize 1e20 > tags/tags.trackInfo.txt      也调用了bedGraphToBigWig，所以这个一样要首先安装好
参考序列所在目录
/Work1/home/wangdong/ensemble/genome.fa

/Work1/home/wangdong/lss/20141021/Undetermined_indices/sw4/RNA_seq/SWC2_1/accepted_hits.bed
configureBclToFastq.pl --input-dir /Share/home/wangdong/data/rawdata/150210_C00126_0166_AC6EYWANXX/Data/Intensities/BaseCalls --output ./Unaligned3_undetermined --sample-sheet ./SampleSheet_undetermined.csv --no-eamss

configureBclToFastq.pl --input-dir /Share/home/wangdong/data/rawdata/150210_C00126_0166_AC6EYWANXX/Data/Intensities/BaseCalls --output ./Unaligned2_undetermined --sample-sheet ./SampleSheet_undetermined.csv --no-eamss
configureBclToFastq.pl --input-dir /Share/home/wangdong/data/rawdata/150210_C00126_0166_AC6EYWANXX/Data/Intensities/BaseCalls --output ./Unaligned --sample-sheet ./SampleSheet.csv --no-eamss --use-bases-mask Y126,I6n,Y126


/Share/home/lulab1/users/liyang/project/wanglab/tmp

homer将mapping文件进行可视化流程，李洋pipeline
#!/usr/bin/bash
## add "chr" to chromsome id; change MT to chrM
bamToBed -i accepted_hits.bam | awk 'BEGIN{FS="\t";OFS="\t"}{if($1=="MT"){print "chrM",$2,$3,$4,$5,$6}else{print "chr"$1,$2,$3,$4,$5,$6}}' > accepted_hits.bed
## create tag index
makeTagDirectory tags/ accepted_hits.bed -genome hg19 -sspe
## create UCSC bigwig file for visualization
removeOutOfBoundsReads.pl tags/ hg19 -chromSizes ./hg19.chrom.sizes
makeUCSCfile tags -o auto -bigWig ./hg19.chrom.sizes -norm 1e7 -strand both -fsize 1e20 > tags/tags.trackInfo.txt

##关于链特异性，来自RNA-seq数据Interpreting the correct strand                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       
    By default, HOMER will interpret each read as it is presented in the alignment file.  This can differ from the expected results if you have paired-end sequencing from a strand-specific RNA-Seq experiment.  It can also result in reads aligning to the opposite strand if a 'first-strand' library synthesis protocol is used, such as methods that use the deoxy-UTP incorporation.  To control how HOMER interprets the strands you have two choices: Either use "-strand -" when running programs like annotatePeaks.pl when you would normally use "-strand +", or you can flip the strand during the makeTagDirectory step: 
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        makeTagDirectory $path0/Tags/$i $path0/Raw/$i/bowtie/$i.bam -unique -genome hg19 -checkGC
        -flip : (will flip the strands of each read in the alignment file)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               948  makeTagDirectory test accepted_hits.bam -unique -genome hg19 -checkGC
        -sspe : (for Strand Specific Paired End sequencing - this will flip the 2nd read of the mate-pair to match the first read).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      949  which makeTagDirectory 
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         950  /Share/home/lulab1/users/liyang/apps/homer/bin/makeTagDirectory tagsDir accepted_hits.bam -unique -genome hg19 -checkGC
    For example, if you're analyzing data from Illumina's newer strand specific RNA-Seq kits, you'll probably want to use "-flip" and also "-sspe" if you have a paired-end experiment                                                                                                                                                                                                                                                                                                                                                                                                                                                   951  ll
  952  cd tagsDir/
  953  ls
  954  ll
  955  less 10.tags.tsv 
  956  ls
  957  ll
  958  cd ../
  959  vi /Share/home/lulab1/users/liyang/project/Wanglab/ChIP/bin/6.makeUCSCfile.sh 
  960  ll
  961  makeUCSCfile $path1/$i -o auto -bigWig $path0/hg19.chrom.sizes -fsize 1e20 > $path1/$i/$i.trackInfo.txt
  962  makeUCSCfile tagsDir/ -o auto -bigWig /Share/home/lulab1/users/liyang/project/Wanglab/ChIP/hg19.chrom.sizes -fsize 1e20 > tagsDir/test.trackInfo.txt
  
cat tags.bed | awk 'BEGIN{FS="\t";OFS="\t"}{print "chr"$1,$2,$3,$4,$5,$6}'

> mx <- matrix(randu())
randu
> mx <- matrix(1:10,ncol=2)
> head(mx)
     [,1] [,2]
[1,]    1    6
[2,]    2    7
[3,]    3    8
[4,]    4    9
[5,]    5   10
> mx2 <- matrix(10:20,ncol=2)
Warning message:
In matrix(10:20, ncol = 2) :
ggplot 
 
  data length [11] is not a sub-multiple or multiple of the number of rows [6]
> mx2 <- matrix(10:21,ncol=2)
> head(mx2)
     [,1] [,2]
[1,]   10   16
[2,]   11   17
[3,]   12   18
[4,]   13   19
[5,]   14   20
[6,]   15   21
> mx  <- cbind(mx,"Red")
> mx2 <- cbind(mx2,"Blue")
> tmp <- rbind(mx,mx2)
> head(tmp)
     [,1] [,2] [,3]  
[1,] "1"  "6"  "Red" 
[2,] "2"  "7"  "Red" 
[3,] "3"  "8"  "Red" 
[4,] "4"  "9"  "Red" 
[5,] "5"  "10" "Red" 
[6,] "10" "16" "Blue"
> tmp <- data.frame(tmp)
> head(tmp)
  X1 X2   X3
1  1  6  Red
2  2  7  Red
3  3  8  Red
4  4  9  Red
5  5 10  Red
6 10 16 Blue
> library(ggplot2)
Error in library(ggplot2) : there is no package called ggplot2?
> library("ggplot2")
Error in library("ggplot2") : there is no package called ggplot2?
> ggplot2(tmp,aes(x=tmp$X1,y=tmp$X2)) + geom_point(aes(colour=tmp$X3))


  git clone 命令参数：
复制代码

usage: git clone [options] [--] <repo> [<dir>]
7.git clone: 
这是较为简单的一种初始化方式，当你已经有一个远程的Git版本库，只需要在本地克隆一份，例如'git clone git://github.com/someone/some_project.git some_project'命令就是将'git://github.com/someone/some_project.git'这个URL地址的远程版 本库完全克隆到本地some_project目录下面

 参数挺多，但常用的就几个：

1. 最简单直接的命令

git clone xxx.git

2. 如果想clone到指定目录

git clone xxx.git "指定目录"

3. clone时创建新的分支替代默认Origin HEAD（master）

git clone -b [new_branch_name]  xxx.git

4. clone 远程分支

　　git clone 命令默认的只会建立master分支，如果你想clone指定的某一远程分支(如：dev)的话，可以如下：

　　A. 查看所有分支(包括隐藏的)  git branch -a 显示所有分支，如：　　　　

* master
  remotes/origin/HEAD -> origin/master
  remotes/origin/dev
  remotes/origin/master

　　B.  在本地新建同名的("dev")分支，并切换到该分支

git checkout -t origin/dev 该命令等同于：
git checkout -b dev origin/dev



correlation作图
m=read.table("ratio100_sw.txt",head=T)  直接读取txt
for (i in 2:17){
    m[,i]=log(m[,i]+1e-3)
}
colnames(m)=c("genes","SW1","SW2","SW3","SW4","SW5","SW6","SW7","SW8","SW9","SW10","SW11","SW12","SW13","SW14","SW15","SW16")


## functions, scatter plot
panel.cor.scale <- function(x, y, digits=2, prefix="", cex.cor)
{
usr <- par("usr"); on.exit(par(usr))
par(usr = c(0, 1, 0, 1))
r = (cor(x, y,use="pairwise"))
txt <- format(c(r, 0.123456789), digits=digits)[1]
txt <- paste(prefix, txt, sep="")
if(missing(cex.cor)) cex <- 0.8/strwidth(txt)
text(0.5, 0.5, txt, cex = cex * abs(r))
}

## function, correlation coefficient
panel.cor <- function(x, y, digits=2, prefix="", cex.cor)
{
usr <- par("usr"); on.exit(par(usr))
par(usr = c(0, 1, 0, 1))
r = (cor(x, y,use="pairwise"))
txt <- format(c(r, 0.123456789), digits=digits)[1]
txt <- paste(prefix, txt, sep="")
if(missing(cex.cor)) cex <- 0.8/strwidth(txt)
text(0.5, 0.5, txt, cex = cex )


}


#panel.hist <- function(x, ...)
#{
#usr <- par("usr"); on.exit(par(usr))
#par(usr = c(usr[1:2], 0, 1.5) )
#h <- hist(x, plot = FALSE)
#breaks <- h$breaks; nB <- length(breaks)
#y <- h$counts; y <- y/max(y)
#rect(breaks[-nB], 0, breaks[-1], y, col="cyan", ...)
#}
pairs.panels <- function (x,y,smooth=FALSE,scale=FALSE)  #为了不要线，此处将smooth设定平滑拟合曲线为FALSE即可以关掉了
{if (smooth){
        if (scale) {
                pairs(x,upper.panel=panel.cor.scale,lower.panel=panel.smooth)
        }
        else {pairs(x,upper.panel=panel.cor,lower.panel=panel.smooth,gap=0)
        } #else {pairs(x,upper.panel=panel.cor,lower.panel=panel.smooth)
}
else #smooth is not true
        { if (scale) {pairs(x,diag.panel=panel.hist,upper.panel=panel.cor.scale)
          } else {pairs(x,upper.panel=panel.cor) }
        } #end of else (smooth)
} #end of function

pdf("cor_sw_again.pdf",height=10,width=10)
pairs.panels(m[,-1])

dev.off()


> mydata <- read.csv("c2_c3.csv")
> tiff("c2_c3_edit.tiff")
> plot(mydata$ctnnb1_shRNA1_rpkm,mydata$ctnnb1_shRNA2_rpkm,lty=2,pch=20,cex=0.5,main="expression difference between ctnnb1_shRNA1 and ctnnb1_shRNA2",xlab="log10(ctnnb1_shRNA1_rpkm)",ylab="log10(ctnnb1_shRNA2_rpkm)")
> dev.off()


王会丽new，分析大库中样品的差异
> library("limma")
> library("edgeR")
> targets = readTargets("F:/lss/work/20150113/WHL/whl/final_result/edger/targets.txt")
Warning message:
In read.table(file, header = TRUE, stringsAsFactors = FALSE, sep = sep,  :
  incomplete final line found by readTableHeader on 'F:/lss/work/20150113/WHL/whl/final_result/edger/targets.txt'
在文件的最后一行加个回车
原来最后一行的结尾不是EOL（End of line）字符，所以R提醒你数据是不是不完全。
targets = readTargets("F:/lss/work/20150113/WHL/whl/final_result/edger/targets.txt")  ##sample information 读取数据的方法不局限，只需要将计数过得reads数据以矩阵形式读入即可

d = readDGE(targets$file,group = targets$group)
colnames(d) = targets$description

keep = rowSums(cpm(d) > 1) >= 2   #edgeR::cpm 		Counts per Million or Reads per Kilobase per Million,rowSums:Give Column Sums of a Matrix or Data Frame
d = d[keep,]
## remove tags with low abundance

d$samples$lib.size = colSums(d$counts)

d = calcNormFactors(d) #默认为TMM标准化

tiff("F:/lss/work/20150113/WHL/whl/final_result/edger/d2_d5_Information.par.tiff")

plotMDS(d) #Multidimensional scaling plot of digital gene expression profiles

d = estimateCommonDisp(d) #Maximizes the negative binomial conditional common likelihood to give the estimate of the common dispersion across all tags
d = estimateTagwiseDisp(d) #Estimates tagwise dispersion values by an empirical Bayes method based on weighted conditional maximum likelihood. 
tiff("F:/lss/work/20150113/WHL/whl/final_result/edger/d2_d5_MeanVar.tiff")
plotMeanVar(d, show.tagwise.vars = TRUE, NBline = TRUE) # edgeR::binMeanVar 		Explore the mean-variance relationship for DGE data
tiff("F:/lss/work/20150113/WHL/whl/final_result/edger/d2_d5_BCV.tiff")
plotBCV(d) #edgeR::plotBCV 		Plot Biological Coefficient of Variation

de = exactTest(d, pair = c("TREAT","CONTROL")) #Exact Tests for Differences between Two Groups of Negative-Binomial Counts，Compute genewise exact tests for differences in the means between two groups of negative-binomially distributed counts
### differential expreesed gene
top = topTags(de)

deinfo = de$table
result = cbind(cpm(d)[rownames(de),],deinfo[rownames(de),])# ，cbind() 把矩阵横向合并成一个大矩阵（列方式），而rbind()是纵向合并（行方式）。在命令中> X <- cbind(arg 1 , arg 2 , arg 3 , ...) cbind() 的参数要么是任何长度的向量，要么是列长度一致的的矩阵（即行数一样）。结果将是一个合并arg1 , arg2 , . . . 的列形成的矩阵。
write.table(result,"F:/lss/work/20150113/WHL/whl/final_result/edger/d2_d5_DEG.txt",quote = F, col.names = T, row.names = T, sep = "\t")

summary(de.sig <- decideTestsDGE(de)) #multiple Testing Across Genes and Contrasts，Classify a series of related differential expression statistics as up, down or not significant. A number of different multiple testing schemes are offered which adjust for multiple testing down the genes as well as across contrasts for each gene. 
> summary(de.sig <- decideTestsDGE(de))
   [,1]
-1 4067
0  1909
1  4108
   [,1]
-1 4181
0  1599
1  4256


detags <- rownames(d)[as.logical(de.sig)]
result.sig = result[detags,]
write.table(result.sig,"F:/lss/work/20150113/WHL/whl/final_result/edger/d2_d5__p.05.txt",quote = F, col.names = T, row.names = T, sep = "\t")

tiff("F:/lss/work/20150113/WHL/whl/final_result/edger/d2_d5_Smear.tiff")
plotSmear(de,de.tags = detags)
abline(h =c(-2,2), col = "blue")

dev.off()


c2_c3_nc过滤
library(limma)
library("edgeR")
targets = readTargets("F:/lss/work/20150113/WHL/whl/final_result/edger/targets.txt")  ##sample information 读取数据的方法不局限，只需要将计数过得reads数据以矩阵形式读入即可

d = readDGE(targets$file,group = targets$group)
colnames(d) = targets$description

s= 2   #edgeR::cpm 		Counts per Million or Reads per Kilobase per Million,rowSums:Give Column Sums of a Matrix or Data Frame
d = d[keep,]
## remove tags with low abundance

d$samples$lib.size = colSums(d$counts)

d = calcNormFactors(d) #默认为TMM标准化

tiff("D:/htseq_result/c2_c3_nc/check_filtered/c_f_Information.par.tiff")

plotMDS(d) #Multidimensional scaling plot of digital gene expression profiles

d = estimateCommonDisp(d)
d = estimateTagwiseDisp(d)
tiff("D:/htseq_result/c2_c3_nc/check_filtered/c_f_MeanVar.tiff")
plotMeanVar(d, show.tagwise.vars = TRUE, NBline = TRUE)
tiff("D:/htseq_result/c2_c3_nc/check_filtered/c_f_BCV.tiff")
plotBCV(d)

de = exactTest(d, pair = c("TREAT","CONTROL"))
### differential expreesed gene
top = topTags(de)

deinfo = de$table
result = cbind(cpm(d)[rownames(de),],deinfo[rownames(de),])# ，cbind() 把矩阵横向合并成一个大矩阵（列方式），而rbind()是纵向合并（行方式）。在命令中> X <- cbind(arg 1 , arg 2 , arg 3 , ...) cbind() 的参数要么是任何长度的向量，要么是列长度一致的的矩阵（即行数一样）。结果将是一个合并arg1 , arg2 , . . . 的列形成的矩阵。
write.table(result,"D:/htseq_result/c2_c3_nc/check_filtered/c_f_DEG.txt",quote = F, col.names = T, row.names = T, sep = "\t")

summary(de.sig <- decideTestsDGE(de))
  [,1] 
-1  2091
0  11821
1   2231


detags <- rownames(d)[as.logical(de.sig)]
result.sig = result[detags,]
write.table(result.sig,"D:/htseq_result/c2_c3_nc/check_filtered/c_f_p.05.txt",quote = F, col.names = T, row.names = T, sep = "\t")

tiff("D:/htseq_result/c2_c3_nc/check_filtered/c_f_Smear.tiff")
plotSmear(de,de.tags = detags)
abline(h =c(-1,1), col = "blue")

dev.off()

m1_m2_nc不过滤
library(limma)
library("edgeR")
targets = readTargets("D:/htseq_result/m1_m2_nc/targets.txt")  ##sample information 读取数据的方法不局限，只需要将计数过得reads数据以矩阵形式读入即可

d = readDGE(targets$file,group = targets$group)s
colnames(d) = targets$description

d$samples$lib.size = colSums(d$counts)

d = calcNormFactors(d)

tiff("D:/htseq_result/m1_m2_nc/m1_m2_nc_no_filtered/Information.par.tiff")

plotMDS(d) #Multidimensional scaling plot of digital gene expression profiles

d = estimateCommonDisp(d)
d = estimateTagwiseDisp(d)
tiff("D:/htseq_result/m1_m2_nc/m1_m2_nc_no_filtered/MeanVar.tiff")
plotMeanVar(d, show.tagwise.vars = TRUE, NBline = TRUE)
tiff("D:/htseq_result/m1_m2_nc/m1_m2_nc_no_filtered/BCV.tiff")
plotBCV(d)

de = exactTest(d, pair = c("TREAT","CONTROL"))
### differential expreesed gene
top = topTags(de)

deinfo = de$table
result = cbind(cpm(d)[rownames(de),],deinfo[rownames(de),])# ，cbind() 把矩阵横向合并成一个大矩阵（列方式），而rbind()是纵向合并（行方式）。在命令中> X <- cbind(arg 1 , arg 2 , arg 3 , ...) cbind() 的参数要么是任何长度的向量，要么是列长度一致的的矩阵（即行数一样）。结果将是一个合并arg1 , arg2 , . . . 的列形成的矩阵。
write.table(result,"D:/htseq_result/m1_m2_nc/m1_m2_nc_no_filtered/DEG.txt",quote = F, col.names = T, row.names = T, sep = "\t")

summary(de.sig <- decideTestsDGE(de))
   [,1] 
-1   117
0  61614
1    337

detags <- rownames(d)[as.logical(de.sig)]
result.sig = result[detags,]
write.table(result.sig,"D:/htseq_result/m1_m2_nc/m1_m2_nc_no_filtered/p.05.txt",quote = F, col.names = T, row.names = T, sep = "\t")

tiff("D:/htseq_result/m1_m2_nc/m1_m2_nc_no_filtered/Smear.tiff")
plotSmear(de,de.tags = detags)
abline(h =c(-1,1), col = "blue")

dev.off()

m1_m2_nc过滤
library(limma)
library("edgeR")
targets = readTargets("D:/htseq_result/m1_m2_nc/targets.txt")  ##sample information 读取数据的方法不局限，只需要将计数过得reads数据以矩阵形式读入即可

d = readDGE(targets$file,group = targets$group)
colnames(d) = targets$description

keep = rowSums(cpm(d) > 1) >= 2   #edgeR::cpm 		Counts per Million or Reads per Kilobase per Million,rowSums:Give Column Sums of a Matrix or Data Frame
d = d[keep,]
## remove tags with low abundance

d$samples$lib.size = colSums(d$counts)

d = calcNormFactors(d)

tiff("D:/htseq_result/m1_m2_nc/m1_m2_nc_filter/Information.par.tiff")

plotMDS(d) #Multidimensional scaling plot of digital gene expression profiles

d = estimateCommonDisp(d)
d = estimateTagwiseDisp(d)
tiff("D:/htseq_result/m1_m2_nc/m1_m2_nc_filter/MeanVar.tiff")
plotMeanVar(d, show.tagwise.vars = TRUE, NBline = TRUE)
tiff("D:/htseq_result/m1_m2_nc/m1_m2_nc_filter/BCV.tiff")
plotBCV(d)

de = exactTest(d, pair = c("TREAT","CONTROL"))
### differential expreesed gene
top = topTags(de)

deinfo = de$table
result = cbind(cpm(d)[rownames(de),],deinfo[rownames(de),])# ，cbind() 把矩阵横向合并成一个大矩阵（列方式），而rbind()是纵向合并（行方式）。在命令中> X <- cbind(arg 1 , arg 2 , arg 3 , ...) cbind() 的参数要么是任何长度的向量，要么是列长度一致的的矩阵（即行数一样）。结果将是一个合并arg1 , arg2 , . . . 的列形成的矩阵。
write.table(result,"D:/htseq_result/m1_m2_nc/m1_m2_nc_filter/DEG.txt",quote = F, col.names = T, row.names = T, sep = "\t")

summary(de.sig <- decideTestsDGE(de))
 [,1] 
-1   313
0  14525
1    545

detags <- rownames(d)[as.logical(de.sig)]
result.sig = result[detags,]
write.table(result.sig,"D:/htseq_result/m1_m2_nc/m1_m2_nc_filter/p.05.txt",quote = F, col.names = T, row.names = T, sep = "\t")

tiff("D:/htseq_result/m1_m2_nc/m1_m2_nc_filter/Smear.tiff")
plotSmear(de,de.tags = detags)
abline(h =c(-1,1), col = "blue")

dev.off()

c2_c3_nc过滤一下

library(limma)
library("edgeR")
targets = readTargets("D:/htseq_result/targets.txt")  ##sample information 读取数据的方法不局限，只需要将计数过得reads数据以矩阵形式读入即可

d = readDGE(targets$file,group = targets$group)
colnames(d) = targets$description

keep = rowSums(cpm(d) > 1) >= 2   #edgeR::cpm 		Counts per Million or Reads per Kilobase per Million,rowSums:Give Column Sums of a Matrix or Data Frame
d = d[keep,]
## remove tags with low abundance

d$samples$lib.size = colSums(d$counts)

d = calcNormFactors(d)

tiff("D:/htseq_result/c2_c3_nc_filtered/Information.par.tiff")

plotMDS(d) #Multidimensional scaling plot of digital gene expression profiles

d = estimateCommonDisp(d)
d = estimateTagwiseDisp(d)
tiff("D:/htseq_result/c2_c3_nc_filtered/MeanVar.tiff")
plotMeanVar(d, show.tagwise.vars = TRUE, NBline = TRUE)
tiff("D:/htseq_result/c2_c3_nc_filtered/BCV.tiff")
plotBCV(d)

de = exactTest(d, pair = c("TREAT","CONTROL"))
### differential expreesed gene
top = topTags(de)

deinfo = de$table
result = cbind(cpm(d)[rownames(de),],deinfo[rownames(de),])# ，cbind() 把矩阵横向合并成一个大矩阵（列方式），而rbind()是纵向合并（行方式）。在命令中> X <- cbind(arg 1 , arg 2 , arg 3 , ...) cbind() 的参数要么是任何长度的向量，要么是列长度一致的的矩阵（即行数一样）。结果将是一个合并arg1 , arg2 , . . . 的列形成的矩阵。
write.table(result,"D:/htseq_result/c2_c3_nc_filtered/DEG.txt",quote = F, col.names = T, row.names = T, sep = "\t")

summary(de.sig <- decideTestsDGE(de))

detags <- rownames(d)[as.logical(de.sig)]
result.sig = result[detags,]
write.table(result.sig,"D:/htseq_result/c2_c3_nc_filtered/p.05.txt",quote = F, col.names = T, row.names = T, sep = "\t")

tiff("D:/htseq_result/c2_c3_nc_filtered/Smear.tiff")
plotSmear(de,de.tags = detags)
abline(h =c(-1,1), col = "blue")

dev.off()

最终结果还是要c2_c3一起，过滤,不然差异表达的可达到4000多个，显然不是想要的，如果想要不过滤的也可以，另外，该软件许多过程评估都需要replicate的，所以讲c2,c3一起作用
library(limma)
library("edgeR")
targets = readTargets("D:/htseq_result/targets.txt")  ##sample information 读取数据的方法不局限，只需要将计数过得reads数据以矩阵形式读入即可

d = readDGE(targets$file,group = targets$group)
colnames(d) = targets$description

d$samples$lib.size = colSums(d$counts)

d = calcNormFactors(d)

tiff("D:/htseq_result/final_c2_c3_nc/Information.par.tiff")

plotMDS(d) #Multidimensional scaling plot of digital gene expression profiles

d = estimateCommonDisp(d)
d = estimateTagwiseDisp(d)
tiff("D:/htseq_result/final_c2_c3_nc/MeanVar.tiff")
plotMeanVar(d, show.tagwise.vars = TRUE, NBline = TRUE)
tiff("D:/htseq_result/final_c2_c3_nc/BCV.tiff")
plotBCV(d)

de = exactTest(d, pair = c("TREAT","CONTROL"))
### differential expreesed gene
top = topTags(de)

deinfo = de$table
result = cbind(cpm(d)[rownames(de),],deinfo[rownames(de),])# ，cbind() 把矩阵横向合并成一个大矩阵（列方式），而rbind()是纵向合并（行方式）。在命令中> X <- cbind(arg 1 , arg 2 , arg 3 , ...) cbind() 的参数要么是任何长度的向量，要么是列长度一致的的矩阵（即行数一样）。结果将是一个合并arg1 , arg2 , . . . 的列形成的矩阵。
write.table(result,"D:/htseq_result/final_c2_c3_nc/DEG.txt",quote = F, col.names = T, row.names = T, sep = "\t")

summary(de.sig <- decideTestsDGE(de))

detags <- rownames(d)[as.logical(de.sig)]
result.sig = result[detags,]
write.table(result.sig,"D:/htseq_result/final_c2_c3_nc/p.05.txt",quote = F, col.names = T, row.names = T, sep = "\t")

tiff("D:/htseq_result/final_c2_c3_nc/Smear.tiff")
plotSmear(de,de.tags = detags)
abline(h =c(-1,1), col = "blue")

dev.off()

重新利用c2_nc,c3_nc,c2_c3做一遍，只需要改变target即可，不进行过滤的
要想得到那几个图，之前就要先定义好tiff等，然后开始plot,不进行过滤的
library(limma)
library("edgeR")
targets = readTargets("D:/htseq_result/c2_nc/targets.txt")  ##sample information 读取数据的方法不局限，只需要将计数过得reads数据以矩阵形式读入即可

d = readDGE(targets$file,group = targets$group)
colnames(d) = targets$description

d$samples$lib.size = colSums(d$counts)

d = calcNormFactors(d)

pdf("D:/htseq_result/c2_nc/Information.par.pdf")

plotMDS(d) #Multidimensional scaling plot of digital gene expression profiles

d = estimateCommonDisp(d)
d = estimateTagwiseDisp(d)
pdf("D:/htseq_result/c2_nc/MeanVar.pdf")
plotMeanVar(d, show.tagwise.vars = TRUE, NBline = TRUE)
pdf("D:/htseq_result/c2_nc/BCV.pdf")
plotBCV(d)

de = exactTest(d, pair = c("TREAT","CONTROL"))
### differential expreesed gene
top = topTags(de)

deinfo = de$table
result = cbind(cpm(d)[rownames(de),],deinfo[rownames(de),])# ，cbind() 把矩阵横向合并成一个大矩阵（列方式），而rbind()是纵向合并（行方式）。在命令中> X <- cbind(arg 1 , arg 2 , arg 3 , ...) cbind() 的参数要么是任何长度的向量，要么是列长度一致的的矩阵（即行数一样）。结果将是一个合并arg1 , arg2 , . . . 的列形成的矩阵。
write.table(result,"D:/htseq_result/c2_nc/DEG.txt",quote = F, col.names = T, row.names = T, sep = "\t")

summary(de.sig <- decideTestsDGE(de))

detags <- rownames(d)[as.logical(de.sig)]
result.sig = result[detags,]
write.table(result.sig,"D:/htseq_result/c2_nc/p.05.txt",quote = F, col.names = T, row.names = T, sep = "\t")

tiff("D:/htseq_result/c2_nc/Smear.tiff")
plotSmear(de,de.tags = detags)
abline(h =c(-1,1), col = "blue")

dev.off()

点保存工作空间就可以得到历史
在自己电脑上R运行的一些历史
source('http://www.bioconductor.org/biocLite.R')
biocLite('cummeRbund')
library('cummeRbund')
cuff_data <- readCufflinks('E:\??\sw_RNASWQ\SWC2')
csDensity(genes(cuff_data))
cuff_data <- readCufflinks('E:\??\sw_RNASWQ\SWC2')
cuff_data <- readCufflinks('E:\work\sw_RNASWQ\SWC2')
cuff_data <- readCufflinks('E:/work/sw_RNASWQ/SWC2')
csDensity(genes(cuff_data))
csScatter(genes(cuff_data),'C2','NC')
csScatter(genes(cuff_data))
?csScatter
?csScatter
csScatter(genes(cuff_data),'SWC2','SWNC')
cuff_data <- readCufflinks('E:/work/sw_RNASWQ/SWC2')
csScatter(genes(cuff_data),'SWC2','SWNC')
q()
library(cummeRbund)
cuff_data <- readCufflinks('E:/work/sw_RNASWQ/SWC2')
mygene <- getGene(cuff_data,'CTNNB1')
expressionBarplot(mygene)
expressionBarplot(mygene)
mygene <- getGene(cuff_data,'CTNNB1')
expressionBarplot(mygene)
expressionBarplot(mygene)
q()
library(cummRbund)
library(cummeRbund)
cuff_data <- readCufflinks('E:/work/sw_RNASWQ/SWC2')
mygene<- getGene(cuff_data,'CTNNB1')
expressionBarplot(mygene)
expressionBarplot(isoforms(mygene))
cuff_data <- readCufflinks('E:/work/sw_RNASWQ/SWM1')
csDensity(genes(cuff_data))
csDensity(genes(cuff_data))
csScatter(genes(cuff_data),'SWM1','SWNC')
csScatter(genes(cuff_data),'SWM1','SWNC')
q()
source("http://bioconductor.org/biocLite.R")
biocLite("edgeR")
library(edgeR)
library(limma)
library(edgeR)
??DEGlist
q()
library(limma)
library(edgeR)
targets = readTargets("D:/htseq_result/targets.txt")
d = readDGE(targets$file,group = targets$group)
d
colnames(d) = targets$description
colnames
colnames(d)
??rowSums
??cpm
keep = rowSums(cpm(d) > 1) >= 2
d = d[keep,]
d$samples$lib.size = colSums(d$counts)
d = calcNormFactors(d)
pdf("D:/htseq_result/Information.par.pdf")
plotMDS??
q()
plotMDS(d)
d = estimateCommonDisp(d)
d = estimateTagwiseDisp(d)
plotMeanVar(d, show.tagwise.vars = TRUE, NBline = TRUE)
plotBCV(d)
de = exactTest(d, pair = c("TREAT","CONTROL"))
top = topTags(de)
deinfo = de$table
result = cbind(cpm(d)[rownames(de),],deinfo[rownames(de),])
write.table(result,"D:/htseq_result/",quote = F, col.names = T, row.names = T, sep = "\t")
write.table(result,"D:/htseq_result/DEG.txt",quote = F, col.names = T, row.names = T, sep = "\t")
summary(de.sig <- decideTestsDGE(de))
detags <- rownames(d)[as.logical(de.sig)]
result.sig = result[detags,]
write.table(result.sig,"D:/htseq_result/p.05.txt",quote = F, col.names = T, row.names = T, sep = "\t")
plotSmear(de,de.tags = detags)
abline(h =c(-1,1), col = "blue")
dev.off()
save.image("C:\\Users\\dell\\Desktop\\edger.RData")

edgeR运行过程（C2_C3_NC）

library(limma)
library("edgeR")
targets = readTargets("D:/htseq_result/targets.txt")  ##sample information 读取数据的方法不局限，只需要将计数过得reads数据以矩阵形式读入即可

d = readDGE(targets$file,group = targets$group)
colnames(d) = targets$description

keep = rowSums(cpm(d) > 1) >= 2   #edgeR::cpm 		Counts per Million or Reads per Kilobase per Million,rowSums:Give Column Sums of a Matrix or Data Frame
d = d[keep,]
## remove tags with low abundance

d$samples$lib.size = colSums(d$counts)

d = calcNormFactors(d)

pdf("D:/htseq_result/Information.par.pdf")

plotMDS(d) #Multidimensional scaling plot of digital gene expression profiles

d = estimateCommonDisp(d)
d = estimateTagwiseDisp(d)

plotMeanVar(d, show.tagwise.vars = TRUE, NBline = TRUE)
plotBCV(d)

de = exactTest(d, pair = c("TREAT","CONTROL"))
### differential expreesed gene
top = topTags(de)

deinfo = de$table
result = cbind(cpm(d)[rownames(de),],deinfo[rownames(de),])# ，cbind() 把矩阵横向合并成一个大矩阵（列方式），而rbind()是纵向合并（行方式）。在命令中> X <- cbind(arg 1 , arg 2 , arg 3 , ...) cbind() 的参数要么是任何长度的向量，要么是列长度一致的的矩阵（即行数一样）。结果将是一个合并arg1 , arg2 , . . . 的列形成的矩阵。
write.table(result,"D:/htseq_result/DEG.txt",quote = F, col.names = T, row.names = T, sep = "\t")

summary(de.sig <- decideTestsDGE(de))

detags <- rownames(d)[as.logical(de.sig)]
result.sig = result[detags,]
write.table(result.sig,"D:/htseq_result/p.05.txt",quote = F, col.names = T, row.names = T, sep = "\t")


plotSmear(de,de.tags = detags)
abline(h =c(-1,1), col = "blue")

dev.off()


整个过程的详细说明
https://wikis.utexas.edu/display/bioiteam/Differential+gene+expression+analysis#Differentialgeneexpressionanalysis-Analyzedifferentialgeneexpression

HTseq运行
samtools sort -n accepted_hits.bam bamsorted
samtools view -help
samtools view -h bamsorted.bam > SWNC.sam
htseq-count --help
htseq-count -s no -t gene SWNC.sam /Work1/home/wangdong/ensemble/genes.gtf > SWNC_final.count

C2
samtools sort -n accepted_hits.bam bamsorted
samtools view -h bamsorted.bam > SWC2.sam
htseq-count -s no -t gene SWC2.sam /Work1/home/wangdong/ensemble/genes.gtf > SWC2_test.count

C3
samtools sort -n accepted_hits.bam bamsorted
samtools view -h bamsorted.bam > SWC3.sam
htseq-count -s no -t gene SWC3.sam /Work1/home/wangdong/ensemble/genes.gtf > SWC3.count
m1
samtools sort -n accepted_hits.bam bamsorted
samtools view -h bamsorted.bam > SWM1.sam
htseq-count -s no -t gene SWM1.sam /Work1/home/wangdong/ensemble/genes.gtf > SWM1.count
M2
samtools sort -n accepted_hits.bam bamsorted
samtools view -h bamsorted.bam > SWM2.sam
htseq-count -s no -t gene SWM2.sam /Work1/home/wangdong/ensemble/genes.gtf > SWM2.count

将SW*.count几个文件做一下merge

安装库是一件十分繁琐的事情,依赖关系的等十分的麻烦,前面的方法十分重要,但是希望有个整合的版本 那就是
(https://store.continuum.io/cshop/anaconda/) 这个版本整合了大量的常用库 ,下载 
直接bash Anaconda-2.1.0-Linux-x86_64.sh  按照提示,一步步安装即可 ,安装完成后基本库就都已经装好了 十分方便

python-leveldb的安装,从(https://pypi.python.org/pypi/leveldb)下载,但是用4.9.1的gcc无法编译通过,用系统自带的gcc能编译过,python setup.py  build ----- python setup.py  install

python-protobuf的安装,从我们前面已经装过的protobuf中,有个python的文件夹,python setup.py  build ----- python setup.py  install就可以了

python-gflags的安装,从(https://pypi.python.org/pypi/python-gflags)下载,解压python setup.py  build ----- python setup.py  install就可以了
library("limma")
library("edgeR")
targets = readTargets("F:/lss/work/20150113/WHL/whl/final_result/edger/targets.txt")  ##sample information

d = readDGE(targets$file,group = targets$group)
colnames(d) = targets$description

keep = rowSums(cpm(d) > 1) >= 2
d = d[keep,]
## remove tags with low abundance

d$samples$lib.size = colSums(d$counts)

d = calcNormFactors(d)

pdf("Information.par_d1_6.pdf")

plotMDS(d)

d = estimateCommonDisp(d)
d = estimateTagwiseDisp(d)
pdf("MeanVar_2.pdf")
plotMeanVar(d, show.tagwise.vars = TRUE, NBline = TRUE)
pdf("BCV.pdf")
plotBCV(d)

de = exactTest(d, pair = c("TREAT","CONTROL"))
### differential expreesed gene
top = topTags(de)

deinfo = de$table
result = cbind(cpm(d)[rownames(de),],deinfo[rownames(de),])
write.table(result,"F:/lss/work/20150113/WHL/whl/final_result/edger/DEG.txt",quote = F, col.names = T, row.names = T, sep = "\t")

summary(de.sig <- decideTestsDGE(de))

detags <- rownames(d)[as.logical(de.sig)]
result.sig = result[detags,]
write.table(result.sig,"F:/lss/work/20150113/WHL/whl/final_result/edger/p.05.txt",quote = F, col.names = T, row.names = T, sep = "\t")

pdf("smear.pdf")
plotSmear(de,de.tags = detags)
abline(h =c(-1,1), col = "blue")

dev.off()


[wangdong@cluster ~/lss/data2/Unaligned/Project_H9WJ0ADXX/R]$Rscript correlation1.R 

进入R环境。（句首'>'为interpreter prefix）
>args = commandArgs()
>print(args)
[1] "/share/apps/R/R-2.5.0/gnu/lib/R/bin/exec/R"

`commandArgs()`数返回一个数组，其中包含命令行上的参数信息17
`print()`函数打印出数组的所有元素。

也可以不进入交互式的R环境，直接创建R脚本然后执行。脚本中包含三行：
args = commandArgs()
print(args)
q()

bowtie2-build -f ref.fasta ref_whl
bowtie2 -N 3 -x ref_whl WHL_C1_R1_antisense.fastq -S C1_3.sam
fastx_trimmer -f 25 -l 46 -Q 33 -i WHL_C1_R1.fastq -o WHL_C1_R1_antisense.fastq
bedtools安装
git clone https://github.com/arq5x/bedtools2.git
mv bedtools2/ app/
 ls
cd app/
cd ce
cd bedtools2/
make
ls
cd bin/
ls
pwd
vi ~/.bashrc
ll
le ~/.bash
le ~/.bashrc
source ~/.bashrc
ll
bedtools

scp wangdong@10.10.0.22:/Share/home/wangdong/lss/data2/Unaligned/Project_H9WJ0ADXX/Sample_WHL_C1/C1_1.bam ./

There was another thread very recently which covered this. You need to add the -Q33 parameter to tell it that you're using Illumina encoded quality scores, not Sanger encoding. 

scp wangdong@166.111.30.164:/Work1/home/wangdong/lss/20141022/new/SampleSheet.csv ./


BclToFastq.pl --input-dir /Share/home/wangdong/lss/data/150106_C00126_0162_AH9WJ0ADXX/Data/Intensities/BaseCalls/ --output ./Unaligned --sample-sheet ./SampleSheet.csv --no-eamss --use-bases-mask Y101,I7n,Y101

values=set()
for key in z.keys():
    val = z[key]
    if val in values: 
        del z[key]
    else:
        values.add(val)

 setdefault(key[, default])

    If key is in the dictionary, return its value. If not, insert key with a value of default and return default. default defaults to None.

    如果键在字典中，返回这个键所对应的值。如果键不在字典中，向字典 中插入这个键，并且以default为这个键的值，并返回 default。default的默认值为None
    >>> dict={}
>>> dict['key']='a'
>>> dict
{'key': 'a'}
>>> dict.setdefault('key', 'b')  # 键key存在，故返回简直a.
'a'
>>> dict
{'key': 'a'}
>>> dict.setdefault('key0', 'b') # 键key0不存在，故插入此键，并以b为键值.
'b'
>>> dict
{'key0': 'b', 'key': 'a'}


configureBclToFastq.pl --input-dir /Work1/home/wangdong/lss/data/141214_C00126_0157_AHB59AADXX/Data/Intensities/BaseCalls/ --output ./Unaligned_whl_why_re --sample-sheet /Work1/home/wangdong/lss/data/141214_C00126_0157_AHB59AADXX/SampleSheet_141214.csv --no-eamss --use-bases-mask Y101,I7n,Y101
ls
cd cd Unaligned_whl_why/


string.atof(s)将字符串转为浮点型数字


问题：查找一些英文词在哪些小句中出现了，当然是用python来实现，当然是用字典，但是怎么让一个key对应一个 类型为列表的value，直接用列表的append()是不行的，比如dic[key].append(value)，因为解释器并不知道 dic[key]的类型，当时赶时间，用了一个折衷的方案，就是先用value连成一个str，最后用str.split()作一个转换，生成一个列表.

    看了python cookbook，上面正好有一个recipe讲到如何处理这样的问题，好了，揭晓答案吧!

(1)value中允许有重复项.

复制代码 代码如下:

dic = {}
dic.setdefault(key，[]).append(value)
#如:
d1.setdefault('bob_hu'，[]).append(1)
d1.setdefault('bob_hu'，[]).append(2)
print d1['bob_hu'] # [1，2]

(2)value中无重复项.

复制代码 代码如下:

dic = {}
dic.setdefault(key，{})[value] = 1
#如:
d1.setdefault('bob'，{})['f'] = 1
d1.setdefault('bob'，{})['h'] = 1
d1.setdefault('bob'，{})['f'] = 1
print d1['bob'] #{'h': 1， 'f': 1}

您可能感兴趣的文章:

    Python获取文件ssdeep值的方法
    跟老齐学Python之赋值，简单也不简单
    shell脚本中执行python脚本并接收其返回值的例子
    Python使用函数默认值实现函数静态变量的方法
    Python采用raw_input读取输入值的方法
    python实现k均值算法示例(k均值聚类算法)
    python函数返回多个值的示例方法
    python读取注册表中值的方法
    python赋值操作方法分享
    python局部赋值的规则
    python函数缺省值与引用学习笔记分享
    python求crc32值的方法

QQ空间 新浪微博 腾讯微博 搜狐微博 人人网 开心网 百度搜藏 更多 0
Tags：字典 值是列表
复制链接收藏本文打印本文关闭本文返回首页
上一篇：python实现的各种排序算法代码
下一篇：python实现多线程暴力破解登陆路由器功能代码分享
相关文章

    2014-10-10Python入门篇之正则表达式
    2008-09-09Python 命令行参数sys.argv
    2014-02-02python使用7z解压软件备份文件脚本分享
    2013-03-03Python 用户登录验证的小例子
    2014-10-10Python数据结构之Array用法实例
    2014-03-03python文件和目录操作方法大全（含实例）
    2012-09-09python代码检查工具pylint 让你的python更规范
    2007-02-02Python入门
    2014-02-02urllib2自定义opener详解
    2013-11-11python聊天程序实例代码分享

文章评论

社交帐号登录:

    微博
    QQ
    人人
    豆瓣
    更多?

最新最早最热

    评论

    还没有评论，沙发等你来抢

脚本之家正在使用多说
友情提醒：本站文件的解压密码：www.jb51.net (请使用最新的winrar)
最 近 更 新

    Python操作Mysql实例代码教程在线版(查询
    Python基于twisted实现简单的web服务器
    python字符串排序方法
    Python入门篇之列表和元组
    Python实现的一个简单LRU cache
    python实现socket客户端和服务端简单示例
    python中使用urllib2获取http请求状态码的
    python进程类subprocess的一些操作方法例
    深入分析在Python模块顶层运行的代码引起
    python BeautifulSoup使用方法详解

热 点 排 行

    Python入门教程 超详细1小时学会
    python 中文乱码问题深入分析
    比较详细Python正则表达式操作指
    Python字符串的encode与decode研
    Python open读写文件实现脚本
    python 字符串split的用法分享
    Python enumerate遍历数组示例应
    Python 深入理解yield
    python 文件和肪恫僮骱数小17
    Python+Django在windows下的开发

Js与CSS工具

    CSS在线压缩格式化(中文)
    css 格式化整理工具(英文)
    CSS整形格式化
    JavaScript 格式化整理工具
    jsbeautifier Js格式化整理工具(英文)
    php 格式化整理工具(英文)
    HTML/JS互相转换工具
    javascript pack加密压缩工具
    JS Minifier压缩
    JS混淆工具
    在线JS脚本校验器错误
    JavaScript 正则表达式在线测试工具

代码转换工具

    Base64编码加密
    Escape加解密
    HTML/UBB代码转换
    GB2312/BIG5繁简字转换
    经典小工具集 数字转换
    HTML多功能代码转换器
    迅雷快车加/解密
    汉字转换拼音

字典 z 如下

z = {
    1: 'a',
    2: 'a',
    3: 'a',
    4: 'b',
    5: 'c',
    6: 'c',
    7: 'd'
}

要求删除重复的 value 的 item，只保留一个，如何实现？
values=set()
for key in z.keys():
    val = z[key]
    if val in values: 
        del z[key]
    else:
        values.add(val)
ls
nohup make -j 8


plot(data$exp1,data$exp2,lty=2,pch=20,cex=0.5,main="exp1_exp2",xlab="log10(exp1)",ylab="log10(exp2)")

configureBclToFastq.pl --input-dir /Work1/home/wangdong/lss/data/141214_C00126_0157_AHB59AADXX/Data/Intensities/BaseCalls/ --output ./Unaligned_whl_why --sample-sheet /Work1/home/wangdong/lss/data/141214_C00126_0157_AHB59AADXX/SampleSheet.csv --no-eamss --use-bases-mask Y101,I7n,Y101
ls
cd cd Unaligned_whl_why/
ls
nohup make -j 8


Linux系统cp:omitting directory`XXX'问题解决
 
在linux系统中复制文件夹时提示如下：
Shell代码  
cp: omitting directory `foldera/'  
其中foldera是我要复制的文件夹名，出现该警告的原因是因为foldera目录下还存在目录，所以不能直接拷贝。
   www.2cto.com  
解决办法：
使用递归拷贝，在cp命令后面加上-r参数，形如：
Shell代码  
[root@localhost opt]# cp -r foldera folderc  
这里的-r代表递归的意思。
 
同样，当我们在linux系统下删除目录时也需要加上-r参数 ，如果目录为空，则会直接删除，如果目录非空，则会级联删除。不过在级联删除时也会有一个问题就是如果目录下存在很多的文件或者子目录，系统会一个一个进行提示。如果想一步删除不用提示的话可以使用rm -rf命令。f是force的意思，代表强制删除，无提示！

epigetics缺BAP18探针

genomeCoverageBed -split -bg -ibam accepted_hits.sorted.bam -g dm3.chrom.sizes > accepted_hits.bedgraphwigToBigWig accepted_hits.bedgraph dm3.chrom.sizes myfile.bw
genomeCoverageBed -split -bg -i 123.bed -g /Work1/home/wangdong/hg19/ChromInfo.txt > SWC2_accepted_hits_sorted_change.bedgraph
genomeCoverageBed -split -bg -i 123.bed -g /Work1/home/wangdong/hg19/ChromInfo.txt > SWC2_accepted_hits_sorted_change_split2.bedgraph
遇到一个.tgz文件，可不知道如何解压缩
实就是tar zxvf就好了的，tgz跟tar.gz是一样的。gunzip -cd| taz -xvf还要通过重定向来中转，相对来说又麻烦了一点，而且命令也长了。
scp wangdong@166.111.30.164:/Share/home/wangdong/apps/src/Python-2.7.7.tgz ./ C集群上目前传东西
tar -xzvf Python-2.7.7.tgz

scp wangdong@166.111.30.164:/Share/home/wangdong/apps/src/RSeQC-2.5.tar.gz ./
tar -xvzf RSeQC-2.5.tar.gz
cd RSeQC-2.5
python setup.py install(说python版本要2.7）

没法看，估计是bed的chr1等没有chr，利用
perl -e 'open IN,"SWM2_accepted_hits_sorted.bed";open OUT,">> 123.bed";while (<IN>) {chomp;@a=split /\t/;print OUT"chr$a[0]\t$a[1]\t$a[2]\t$a[3]\t$a[4]\t$a[5]\n";}'
加入chr
重新转化为bed_graph

$genomeCoverageBed -bg -i 123.bed -g /Work1/home/wangdong/hg19/ChromInfo.txt > SWM2_accepted_hits_sorted_change.bedgraph
bedGraphToBigWig SWM2_accepted_hits_sorted_change.bedgraph /Work1/home/wangdong/hg19/ChromInfo.txt SWM2_change.bigwig

bedtools是source李洋的
在http://hgdownload.cse.ucsc.edu/admin/exe/linux.x86_64/下面下载bedGraphToBigWig，传到/Share/home/wangdong/apps/bin下
chmod 777 bedGraphToBigWig
将/Share/home/wangdong/apps/bin加入bash_rc, source
直接enter之便可以用了
bedGraphToBigWig
bedGraphToBigWig v 4 - Convert a bedGraph file to bigWig format.
usage:
   bedGraphToBigWig in.bedGraph chrom.sizes out.bw
where in.bedGraph is a four column file in the format:
      <chrom> <start> <end> <value>
and chrom.sizes is two column: <chromosome name> <size in bases>
and out.bw is the output indexed big wig file.
Use the script: fetchChromSizes to obtain the actual chrom.sizes information
from UCSC, please do not make up a chrom sizes from your own information.
The input bedGraph file must be sorted, use the unix sort command:
  sort -k1,1 -k2,2n unsorted.bedGraph > sorted.bedGraph
options:
   -blockSize=N - Number of items to bundle in r-tree.  Default 256
   -itemsPerSlot=N - Number of data points bundled at lowest level. Default 1024
   -unc - If set, do not use compression.

c2_c3_nc操作过程
 1036  perl -e 'open IN,"SWM2_accepted_hits_sorted.bed";open OUT,">> 123.bed";while (<IN>) {chomp;@a=split /\t/;print OUT"chr$a[0]\t$a[1]\t$a[2]\t$a[3]\t$a[4]\t$a[5]\n";}'
 1037  ll
 1038  le 123.bed 
 1039  ll
 1040  ll
 1041  ll
 1042  ls
 1043  cd ..
 1044  ls
 1045  cd SWC2_1
 1046  ls
 1047  ll
 1048  bamToBed -i accepted_hits.bam >> SWC2_accepted_hits.bed
 1049  perl -e 'open IN,"SWC2_accepted_hits.bed";open OUT,">> 123.bed";while (<IN>) {chomp;@a=split /\t/;print OUT"chr$a[0]\t$a[1]\t$a[2]\t$a[3]\t$a[4]\t$a[5]\n";}'
 1050  genomeCoverageBed -bg -i 123.bed -g /Work1/home/wangdong/hg19/ChromInfo.txt > SWC2_accepted_hits_sorted_change.bedgraph
 1051  bedGraphToBigWig SWC2_accepted_hits_sorted_change.bedgraph /Work1/home/wangdong/hg19/ChromInfo.txt SWC2_change.bigwig
 1052  ll
 1053  cd ..
 1054  ls
 1055  cd SWC3_1
 1056  LS
 1057  ls
 1058  bamToBed -i accepted_hits.bam >> SWC3_accepted_hits.bed
 1059  perl -e 'open IN,"SWC3_accepted_hits.bed";open OUT,">> 123.bed";while (<IN>) {chomp;@a=split /\t/;print OUT"chr$a[0]\t$a[1]\t$a[2]\t$a[3]\t$a[4]\t$a[5]\n";}'
 1060  genomeCoverageBed -bg -i 123.bed -g /Work1/home/wangdong/hg19/ChromInfo.txt > SWC3_accepted_hits_sorted_change.bedgraph
 1061  bedGraphToBigWig SWC3_accepted_hits_sorted_change.bedgraph /Work1/home/wangdong/hg19/ChromInfo.txt SWC3_change.bigwig
 1062  cd ..
 1063  ls
 1064  cd SWNC_1/
 1065  ls
 1066  ll
 1067  bamToBed -i accepted_hits.bam >> SWNC_accepted_hits.bed
 1068  perl -e 'open IN,"SWNC_accepted_hits.bed";open OUT,">> 123.bed";while (<IN>) {chomp;@a=split /\t/;print OUT"chr$a[0]\t$a[1]\t$a[2]\t$a[3]\t$a[4]\t$a[5]\n";}'
 1069  genomeCoverageBed -bg -i 123.bed -g /Work1/home/wangdong/hg19/ChromInfo.txt > SWNC_accepted_hits_sorted_change.bedgraph
 1070  bedGraphToBigWig SWNC_accepted_hits_sorted_change.bedgraph /Work1/home/wangdong/hg19/ChromInfo.txt SWNC.bigwig
bamtobed
bamToBed -i accepted_hits.bam >> SWM2_accepted_hits.bed
le SWM2_accepted_hits.bed
sort -k 1,1 SWM2_accepted_hits.bed > SWM2_accepted_hits_sorted.bed #-k 1,1(start pos__end_pos)
genomeCoverageBed
曾输入为bam，错误信息比较多(genomeCoverageBed -bg -ibam wgEncodeCshlLongRnaSeq/wgEncodeCshlLongRnaSeqK562ChromatinTotalAlnRep4.bam.sort -g hg19.chromInfo -strand + >K562-Chromatin-POS-4.bedgraph)
Input error: Chromosome 19 found in non-sequential lines. This suggests that the input file is not sorted correc
所以改输入为bed,经过上面的转化，排序，然后genomeCoverageBed进行bed转化为bedgraph
genomeCoverageBed -bg -i SWM2_accepted_hits_sorted.bed -g /Work1/home/wangdong/ensemble/ChromInfo.txt > SWM2_accepted_hits_sorted.bedgraph #那个染色体信息必须要与自己的bed一致，如果是chr1，就都chr1(/Work1/home/wangdong/hg19/ChromInfo.txt)
但现在的bed头为1，所以用这个
$genomeCoverageBed -bg -i SWM2_accepted_hits_sorted.bed -g /Work1/home/wangdong/ensemble/ChromInfo.txt > SWM2_accepted_hits_sorted.bedgraph 
$le SWM2_accepted_hits_sorted.bedgraph 
1       10017   10041   1
1       10041   10068   2
1       10068   10092   1
1       11593   11644   2
1       11696   11708   1
1       11708   11747   2
1       11747   11759   1
1       11761   11805   1
1       11805   11812   2
1       11812   11854   1
1       11854   11856   4
1       11856   11904   3
1       11904   11905   4
1       11905   11929   1
1       11929   11955   2

可成功进行转化
bedGraphToBigWig SWM2_accepted_hits_sorted.bedgraph /Work1/home/wangdong/ensemble/ChromInfo.txt SWM2_accepted_hits_sorted.bed_graph.bigwig
b
ig_wig二进制文件
"SWM2_accepted_hits_sorted.bed_graph.bigwig" may be a binary file

流程大致为
bamToBed -i accepted_hits.bam >> SWM2_accepted_hits.bed然后sort -k 1,1 SWM2_accepted_hits.bed > SWM2_accepted_hits_sorted.bed然后 genomeCoverageBed -bg -i SWM2_accepted_hits_sorted.bed -g /Work1/home/wangdong/ensemble/ChromInfo.txt > SWM2_accepted_hits_sorted.bedgraph然后bedGraphToBigWig SWM2_accepted_hits_sorted.bedgraph /Work1/home/wangdong/ensemble/ChromInfo.txt SWM2_accepted_hits_sorted.bed_graph.bigwig，成功的到了bigwig，可是我的bigwig放在ucsc上啥也没有呀，



被人的流程如下（上面的有学下面的）
Data Processing
BAM sort

samtools sort wgEncodeCshlLongRnaSeq/wgEncodeCshlLongRnaSeqK562ChromatinTotalAlnRep4.bam wgEncodeCshlLongRnaSeq/wgEncodeCshlLongRnaSeqK562ChromatinTotalAlnRep4.bam.sort
Genome Coverage

I refer to the standard manual of BEDtools, I'll use forward strand as example, and the reverse strand signal is generated in the same way.

genomeCoverageBed -bg -ibam wgEncodeCshlLongRnaSeq/wgEncodeCshlLongRnaSeqK562ChromatinTotalAlnRep4.bam.sort -g hg19.chromInfo -strand + >K562-Chromatin-POS-4.bedgraph

Note that I've used -strand flag to separate the two strands.
bedgraphtoBigWig

bedGraphToBigWig executive script available from UCSC exe list （http://hgdownload.cse.ucsc.edu/admin/exe/linux.x86_64/）各种软件均有

bedGraphToBigWig K562-Chromatin-POS-4.bedgraph hg19.chromInfo K562-Chromatin-POS-4.bigwig

20141209可以直接用 configureBclToFastq
configureBclToFastq.pl --input-dir /Work1/home/wangdong/lss/20141021_bcl/141021_C00126_0147_AHA4C4ADXX/Data/Intensities/BaseCalls/ --output ./Unaligned_test2 --sample-sheet /Work1/home/wangdong/lss/20141021_bcl/SampleSheet_141021.csv --no-eamss --use-bases-mask Y51,I6n,Y51
ls
cd Unaligned_test2
ls
nohup make -j 8


GO/KEGG富集分析enrichment analysis
2014年01月26日 ? Bioinformatics ? 字号 小 中 大 ? 评论 1 条 ? 阅读 1,384 次 [点击加入在线收藏夹]

本教程使用GOstats来对给定的基因symbols做富集分析。

> library("org.Hs.eg.db")
> library("GSEABase")
> library("GOstats")
> genes <- c("AREG", "FKBP5", "CXCL13", "KLF9", "ZC3H12A", "P4HA1", "TLE1", "CREB3L2", "TXNIP", "PBX1", "GJA1", "ITGB8", "CCL3", "CCND2", "KCNJ15", "CFLAR", "CXCL10", "CYSLTR1", "IGFBP7", "RHOB", "MAP3K5", "CAV2", "CAPN2", "AKAP13", "RND3", "IL6ST", "RGS1", "IRF4", "G3BP1", "SEL1L", "VEGFA", "SMAD1", "CCND1", "CLEC3B", "NEB", "AMD1", "PDCD4", "SCD", "TM2D3", "BACH2", "LDLR", "BMPR1B", "RFXAP", "ASPH", "PTK2B", "SLC1A5", "ENO2", "TRPM8", "SATB1", "MIER1", "SRSF1", "ATF3", "CCL5", "MCM6", "GCH1", "CAV1", "SLC20A1")
> ##GO BP enrichment analysis
> goAnn <- get("org.Hs.egGO")
> universe <- Lkeys(goAnn)
> head(universe)
[1] "1"         "10"        "100"       "1000"      "10000"     "100008586"
> entrezIDs <- mget(genes, org.Hs.egSYMBOL2EG, ifnotfound=NA)
> head(entrezIDs)
$AREG
[1] "374"
 
$FKBP5
[1] "2289"
 
$CXCL13
[1] "10563"
 
$KLF9
[1] "687"
 
$ZC3H12A
[1] "80149"
 
$P4HA1
[1] "5033"
 
> entrezIDs <- as.character(entrezIDs)
> head(entrezIDs)
[1] "374"   "2289"  "10563" "687"   "80149" "5033" 
> params <- new("GOHyperGParams",
+                  geneIds=entrezIDs,
+                  universeGeneIds=universe,
+                  annotation="org.Hs.eg.db",
+                  ontology="BP",
+                  pvalueCutoff=0.01,
+                  conditional=FALSE,
+                  testDirection="over")
> over <- hyperGTest(params)
> library(Category)
> glist <- geneIdsByCategory(over)
> glist <- sapply(glist, function(.ids) {
+ 	.sym <- mget(.ids, envir=org.Hs.egSYMBOL, ifnotfound=NA)
+ 	.sym[is.na(.sym)] <- .ids[is.na(.sym)]
+ 	paste(.sym, collapse=";")
+ 	})
> head(glist)
                                                                                                                                                                                         GO:0002690 
                                                                                                                                                              "PTK2B;CXCL10;CCL3;CCL5;VEGFA;CXCL13" 
                                                                                                                                                                                         GO:0002688 
                                                                                                                                                              "PTK2B;CXCL10;CCL3;CCL5;VEGFA;CXCL13" 
                                                                                                                                                                                         GO:0048518 
"RHOB;ASPH;ATF3;CCND1;BMPR1B;CAV1;CAV2;CCND2;PTK2B;GJA1;IL6ST;CXCL10;IRF4;LDLR;SMAD1;MAP3K5;PBX1;RFXAP;CCL3;CCL5;SLC20A1;TLE1;CLEC3B;VEGFA;CFLAR;CXCL13;TXNIP;CYSLTR1;AKAP13;MIER1;CREB3L2;ZC3H12A" 
                                                                                                                                                                                         GO:1901623 
                                                                                                                                                                           "PTK2B;CCL3;CCL5;CXCL13" 
                                                                                                                                                                                         GO:0002687 
                                                                                                                                                              "PTK2B;CXCL10;CCL3;CCL5;VEGFA;CXCL13" 
                                                                                                                                                                                         GO:2000403 
                                                                                                                                                                           "PTK2B;CCL3;CCL5;CXCL13" 
> bp <- summary(over)
> bp$Symbols <- glist[as.character(bp$GOBPID)]
> head(bp)
      GOBPID       Pvalue OddsRatio    ExpCount Count Size                                        Term
1 GO:0002690 3.490023e-08  39.24490  0.19267044     6   52 positive regulation of leukocyte chemotaxis
2 GO:0002688 9.278865e-08  32.80297  0.22601725     6   61          regulation of leukocyte chemotaxis
3 GO:0048518 1.170957e-07   4.28014 13.56103476    32 3660   positive regulation of biological process
4 GO:1901623 1.176400e-07 128.80174  0.04816761     4   13         regulation of lymphocyte chemotaxis
5 GO:0002687 1.496978e-07  30.05918  0.24454325     6   66  positive regulation of leukocyte migration
6 GO:2000403 2.969857e-07  96.58170  0.05928321     4   16 positive regulation of lymphocyte migration
                                                                                                                                                                                            Symbols
1                                                                                                                                                               PTK2B;CXCL10;CCL3;CCL5;VEGFA;CXCL13
2                                                                                                                                                               PTK2B;CXCL10;CCL3;CCL5;VEGFA;CXCL13
3 RHOB;ASPH;ATF3;CCND1;BMPR1B;CAV1;CAV2;CCND2;PTK2B;GJA1;IL6ST;CXCL10;IRF4;LDLR;SMAD1;MAP3K5;PBX1;RFXAP;CCL3;CCL5;SLC20A1;TLE1;CLEC3B;VEGFA;CFLAR;CXCL13;TXNIP;CYSLTR1;AKAP13;MIER1;CREB3L2;ZC3H12A
4                                                                                                                                                                            PTK2B;CCL3;CCL5;CXCL13
5                                                                                                                                                               PTK2B;CXCL10;CCL3;CCL5;VEGFA;CXCL13
6                                                                                                                                                                            PTK2B;CCL3;CCL5;CXCL13
> dim(bp)
[1] 525   8
> ##KEGG enrichment analysis
> keggAnn <- get("org.Hs.egPATH")
> universe <- Lkeys(keggAnn)
> params <- new("KEGGHyperGParams", 
+                     geneIds=entrezIDs, 
+                     universeGeneIds=universe, 
+                     annotation="org.Hs.eg.db", 
+                     categoryName="KEGG", 
+                     pvalueCutoff=0.01,
+                     testDirection="over")
> over <- hyperGTest(params)
> kegg <- summary(over)
> library(Category)
> glist <- geneIdsByCategory(over)
> glist <- sapply(glist, function(.ids) {
+ 	.sym <- mget(.ids, envir=org.Hs.egSYMBOL, ifnotfound=NA)
+ 	.sym[is.na(.sym)] <- .ids[is.na(.sym)]
+ 	paste(.sym, collapse=";")
+ 	})
> kegg$Symbols <- glist[as.character(kegg$KEGGID)]
> kegg
  KEGGID       Pvalue OddsRatio ExpCount Count Size                                   Term                                    Symbols
1  04510 9.643801e-05  7.873256 1.124361     7  200                         Focal adhesion    CCND1;CAPN2;CAV1;CAV2;CCND2;ITGB8;VEGFA
2  04060 5.487123e-04  5.821855 1.489779     7  265 Cytokine-cytokine receptor interaction BMPR1B;IL6ST;CXCL10;CCL3;CCL5;VEGFA;CXCL13
3  04062 3.739699e-03  5.486219 1.062521     5  189            Chemokine signaling pathway              PTK2B;CXCL10;CCL3;CCL5;CXCL13
> library("pathview")
> gIds <- mget(genes, org.Hs.egSYMBOL2EG, ifnotfound=NA)
> gEns <- unlist(gIds)
> gene.data <- rep(1, length(gEns))
> names(gene.data) <- gEns
> for(i in 1:3){pv.out <- pathview(gene.data, pathway.id=as.character(kegg$KEGGID)[i], species="hsa"


无root权限解决lib××× not found，以及Linux上设置良好的目录结构
本文帮助你解决在无root权限的linux系统上安装软件时候遇到的lib××× not found的问题，并推荐一个Linux上良好的目录结构。

1. 安装软件到自己的软件目录。

缺乏的lib×××库大多都能在网上下载到源码，可自己下载安装。

下图是我在服务器上的目录结构，软件都安装在～/local/app里面，在~/local/bin里面分别建立软链接指向所安装软件的可执行文件；如果该软件里面的可执行文件太多，方便的做法是讲其所在目录加入到环境变量$PATH中。
2. 设置环境变量

有的软件安装后只生成可执行文件，有的则产生一些库文件和头文件，则需要将其添加到相应环境变量中；share目录等可忽略。如下所示
# 可执行文件 export PATH=$HOME/local/app/bin:$PATH # 静态链接库 export LIBRARY_PATH=$HOME/local/app/libevent/lib:$LIBRARY_PATH # 动态链接库 export LD_LIBRARY_PATH=$HOME/local/app/libevent/lib:$LD_LIBRARY_PATH # gcc头文件 export C_INCLUDE_PATH=$HOME/local/app/libevent/include:$C_INCLUDE_PATH # g++头文件 export CPLUS_INCLUDE_PATH=$HOME/local/app/libevent/include:$CPLUS_INCLUDE_PATH
	
# 可执行文件
export PATH=$HOME/local/app/bin:$PATH
# 静态链接库
export LIBRARY_PATH=$HOME/local/app/libevent/lib:$LIBRARY_PATH
# 动态链接库
export LD_LIBRARY_PATH=$HOME/local/app/libevent/lib:$LD_LIBRARY_PATH
# gcc头文件
export C_INCLUDE_PATH=$HOME/local/app/libevent/include:$C_INCLUDE_PATH
# g++头文件
export CPLUS_INCLUDE_PATH=$HOME/local/app/libevent/include:$CPLUS_INCLUDE_PATH

注意：等号前面不要有空格。本例中，～/local/app/libevent/lib中包含了动态和静态链接库，不确定编译器类型，故加入到gcc和g++头文件搜索目录中。

怎样改变使UltraEdit有多个窗口出来
视图’――‘视图/列表’――‘打开文件标签


解决fastqc不能运行的问题
重新下载安装jdk,并且将安装位置local/java加入~/.bashrc
还是报找不到libjli.so的错，于是将新安装的/Share/home/wangdong/apps/src/jdk1.8.0_25/lib/i386/jli下面的这个库加到~/.bash_profile里
发现报找不到libjava.so，于是将自己安装的/Share/home/wangdong/apps/src/jdk1.8.0_25/jre/lib/i386的库，以及locate libjli.so libjava.s的所得信息全部一并加入
~/.bash_profile，上面同时也报Error: Could not find Java SE Runtime Environment，这是因为没有把安装的那个bin加入~/.bashrc
（java runtime environment 就是JRE，要么就是你没装JDK（java development kit），JDK包含了JRE，装完JDK就不要装JRE了，也有可能你没配置环境变量到JDK安装目录下的bin）
所以将/Share/home/wangdong/apps/src/jdk1.8.0_25/bin:\加入~/.bashrc
并且要加载x
 /Share/home/wangdong/local/java:\前面
 因为jdk是环境，后者是执行文件
LD_LIBRARY_PATH=/Share/home/wangdong/apps/src/jdk1.8.0_25/bin:\
/Share/home/wangdong/apps/src/jdk1.8.0_25/lib/i386/jli:\
/Share/home/wangdong/apps/src/jdk1.8.0_25/jre/lib/i386:\
/home/ibm/lsf8.3_lsfinstall/lap/jre/ibm-java-x86_64-60/jre/lib/amd64/jli:\
/opt/lsf83/8.3/install/lap/jre/ibm-java-x86_64-60/jre/lib/amd64/jli:\
/opt/pac83-new/jre/linux-x86_64/lib/amd64/jli:\
/usr/java/jre1.7.0_25/lib/amd64/jli:\
/usr/lib/jvm/java-1.6.0-openjdk-1.6.0.0.x86_64/jre/lib/amd64/jli:\
/home/ibm/lsf8.3_lsfinstall/lap/jre/ibm-java-x86_64-60/jre/lib/amd64:\
/opt/lsf83/8.3/install/lap/jre/ibm-java-x86_64-60/jre/lib/amd64:\
/opt/pac83-new/jre/linux-x86_64/lib/amd64:\
/usr/java/jre1.7.0_25/lib/amd64:\
/usr/lib/jvm/java-1.6.0-openjdk-1.6.0.0.x86_64/jre/lib/amd64
export LD_LIBRARY_PATH

最后输入java就可以发现好使了，然后运行fastqc，一切OK
/Share/home/wangdong/apps/src/jdk1.8.0_25/bin:\
$fastqc -o ./SW_24_R2 CTTGTA.R1.fastq

.bash_profile和.bashrc的什么区别
/etc/profile:此文件为系统的每个用户设置环境信息,当用户第一次登录时,该文件被执行.
并从/etc/profile.d目录的配置文件中搜集shell的设置.
/etc/bashrc:为每一个运行bash shell的用户执行此文件.当bash shell被打开时,该文件被读取.
~/.bash_profile:每个用户都可使用该文件输入专用于自己使用的shell信息,当用户登录时,该
文件仅仅执行一次!默认情况下,他设置一些环境变量,执行用户的.bashrc文件.
~/.bashrc:该文件包含专用于你的bash shell的bash信息,当登录时以及每次打开新的shell时,该
该文件被读取.
~/.bash_logout:当每次退出系统(退出bash shell)时,执行该文件. 

另外,/etc/profile中设定的变量(全局)的可以作用于任何用户,而~/.bashrc等中设定的变量(局部)只能继承/etc/profile中的变量,他们是"父子"关系.
 
~/.bash_profile 是交互式、login 方式进入 bash 运行的
~/.bashrc 是交互式 non-login 方式进入 bash 运行的
通常二者设置大致相同，所以通常前者会调用后者。 
我们在ubuntu图形界面下用eclipse写了一个动态库，到centos下调用时出现错误，   

error while loading shared libraries: libmysqlclientso.so.0: cannot open shared object file: No such file or directory

以为没装mysql-client，因为ubuntu下叫这个，但是centos下直接就叫mysql，服务器版本叫mysql-server，查找了一下libmysqlclient.so

find / -name libmysqlclient.so，果然发现不同：

这是因为没有把动态链接库的安装路径（例如说是 /usr/local/lib ）放到变量 LD_LIBRARY_PATH 里。

这时，可以用命令 export 来临时测试确认是不是这个问题：

export LD_LIBRARY_PATH=/usr/local/lib

在终端里运行上面这行命令，再运行这个可执行文件，如果运行正常就说明是这个问题。

接下来的问题是：以上做法，只是临时设置变量 LD_LIBRARY_PATH ，下次开机，一切设置将不复存在；如何把这个值持续写到 LD_LIBRARY_PATH 里呢？

我们可以在 ~/.bashrc 或者 ~/.bash_profile 中加入 export 语句，前者在每次登陆和每次打开 shell 都读取一次，后者只在登陆时读取一次。我的习惯是加到 ~/.bashrc 中，在该文件的未尾，可采用如下语句来使设置生效：

export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/lib

修改完后，记得关掉当前终端并重新打开一个新的终端，从而使上面的配置生效。



关于函数库和软件安装的相关问题请教
Hi Shasha,

  Yeah, we also have this kind of issue in our server, generally to solve this problem you need to add the path of your software to the PATH environment variable.
For example, suppose your FastQC is in $HOME/software/FastQC
Then, in the $HOME/.bashrc file add the line:
export PATH=$HOME/software/FastQC:$PATH

For the libraries (xx.so) you need to add it to the LD_LIBRARY_PATH environment variable

export LD_LIBRARY_PATH=$HOME/software/FastQC/lib:LD_LIBRARY_PATH

For me generally I install all the software in $HOME/.local

when installing a software you can configure it to be on the .local file, so it will put all the bin and library files there.

./configure --prefix=$HOME/.local

for python packages, you can install them using

python setup.py install --user


Then in the $HOME/.bashrc file I add the following lines

export PATH=$HOME/.local/bin:$HOME
export LD_LIBRARY_PATH=$HOME/.local/lib:$LD_LIBRARY_PATH

Hope it helps,
You are welcome any time :),

Regards,

-Nadhir
通过一下命令进行library路径设置


另外，如果不想每次新启一个shell都设置LD_LIBRARY_PATH，可以编辑~/.bash_profile文件：

$ vi ~/.bash_profile 

添加：

LD_LIBRARY_PATH=/usr/local/lib

export LD_LIBRARY_PATH

这两行，完成之后.bash_profile如下所示：

 

# .bash_profile

# Get the aliases and functions

if [ -f ~/.bashrc ]; then

        . ~/.bashrc

fi

# User specific environment and startup programs

PATH=$PATH:$HOME/bin

LD_LIBRARY_PATH=/usr/local/lib

export PATH

export LD_LIBRARY_PATH
然后运行
$ source ~/.bash_profile 就行了。


vi ~/.bash_profile 

探针放的顺序：d在a后面

运行fastqc,报下面的错误
java：error while loading shared libraries: libjli.so: cannot open shared object file: No such file or directory 
export LD_LIBRARY_PATH=/usr/lib/jvm/java-1.6.0-openjdk-1.6.0.0.x86_64/jre/lib/amd64/jli:$LD_LIBRARY_PATH
source /etc/profile
然后java可以出现了，但是还是会报下面的错
同时fastqc无法enter出来，所以需要source一下~/.bashrc
java: error while loading shared libraries: libjli.so: wrong ELF class: ELFCLASS64
网上显示是系统位数不对，可能改为32为就好了

LD_LIBRARY_PATH=/home/ibm/lsf8.3_lsfinstall/lap/jre/ibm-java-x86_64-60/jre/lib/amd64/jli
/Share/home/wangdong/local/java

ln -s /Share/home/wangdong/local/java/javac /usr/bin/javac
ln -s /Share/home/wangdong/local/java/jar /usr/bin/jar

ln -s /Share/home/wangdong/local/java/javac /usr/bin/java

整个过程的详细说明
https://wikis.utexas.edu/display/bioiteam/Differential+gene+expression+analysis#Differentialgeneexpressionanalysis-Analyzedifferentialgeneexpression

HTseq运行
samtools sort -n accepted_hits.bam bamsorted
samtools view -help
samtools view -h bamsorted.bam > SWNC.sam
htseq-count --help
htseq-count -s no SWNC.sam /Work1/home/wangdong/ensemble/genes.gtf > SWNC_final.count

C2
samtools sort -n accepted_hits.bam bamsorted
samtools view -h bamsorted.bam > SWC2.sam
htseq-count -s no SWC2.sam /Work1/home/wangdong/ensemble/genes.gtf > SWC2.count

C3
samtools sort -n accepted_hits.bam bamsorted
samtools view -h bamsorted.bam > SWC3.sam
htseq-count -s no SWC3.sam /Work1/home/wangdong/ensemble/genes.gtf > SWC3.count
m1
samtools sort -n accepted_hits.bam bamsorted
samtools view -h bamsorted.bam > SWM1.sam
htseq-count -s no SWM1.sam /Work1/home/wangdong/ensemble/genes.gtf > SWM1_2.count
M2
samtools sort -n accepted_hits.bam bamsorted
samtools view -h bamsorted.bam > SWM2.sam
htseq-count -s no SWM2.sam /Work1/home/wangdong/ensemble/genes.gtf > SWM2.count

将SW*.count几个文件做一下merge

安装库是一件十分繁琐的事情,依赖关系的等十分的麻烦,前面的方法十分重要,但是希望有个整合的版本 那就是
(https://store.continuum.io/cshop/anaconda/) 这个版本整合了大量的常用库 ,下载 
直接bash Anaconda-2.1.0-Linux-x86_64.sh  按照提示,一步步安装即可 ,安装完成后基本库就都已经装好了 十分方便

python-leveldb的安装,从(https://pypi.python.org/pypi/leveldb)下载,但是用4.9.1的gcc无法编译通过,用系统自带的gcc能编译过,python setup.py  build ----- python setup.py  install

python-protobuf的安装,从我们前面已经装过的protobuf中,有个python的文件夹,python setup.py  build ----- python setup.py  install就可以了

python-gflags的安装,从(https://pypi.python.org/pypi/python-gflags)下载,解压python setup.py  build ----- python setup.py  install就可以了

HTseq安装
一下为李洋的运行历史
  927  ./configure --help
  928  R/
  929  R
  930  ls
  931  cd src/
  932  ls
  933  ll
  934  cd
  935  ls
  936  cd apps/
  937  ls
  938  cd src/
  939  ls
  940  ls
  941  ll
  942  chmod 755 Anaconda-2.1.0-Linux-x86_64.sh 
  943  bash Anaconda-2.1.0-Linux-x86_64.sh 
  944  cd
  945  ls
  946  cd apps/
  947  pwd
  948  cd src/
  949  ls
  950  bash Anaconda-2.1.0-Linux-x86_64.sh 
  951  cd
  952  ls
  953  vi .bashrc
  954  which python
  955  vi .bashrc
  956  source .bashrc
  957  which python
  958  vi .bashrc
  959  source .bashrc
  960  which python
  961  python
  962  vi .bash_profile 
  963  vi .bashrc
  964  echo $PATH
  965  vi .bashrc
  966  source .bashrc
  967  echo $PATH
  968  which python
  969  cd
  970  ls
  971  cd 
  972  ls
  973  vi .bashrc
  974  /usr/bin/vim .bashrc
  975  source .bashrc
  976  echo $PATH
  977  which python

先下载Anaconda-2.1.0-Linux-x86_64.sh
 chmod 755 Anaconda-2.1.0-Linux-x86_64.sh 
bash Anaconda-2.1.0-Linux-x86_64.sh进行编译
yes选择安装
yes选择将路径加入PATH里

既可以自动导入一些库，不用重新安装，
此时which python就应该在~/anaconda/bin/python下
此时可以进行HTSEQ安装了
解压，cd 
 python setup.py build
 python setup.py install
 将/Share/home/wangdong/local/app/HTSeq-0.6.1/build/scripts-2.6:\加入bashrc中即可
 htseq-count就有了
然后直接利用python setup.py build进行build，然后python setup.py install进行安装即可
然后python
import HTSeq证明安装好了
一下为李洋的运行过程

/Share/home/wangdong/apps/anaconda/bin/python
 cd apps/
 1011  ls
 1012  cd src/
 1013  ls
 1014  cd HTSeq-0.6.1
 1015  LS
 1016  LS
 1017  ls
 1018  python setup.py build
 1019  python setup.py install
 1020  cd
 1021  ls
 1022  python
 1023  history

gFOLD安装
同济GFOLD软件是一款根据mapping结果直接进行差异表达分析的软件，http://www.tongji.edu.cn/~zhanglab/GFOLD/index.html，文献中提到，该软件在分析无重复的转录组数据的时候，不以p值为计算依据，而是以GFOLD值作为标准筛选差异表达基因，命令也比较简单。国产软件还是支持一下。
结果要优于其他DESeq、edgeR、cuffldiff等软件，故在此决定安装该软件。
在安装该软件之前需要先安装GSL的一款基于GNU的数值计算工具（个人这么认为，不管怎么样要安装）。
这是GSL的官方介绍http://www.gnu.org/software/gsl/
首先下载下来 这个比较简单我就不说了 wget命令
tar解压
cd 进入目录
进去以后参考INSTALL中说明，里面会把遇到的一些常见问题列出方便解决。但是基本步骤就是
>./confiure
>make
>make check
>make install
一般来说会把库文件 和lib文件安装在usr/local下的include和lib下 ，记住这两个目录方便后面使用。
上面步骤一般来说不会报错，所以就到了下一步。安装GFOLD软件

从上述网站下载GFOLD的安装包，同样wget
解压，进入目录，可以先打开READEME看一下，需要改环境变量，没错这里就蛋疼了，可以的话推荐手动改，
linux系统直接改~/.bashrc文件
在里面加上这两句
export CXXFLAGS="-g -O3 -I/usr/local/include -L/~/usr/local/lib";
export LD_LIBRARY_PATH="~/usr/local/lib:"$LD_LIBRARY_PATH
注意-I 和-L后面没有空格，不知道同济的开发者怎么想的 我试了半天，才搞定。一般来说这里改完就ok了。
然后就是编译：make
很喜人，这里没有报错，gfold文件终于生成了。
但是这里结束了么，我错，问题没有ok。
经常装软件的人一般都会装好习惯下的测试运行一下，你猜对了，我测试运行出错了哦。

./gfold: error while loading shared libraries: libgsl.so.0: cannot open shared object file: No such file or directory

这是报错的信息，我表示不解，我明明都装好，路径也设对了，但是这个就是找不到。
于是google神器爆发找到了答案，原因是在GSL装好以后可能会出现共享lib找不到的情况，这个是系统造成的，解决方法是手动的重新更改系统变量 设置LD_LIBRARY_PATH。

> LD_LIBRARY_PATH=/usr/local/lib
> export LD_LIBRARY_PATH

 /Work1/home/wangdong/lje/cufflinks-2.1.1.Linux_x86_64/cuffmerge -g /Work1/home/wangdong/ensemble/genes.gtf -s /Work1/home/wangdong/ensemble/genome.fa -p 4 /Work1/home/wangdong/lss/20141021/Undetermined_indices/sw4/RNA_seq/again/assemblies.txt
 library(ggplot2)
  4 library(scales)
  5 library(reshape2)
  6 library(hgu95av2.db)
  7 pdf(file="pri_finalres.pdf", height=15, width=20)
  8 data<-read.csv("./c2_nc_heat.csv",head=T)
  9 data<-data.matrix(data)
 10 hc<-hclust(dist(data))
 11 rowInd<-hc$order
 12 hc<-hclust(dist(t(data)))
 13 colInd<-hc$order
 14 data.m<-data[rowInd,]
 15 data.m<-apply(data.m,1,rescale)
 16 data.m<-t(data.m)
 17 coln<-colnames(data.m)
 18 rown<-rownames(data.m)
 19 colnames(data.m)<-1:ncol(data.m)
 20 rownames(data.m)<-1:nrow(data.m)
 21 data.m<-melt(data.m)
 22 head(data.m)
 23 base_size<-12
 24 (p <- ggplot(data.m, aes(Var2,Var1)) + geom_tile(aes(fill = value),colour = "white") + scale_fill_gradient(low = "green", high = "red")) + theme_grey(base_size = base_size)+ labs(x = "",y = "") + s    cale_x_continuous(expand = c(0,0),labels=coln,breaks=1:length(coln)) + scale_y_continuous(expand = c(0, 0),labels=rown,breaks=1:length(rown)) + theme(axis.ticks = element_blank(), axis.text.x = ele    ment_text(size = base_size *0.6, angle = 90, hjust = 0, colour = "grey50"),  axis.text.y = element_text(size = base_size * 0.6, hjust=1, colour="grey50"))
 25 #pdf(file="finalresult", height=40, width=50)
 26 dev.off()

我现在的任务都是qw状态，能查找这种状态出现的原因吗？？
【活跃】gyfeng-葛云峰(285570784) 2014-11-20 10:09:59
qstat -j +作业ID

> mydata <- read.csv("s1_s2_3.csv")
> tiff("s1_s2_3.tiff")
> plot(mydata$splint0.1,mydata$splint0.2,lty=2,pch=20,cex=0.5,xlab="log10(splint0.1_normalization)",ylab="log10(splint0.2_normalization)")
> dev.off()
null device 
          1 
> 

> mydata <- read.csv("c2_c3.csv")
> tiff("c2_c3_edit.tiff")
> plot(mydata$ctnnb1_shRNA1_rpkm,mydata$ctnnb1_shRNA2_rpkm,lty=2,pch=20,cex=0.5,xlab="log10(ctnnb1_shRNA1_rpkm)",ylab="log10(ctnnb1_shRNA2_rpkm)")
> dev.off()
null device 
          1 

m1_m2
> mydata <- read.csv("m1_m2_com.csv")
> tiff("m1_m2_final.tiff")
> plot(mydata$c_myc_shRNA1,mydata$c_myc_shRNA2,lty=2,pch=20,cex=0.5,xlab="log10(c_myc_shRNA1_rpkm)",ylab="log10(c_myc_shRNA2_rpkm)")
> dev.off()
null device 
          1 
> q() 

C2_NC
> mydata <- read.csv("C2_NC.csv")
> tiff("C2_nc_final.tiff")
> plot(mydata$CTNNB1_shRNA1_RPKM,mydata$NC_RPKM,lty=2,pch=20,cex=0.5,xlab="log10(ctnnb1_shRNA1_rpkm)",ylab="log10(NC_rpkm)")
> dev.off()
null device 
          1 
> q()

C3_NC
> mydata <- read.csv("C3_NC.csv")
> tiff("C3_NC_final.tiff")
> plot(mydata$CTNNB1_shRNA2_RPKM,mydata$NC_RPKM,lty=2,pch=20,cex=0.5,xlab="log10(ctnnb1_shRNA2_rpkm)",ylab="log10(NC_rpkm)")
> dev.off()
null device 
          1 
M1_NC
 > mydata <- read.csv("m1_NC.csv")
> tiff("m1_nc_final.tiff")
> plot(mydata$myc_shRNA1_RPKM,mydata$NC_RPKM,lty=2,pch=20,cex=0.5,xlab="log10(c_myc_shRNA1_rpkm)",ylab="log10(NC_rpkm)") 
> dev.off()
null device 
          1 

M2_NC
plot(mydata$myc_shRNA2_RPKM,mydata$NC_RPKM,lty=2,pch=20,cex=0.5,xlab="log10(c_myc_shRNA2_rpkm)",ylab="log10(NC_rpkm)")    
任务提交bjobs -l
[wangdong@mgt /Work1/home/wangdong/lss/20141021/Undetermined_indices/sw4/RNA_seq/cuffdiff/com]$bjobs -l

Job <484207>, Job Name <cuffdiff_test>, User <wangdong>, Project <default>, Sta
                     tus <RUN>, Queue <TINY>, Command <#!/bin/sh;#BSUB -J cuffd
                     iff_test;#BSUB -o cuffdiff_test.o%J;#BSUB -e cuffdiff_test
                     .e%J;#BSUB -n 4;#BSUB -R "span[hosts=1]"; # Environment;so
                     urce ./source.txt; # JOBS;echo -ne "Start cuffdiff_test at
                      ... ";date; /Share/home/wangdong/local/bin/cuffdiff -o /W
                     ork1/home/wangdong/lss/20141021/Undetermined_indices/sw4/R
                     NA_seq/cuffdiff/c1_c2/cuff_diff2 -b /Work1/home/wangdong/e
                     nsemble/genome.fa -L SWC2,SWC3 /Work1/home/wangdong/lss/20
                     141021/Undetermined_indices/sw4/RNA_seq/cuffdiff/c1_c2/mer
                     ged_asm/merged.gtf /Work1/home/wangdong/lss/20141021/Undet
                     ermined_indices/sw4/RNA_seq/SWC2_1/accepted_hits.bam /Work
                     1/home/wangdong/lss/20141021/Undetermined_indices/sw4/RNA_
                     seq/SWC3_1/accepted_hits.bam;  echo -ne "End cuffdiff_test
                      at ";date>
Tue Nov 18 15:03:32: Submitted from host <mgt>, CWD </Share/home/wangdong/pipel
                     ine/RNA_finder/bin>, Output File <cuffdiff_test.o%J>, Erro
                     r File <cuffdiff_test.e%J>, 4 Processors Requested, Reques
                     ted Resources <span[hosts=1]>;
Tue Nov 18 15:03:34: Started on 4 Hosts/Processors <4*node04>, Execution Home <
                     /Share/home/wangdong/>, Execution CWD </Share/home/wangdon
                     g/pipeline/RNA_finder/bin>;
Wed Nov 19 15:41:20: Resource usage collected.
                     The CPU time used is 88632 seconds.
                     MEM: 3 Gbytes;  SWAP: 3 Gbytes;  NTHREAD: 6
                     PGID: 11149;  PIDs: 11149 11150 11154 11157 


 SCHEDULING PARAMETERS:
           r15s   r1m  r15m   ut      pg    io   ls    it    tmp    swp    mem
 loadSched   -     -     -     -       -     -    -     -     -      -      -  
 loadStop    -     -     -     -       -     -    -     -     -      -      -  

          adapter_windows     poe nrt_windows 
 loadSched             -       -           -  
 loadStop              -       -           -
 
将两个样品放在一起做cuffmerge，保证用同一个注释标准
/Share/home/wangdong/local/bin/cuffmerge -g /Work1/home/wangdong/ensemble/genes.gtf -s /Work1/home/wangdong/ensemble/genome.fa -p 4 /Work1/home/wangdong/lss/20141021/Undetermined_indices/sw4/RNA_seq/cuffdiff/c1_c2/assemblies.txt
/Share/home/wangdong/local/bin/cuffmerge -g /Work1/home/wangdong/ensemble/genes.gtf -s /Work1/home/wangdong/ensemble/genome.fa -p 4 /Work1/home/wangdong/lss/20141021/Undetermined_indices/sw4/RNA_seq/cuffdiff/m1_m2/assemblies.txt
重新做一个cuff_diff
/Share/home/wangdong/local/bin/cuffdiff -o /Work1/home/wangdong/lss/20141021/Undetermined_indices/sw4/RNA_seq/cuffdiff/c1_c2/cuff_diff -b /Work1/home/wangdong/ensemble/genome.fa -L SWC2,SWC3 /Work1/home/wangdong/lss/20141021/Undetermined_indices/sw4/RNA_seq/cuffdiff/c1_c2/merged_asm/merged.gtf /Work1/home/wangdong/lss/20141021/Undetermined_indices/sw4/RNA_seq/SWC2_1/accepted_hits.bam /Work1/home/wangdong/lss/20141021/Undetermined_indices/sw4/RNA_seq/SWC3_1/accepted_hits.bam
/Share/home/wangdong/local/bin/cuffdiff -o /Work1/home/wangdong/lss/20141021/Undetermined_indices/sw4/RNA_seq/cuffdiff/m1_m2/cuff_diff -b /Work1/home/wangdong/ensemble/genome.fa -L SWM1,SWM2 /Work1/home/wangdong/lss/20141021/Undetermined_indices/sw4/RNA_seq/cuffdiff/m1_m2/merged_asm/merged.gtf /Work1/home/wangdong/lss/20141021/Undetermined_indices/sw4/RNA_seq/SWM1_1/accepted_hits.bam /Work1/home/wangdong/lss/20141021/Undetermined_indices/sw4/RNA_seq/SWM2_1/accepted_hits.bam
 biocLite(c("cummeRbund"))
for i in /Share/home/wangdong/.Work1/lss/20141021/Undetermined_indices/sw4/RNA_seq/SW*/accepted_hits.bam; do echo $i; samtools index $i ; done;
for i in /Share/home/wangdong/.Work1/lss/20141021/Undetermined_indices/sw4/RNA_seq/SW*/accepted_hits.bam; do echo $i; samtools idxstats $i ; done;
R下载网址
http://cran.r-project.org/bin/windows/base/old/
更新
biocLite("BiocUpgrade")
http://bioconductor.org/packages/3.0/bioc/src/contrib/GenomicAlignments_1.2.1.tar.gz'
'http://bioconductor.org/packages/3.0/bioc/src/contrib/Rsamtools_1.18.1.tar.gz'
 'http://bioconductor.org/packages/3.0/bioc/src/contrib/VariantAnnotation_1.12.3.tar.gz'
 
trying URL 'http://cran.fhcrc.org/src/contrib/RCurl_1.95-4.3.tar.gz'
'http://bioconductor.org/packages/3.0/bioc/src/contrib/Rsamtools_1.18.1.tar.gz'
 'http://bioconductor.org/packages/3.0/bioc/src/contrib/GenomicAlignments_1.2.1.tar.gz'
 'http://bioconductor.org/packages/3.0/bioc/src/contrib/biomaRt_2.22.0.tar.gz'
 'http://bioconductor.org/packages/3.0/bioc/src/contrib/GenomicFeatures_1.18.2.tar.gz'
  'http://bioconductor.org/packages/3.0/bioc/src/contrib/BSgenome_1.34.0.tar.gz'
   'http://bioconductor.org/packages/3.0/bioc/src/contrib/biovizBase_1.14.0.tar.gz'
   trying URL 'http://bioconductor.org/packages/3.0/bioc/src/contrib/rtracklayer_1.26.1.tar.gz'
Content type 'application/x-gzip' length 1338692 bytes (1.3 Mb)
opened URL
==================================================
downloaded 1.3 Mb

trying URL 'http://bioconductor.org/packages/3.0/bioc/src/contrib/Gviz_1.10.2.tar.gz'
Content type 'application/x-gzip' length 2550604 bytes (2.4 Mb)
trying URL 'http://bioconductor.org/packages/3.0/bioc/src/contrib/cummeRbund_2.8.2.tar.gz'
Content type 'application/x-gzip' length 2313860 bytes (2.2 Mb)

关于解压
.tar.gz的，用tar zxvf
.tar.bz2的，用tar jxvf
.tar的，用tar xvf
 filename.gz
 gzip -d filename.gz （或gunzip filename.gz） 解压，但要注意，用此命令解压，会删除原文件。（filename换成相应文件名）

如果想保留原文件，用
zcat filename.gz > filename

abi的sra数据转化为fastq
fastq-dump得到数值
solidstandard的一个perl程序可以将数字转化为fastq
BCL2FASTQ报错
[wangdong@mgt /Work1/home/wangdong/lss/20141022/whl_sw]$sh 7_index.sh
[2014-10-23 00:49:42]   [configureBclToFastq.pl]        INFO: Creating directory '/Work1/home/wangdong/lss/20141022/whl_sw/Unaligned'
could not find ParserDetails.ini in /usr/lib/perl5/site_perl/5.8.8/XML/SAX
[2014-10-23 00:49:43]   [configureBclToFastq.pl]        INFO: Basecalling software: RTA
[2014-10-23 00:49:43]   [configureBclToFastq.pl]        INFO:              version: 1.18 (build 61)
[2014-10-23 00:49:43]   [configureBclToFastq.pl]        INFO: Original use-bases mask: Y51,I7n,Y51
[2014-10-23 00:49:43]   [configureBclToFastq.pl]        INFO: Guessed use-bases mask: YYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYY,IIIIIIIn,YYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYY
[2014-10-23 00:49:43]   [configureBclToFastq.pl]        WARNING: SampleSheet flowcell ID 'H9RYYADXX' does not match. Expected: 'HA4C4ADXX'
Creating directory '/Work1/home/wangdong/lss/20141022/whl_sw/Unaligned/Project_H9RYYADXX/Sample_B4'
Creating directory '/Work1/home/wangdong/lss/20141022/whl_sw/Unaligned/Project_H9RYYADXX/Sample_C1'
Creating directory '/Work1/home/wangdong/lss/20141022/whl_sw/Unaligned/Project_H9RYYADXX/Sample_C2'
Creating directory '/Work1/home/wangdong/lss/20141022/whl_sw/Unaligned/Project_H9RYYADXX/Sample_B1'
Creating directory '/Work1/home/wangdong/lss/20141022/whl_sw/Unaligned/Project_H9RYYADXX/Sample_C3'
Creating directory '/Work1/home/wangdong/lss/20141022/whl_sw/Unaligned/Project_H9RYYADXX/Sample_B8'
Creating directory '/Work1/home/wangdong/lss/20141022/whl_sw/Unaligned/Project_H9RYYADXX/Sample_SWS2'
Creating directory '/Work1/home/wangdong/lss/20141022/whl_sw/Unaligned/Project_H9RYYADXX/Sample_B2'
Creating directory '/Work1/home/wangdong/lss/20141022/whl_sw/Unaligned/Project_H9RYYADXX/Sample_SWT4'
Creating directory '/Work1/home/wangdong/lss/20141022/whl_sw/Unaligned/Project_H9RYYADXX/Sample_SWS1'
Creating directory '/Work1/home/wangdong/lss/20141022/whl_sw/Unaligned/Project_H9RYYADXX/Sample_B3'
Creating directory '/Work1/home/wangdong/lss/20141022/whl_sw/Unaligned/Project_H9RYYADXX/Sample_C6'
Creating directory '/Work1/home/wangdong/lss/20141022/whl_sw/Unaligned/Undetermined_indices/Sample_lane1'
Creating directory '/Work1/home/wangdong/lss/20141022/whl_sw/Unaligned/Project_H9RYYADXX/Sample_B5'
Creating directory '/Work1/home/wangdong/lss/20141022/whl_sw/Unaligned/Project_H9RYYADXX/Sample_B6'
Creating directory '/Work1/home/wangdong/lss/20141022/whl_sw/Unaligned/Project_H9RYYADXX/Sample_B7'
Creating directory '/Work1/home/wangdong/lss/20141022/whl_sw/Unaligned/Project_H9RYYADXX/Sample_C4'
Creating directory '/Work1/home/wangdong/lss/20141022/whl_sw/Unaligned/Project_H9RYYADXX/Sample_C5'
Creating directory '/Work1/home/wangdong/lss/20141022/whl_sw/Unaligned/Undetermined_indices/Sample_lane2'
[2014-10-23 00:49:43]   [configureBclToFastq.pl]        INFO: Read 1: length = 51: mask = YYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYY
[2014-10-23 00:49:43]   [configureBclToFastq.pl]        WARNING: use-base-mask length is 8 for read 2: expected length from config file is 7
[2014-10-23 00:49:43]   [configureBclToFastq.pl]        INFO: Read 2: length = 8: mask = IIIIIIIn
[2014-10-23 00:49:43]   [configureBclToFastq.pl]        WARNING: Read 3: current cycle in mask is 60: first cycle in read is 59
[2014-10-23 00:49:43]   [configureBclToFastq.pl]        INFO: Read 3: length = 51: mask = YYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYY
[2014-10-23 00:49:44]   [configureBclToFastq.pl]        WARNING: use-base-mask length is 8 for read 2: expected length from config file is 7
[2014-10-23 00:49:44]   [configureBclToFastq.pl]        WARNING: Read 3: current cycle in mask is 60: first cycle in read is 59
[2014-10-23 00:49:44]   [configureBclToFastq.pl]        INFO: Running self tests: 'make self_test'
[2014-10-23 00:49:46]   [configureBclToFastq.pl]        WARNING:
output of 'make self_test':

mkdir -p Basecall_Stats_HA4C4ADXX || ( sleep 5 && mkdir -p Basecall_Stats_HA4C4ADXX ); touch Basecall_Stats_HA4C4ADXX/.sentinel
[2014-10-23 00:49:46]   [mgt]   [Basecall_Stats_HA4C4ADXX/.sentinel]    make Target:    Basecall_Stats_HA4C4ADXX/.sentinel
[2014-10-23 00:49:46]   [mgt]   [Basecall_Stats_HA4C4ADXX/.sentinel]    make Reason:
[2014-10-23 00:49:46]   [mgt]   [Basecall_Stats_HA4C4ADXX/.sentinel]    make Prereqs:
[2014-10-23 00:49:46]   [mgt]   [Basecall_Stats_HA4C4ADXX/.sentinel]    make Cmd:       mkdir -p Basecall_Stats_HA4C4ADXX || ( sleep 5 && mkdir -p Basecall_Stats_HA4C4ADXX ); touch Basecall_Stats_HA4C4ADXX/.sentinel
/Share/home/lulab1/users/liyang/apps/bcl2fastq-1.8.4/libexec/bcl2fastq-1.8.4/BaseCalls/copyConfig.pl --input-file /Work1/home/wangdong/lss/20141022/141021_C00126_0147_AHA4C4ADXX/Data/Intensities/BaseCalls/config.xml --tiles "" > Basecall_Stats_HA4C4ADXX/config.xml.tmp && mv Basecall_Stats_HA4C4ADXX/config.xml.tmp Basecall_Stats_HA4C4ADXX/config.xml
[2014-10-23 00:49:46]   [mgt]   [Basecall_Stats_HA4C4ADXX/config.xml]   make Target:    Basecall_Stats_HA4C4ADXX/config.xml
[2014-10-23 00:49:46]   [mgt]   [Basecall_Stats_HA4C4ADXX/config.xml]   make Reason:    /Work1/home/wangdong/lss/20141022/141021_C00126_0147_AHA4C4ADXX/Data/Intensities/BaseCalls/config.xml Basecall_Stats_HA4C4ADXX/.sentinel
[2014-10-23 00:49:46]   [mgt]   [Basecall_Stats_HA4C4ADXX/config.xml]   make Prereqs:   /Work1/home/wangdong/lss/20141022/141021_C00126_0147_AHA4C4ADXX/Data/Intensities/BaseCalls/config.xml Basecall_Stats_HA4C4ADXX/.sentinel
[2014-10-23 00:49:46]   [mgt]   [Basecall_Stats_HA4C4ADXX/config.xml]   make Cmd:       /Share/home/lulab1/users/liyang/apps/bcl2fastq-1.8.4/libexec/bcl2fastq-1.8.4/BaseCalls/copyConfig.pl --input-file /Work1/home/wangdong/lss/20141022/141021_C00126_0147_AHA4C4ADXX/Data/Intensities/BaseCalls/config.xml --tiles "" > Basecall_Stats_HA4C4ADXX/config.xml.tmp && mv Basecall_Stats_HA4C4ADXX/config.xml.tmp Basecall_Stats_HA4C4ADXX/config.xml
[2014-10-23 00:49:46]   [mgt]   [Basecall_Stats_HA4C4ADXX/config.xml]   could not find ParserDetails.ini in /usr/lib/perl5/site_perl/5.8.8/XML/SAX
xsltproc \
          --stringparam DEMUX_READS_PARAM '1 2' \
          --stringparam READ_DEMUX_FIRST_CYCLES_PARAM '1 52' \
          --stringparam READ_DEMUX_LAST_CYCLES_PARAM '51 102' \
          /Share/home/lulab1/users/liyang/apps/bcl2fastq-1.8.4/share/bcl2fastq-1.8.4/ExcludeReadFromBustardConfig.xsl Basecall_Stats_HA4C4ADXX/config.xml > DemultiplexedBustardConfig.xml.tmp && mv DemultiplexedBustardConfig.xml.tmp DemultiplexedBustardConfig.xml
[2014-10-23 00:49:46]   [mgt]   [DemultiplexedBustardConfig.xml]        make Target:    DemultiplexedBustardConfig.xml
[2014-10-23 00:49:46]   [mgt]   [DemultiplexedBustardConfig.xml]        make Reason:    Basecall_Stats_HA4C4ADXX/config.xml
[2014-10-23 00:49:46]   [mgt]   [DemultiplexedBustardConfig.xml]        make Prereqs:   Basecall_Stats_HA4C4ADXX/config.xml
[2014-10-23 00:49:46]   [mgt]   [DemultiplexedBustardConfig.xml]        make Cmd:       xsltproc  --stringparam DEMUX_READS_PARAM '1 2'  --stringparam READ_DEMUX_FIRST_CYCLES_PARAM '1 52'  --stringparam READ_DEMUX_LAST_CYCLES_PARAM '51 102'  /Share/home/lulab1/users/liyang/apps/bcl2fastq-1.8.4/share/bcl2fastq-1.8.4/ExcludeReadFromBustardConfig.xsl Basecall_Stats_HA4C4ADXX/config.xml > DemultiplexedBustardConfig.xml.tmp && mv DemultiplexedBustardConfig.xml.tmp DemultiplexedBustardConfig.xml
[2014-10-23 00:49:46]   [mgt]   [DemultiplexedBustardConfig.xml]        /bin/bash: line 4:  2391 Segmentation fault      xsltproc --stringparam DEMUX_READS_PARAM '1 2' --stringparam READ_DEMUX_FIRST_CYCLES_PARAM '1 52' --stringparam READ_DEMUX_LAST_CYCLES_PARAM '51 102' /Share/home/lulab1/users/liyang/apps/bcl2fastq-1.8.4/share/bcl2fastq-1.8.4/ExcludeReadFromBustardConfig.xsl Basecall_Stats_HA4C4ADXX/config.xml > DemultiplexedBustardConfig.xml.tmp
make: *** [DemultiplexedBustardConfig.xml] Error 139


Self test command exited with error 2 (signal 0)

Investigate and fix errors, then retry
[2014-10-23 00:49:46]   [configureBclToFastq.pl]        Self test command exited with error 2 (signal 0)
[2014-10-23 00:49:46]   [configureBclToFastq.pl]        BACKTRACE:  at /Share/home/lulab1/users/liyang/apps/bcl2fastq-1.8.4/lib/bcl2fastq-1.8.4/perl/Casava/Demultiplex.pm line 871.
        Casava::Demultiplex::selfTest(Casava::Demultiplex=HASH(0x2037a6d0), "/Work1/home/wangdong/lss/20141022/whl_sw/Unaligned") called at /Share/home/lulab1/users/liyang/apps/bcl2fastq-1.8.4/bin/configureBclToFastq.pl line 438
Died at /Share/home/lulab1/users/liyang/apps/bcl2fastq-1.8.4/lib/bcl2fastq-1.8.4/perl/Casava/Common/Log.pm line 310.
: command not found
nohup: appending output to `nohup.out'
应该是该包放的地方路径不对

/usr/lib/perl5/vendor_perl/5.8.8/XML/SAX/ParserDetails.ini
/usr/lib/perl5/site_perl/5.8.8/XML/SAX
export PKG_CONFIG_PATH=/Share/home/wangdong/lib/lib/pkgconfig:$PKG_CONFIG_PATH
$echo $PATH (打印输出)
将/usr/lib/perl5/vendor_perl/5.8.8/XML/SAX加入PATH后无这个错，但是下面的错还在


/opt/mysql/bin:/opt/lsf83/8.3/linux2.6-glibc2.3-x86_64/etc:/opt/lsf83/8.3/linux2.6-glibc2.3-x86_64/bin:/opt/xcat/bin:/opt/xcat/sbin:/usr/kerberos/sbin:/usr/kerberos/bin:/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin:/root/bin:/usr/lpp/mmfs/bin:/usr/lpp/mmfs/bin:/Share/home/wangdong/apps/sratoolkit.2.3.5-2-centos_linux64/bin:/Share/home/wangdong/apps/samtools-0.1.17:/Share/home/wangdong/local/bin:/Work1/home/wangdong/lje/cufflinks-2.1.1.Linux_x86_64:/Work1/home/wangdong/lje/cutadapt-1.3/bin:/Share/home/wangdong/pipeline/RNA_finder/blast/ncbi-blast-2.2.29+/bin:/Work1/home/wangdong/tool_package/bzip2-1.0.6:/Share/home/wangdong/apps/src:/Share/home/wangdong/.Work1/lje/bowtie2-2.1.0:/Share/home/wangdong/local/java:/Share/home/wangdong/apps/src/FastQC:/Share/home/wangdong/local/lib:/usr/local/bin:/Share/home/wangdong/apps/src/ncbi-blast-2.2.29+/bin:/share/home/wangdong/apps/src/libgtextutils-0.1:/Share/home/wangdong/apps/bin:/etc/apache/bin

20141102 邵伟RNA_SEQ分析
/Share/home/wangdong/local/bin/tophat -p 4 -o /Work1/home/wangdong/lss/20141021/Undetermined_indices/sw4/RNA_seq/SWC2_1 /Work1/home/wangdong/lss/mm9/index /Work1/home/wangdong/lss/20141021/Undetermined_indices/sw4/SWC2_1_t.fastq;
/Share/home/wangdong/local/bin/tophat -p 4 -o /Work1/home/wangdong/lss/20141021/Undetermined_indices/sw4/RNA_seq/SWC3_1 /Work1/home/wangdong/lss/mm9/index /Work1/home/wangdong/lss/20141021/Undetermined_indices/sw4/SWC3_1_t.fastq;
/Share/home/wangdong/local/bin/tophat -p 4 -o /Work1/home/wangdong/lss/20141021/Undetermined_indices/sw4/RNA_seq/SWM1_1 /Work1/home/wangdong/lss/mm9/index /Work1/home/wangdong/lss/20141021/Undetermined_indices/sw4/SWM1_1_t.fastq;
/Share/home/wangdong/local/bin/tophat -p 4 -o /Work1/home/wangdong/lss/20141021/Undetermined_indices/sw4/RNA_seq/SWM2_1 /Work1/home/wangdong/lss/mm9/index /Work1/home/wangdong/lss/20141021/Undetermined_indices/sw4/SWM2_1_t.fastq;
/Share/home/wangdong/local/bin/tophat -p 4 -o /Work1/home/wangdong/lss/20141021/Undetermined_indices/sw4/RNA_seq/SWNC_1 /Work1/home/wangdong/lss/mm9/index /Work1/home/wangdong/lss/20141021/Undetermined_indices/sw4/SWNC_1_t.fastq;
报错1
[2014-11-02 20:41:52] Beginning TopHat run (v2.0.13)
-----------------------------------------------
[2014-11-02 20:41:52] Checking for Bowtie
/Work1/home/wangdong/lje/bowtie2-2.1.0/bowtie2-align: /usr/lib64/libstdc++.so.6: version `GLIBCXX_3.4.11' not found (required by /Work1/home/wangdong/lje/bowtie2-2.1.0/bowtie2-align)
/Work1/home/wangdong/lje/bowtie2-2.1.0/bowtie2-align: /usr/lib64/libstdc++.so.6: version `GLIBCXX_3.4.9' not found (required by /Work1/home/wangdong/lje/bowtie2-2.1.0/bowtie2-align)
Died at /Work1/home/wangdong/lje/bowtie2-2.1.0/bowtie2 line 75.
Traceback (most recent call last):
  File "/Share/home/wangdong/local/bin/tophat", line 4088, in ?
    sys.exit(main())
  File "/Share/home/wangdong/local/bin/tophat", line 3885, in main
    check_bowtie(params)
  File "/Share/home/wangdong/local/bin/tophat", line 1512, in check_bowtie
    bowtie_version = get_bowtie_version()
  File "/Share/home/wangdong/local/bin/tophat", line 1385, in get_bowtie_version
    bowtie_out = stdout_value.splitlines()[0]
IndexError: list index out of range
重新安装bowtie2.2.3.0解决了(估计均由于函数库更新导致)

报错2
[2014-11-02 21:43:11] Beginning TopHat run (v2.0.13)                                                                                                            
-----------------------------------------------                                                                                                                 
[2014-11-02 21:43:11] Checking for Bowtie                                                                                                                       
                  Bowtie version:        2.2.3.0                                                                                                                
[2014-11-02 21:43:11] Checking for Bowtie index files (genome)..                                                                                                
[2014-11-02 21:43:11] Checking for reference FASTA file                                                                                                         
[2014-11-02 21:43:11] Generating SAM header for /Work1/home/wangdong/lss/mm9/index                                                                              
[2014-11-02 21:43:23] Preparing reads                                                                                                                           
        [FAILED]                                                                                                                                                 
Error running 'prep_reads'                                                                                                                                      
/Share/home/wangdong/local/bin/prep_reads: /usr/lib64/libstdc++.so.6: version `GLIBCXX_3.4.11' not found (required by /Share/home/wangdong/local/bin/prep_reads)
重新安装tophat解决

cufflinks试运行
Warning: Your version of Cufflinks is not up-to-date. It is recommended that you upgrade to Cufflinks v2.2.1 to benefit from the most recent features and bug fixes (http://cufflinks.cbcb.umd.edu).
[16:05:53] Inspecting reads and determining fragment length distribution.
> Processed 1 loci.                            [*************************] 100%
Warning: Using default Gaussian distribution due to insufficient paired-end reads in open ranges.  It is recommended that correct parameters (--frag-len-mean and --frag-len-std-dev) be provided.
> Map Properties:
>       Normalized Map Mass: 92.00
>       Raw Map Mass: 92.00
>       Fragment Length Distribution: Truncated Gaussian (default)
>                     Default Mean: 200
>                  Default Std Dev: 80
[16:05:53] Assembling transcripts and estimating abundances.
> Processed 1 loci.                            [*************************] 100%
cuffmerge试运行
[wangdong@mgt /Work1/home/wangdong/lss/tophat_test/test_data/test4]$/Work1/home/wangdong/lje/cufflinks-2.1.1.Linux_x86_64/cuffmerge -g /Work1/home/wangdong/ensemble/genes.gtf -s /Work1/home/wangdong/ensemble/genome.fa -p 4 /Work1/home/wangdong/lss/tophat_test/test_data/test4/cuffmerge/assemblies.txt -o /Work1/home/wangdong/lss/tophat_test/test_data/test4/cuffmerge

[Tue Nov  4 16:16:45 2014] Beginning transcriptome assembly merge
-------------------------------------------

[Tue Nov  4 16:16:45 2014] Preparing output location ./merged_asm/
[Tue Nov  4 16:16:54 2014] Converting GTF files to SAM
[16:16:54] Loading reference annotation.
[16:16:54] Loading reference annotation.
[Tue Nov  4 16:16:54 2014] Quantitating transcripts
Warning: Your version of Cufflinks is not up-to-date. It is recommended that you upgrade to Cufflinks v2.2.1 to benefit from the most recent features and bug fixes (http://cufflinks.cbcb.umd.edu).
Command line:
cufflinks -o ./merged_asm/ -F 0.05 -g /Work1/home/wangdong/ensemble/genes.gtf -q --overhang-tolerance 200 --library-type=transfrags -A 0.0 --min-frags-per-transfrag 0 --no-5-extend -p 4 ./merged_asm/tmp/mergeSam_fileDgeYh1 
[bam_header_read] EOF marker is absent.
[bam_header_read] invalid BAM binary header (this is not a BAM file).
File ./merged_asm/tmp/mergeSam_fileDgeYh1 doesn't appear to be a valid BAM file, trying SAM...
[16:16:55] Loading reference annotation.
[16:17:15] Inspecting reads and determining fragment length distribution.
Processed 36885 loci.                       
> Map Properties:
>       Normalized Map Mass: 36886.00
>       Raw Map Mass: 36886.00
>       Fragment Length Distribution: Truncated Gaussian (default)
>                     Default Mean: 200
>                  Default Std Dev: 80
[16:17:16] Assembling transcripts and estimating abundances.

6:126102306-130463972   Warning: Skipping large bundle.
Processed 36884 loci.                       
[Tue Nov  4 16:20:30 2014] Comparing against reference file /Work1/home/wangdong/ensemble/genes.gtf
Warning: Your version of Cufflinks is not up-to-date. It is recommended that you upgrade to Cufflinks v2.2.1 to benefit from the most recent features and bug fixes (http://cufflinks.cbcb.umd.edu).
No fasta index found for /Work1/home/wangdong/ensemble/genome.fa. Rebuilding, please wait..
Fasta index rebuilt.
Warning: couldn't find fasta record for 'GL000191.1'!
Warning: couldn't find fasta record for 'GL000192.1'!
Warning: couldn't find fasta record for 'GL000193.1'!
Warning: couldn't find fasta record for 'GL000194.1'!
Warning: couldn't find fasta record for 'GL000195.1'!
Warning: couldn't find fasta record for 'GL000196.1'!
Warning: couldn't find fasta record for 'GL000199.1'!
Warning: couldn't find fasta record for 'GL000201.1'!
Warning: couldn't find fasta record for 'GL000204.1'!!
Warning: couldn't find fasta record for 'test_chromosome'!
[Tue Nov  4 16:23:27 2014] Comparing against reference file /Work1/home/wangdong/ensemble/genes.gtf
Warning: Your version of Cufflinks is not up-to-date. It is recommended that you upgrade to Cufflinks v2.2.1 to benefit from the most recent features and bug fixes (http://cufflinks.cbcb.umd.edu).
Warning: couldn't find fasta record for 'GL000191.1'!
Warning: couldn't find fasta record for 'GL000192.1'!
Warning: couldn't find fasta record for 'GL000193.1'!
Warning: couldn't find fasta record for 'GL000194.1'!
Warning: couldn't find fasta record for 'GL000195.1'!
Warning: couldn't find fasta record for 'GL000196.1'!
Warning: couldn't find fasta record for 'GL000199.1'!
Warning: couldn't find fasta record for 'GL000201.1'!
Warning: couldn't find fasta record for 'GL000204.1'!
Warning: couldn't find fasta record for 'GL000205.1'!
Warning: couldn't find fasta record for 'GL000209.1'!
Warning: couldn't find fasta record for 'GL000211.1'!
Warning: couldn't find fasta record for 'GL000212.1'!
Warning: couldn't find fasta record for 'GL000213.1'!
Warning: couldn't find fasta record for 'test_chromosome'!
但是运行结束依旧
RNA-seq运行过程
/pipeline/RNAFinder/bin下
tophat_sw.sh
cufflinks_sw.sh
每个样品与配对比较的样品之间做一个assemblies.txt，为cuffmerge做准备
cuffmerge（本地运行，bsub提交结果不知输出到哪里，估计为/pipeline/RNAFinder/bin）
/Work1/home/wangdong/lje/cufflinks-2.1.1.Linux_x86_64/cuffmerge -g /Work1/home/wangdong/ensemble/genes.gtf -s /Work1/home/wangdong/ensemble/genome.fa -p 4 /Work1/home/wangdong/lss/20141021/Undetermined_indices/sw4/RNA_seq/cuffmerge/SWC2/assemblies.txt
/Work1/home/wangdong/lje/cufflinks-2.1.1.Linux_x86_64/cuffmerge -g /Work1/home/wangdong/ensemble/genes.gtf -s /Work1/home/wangdong/ensemble/genome.fa -p 4 /Work1/home/wangdong/lss/20141021/Undetermined_indices/sw4/RNA_seq/cuffmerge/SWC3/assemblies.txt
/Work1/home/wangdong/lje/cufflinks-2.1.1.Linux_x86_64/cuffmerge -g /Work1/home/wangdong/ensemble/genes.gtf -s /Work1/home/wangdong/ensemble/genome.fa -p 4 /Work1/home/wangdong/lss/20141021/Undetermined_indices/sw4/RNA_seq/cuffmerge/SWM1/assemblies.txt
/Work1/home/wangdong/lje/cufflinks-2.1.1.Linux_x86_64/cuffmerge -g /Work1/home/wangdong/ensemble/genes.gtf -s /Work1/home/wangdong/ensemble/genome.fa -p 4 /Work1/home/wangdong/lss/20141021/Undetermined_indices/sw4/RNA_seq/cuffmerge/SWM2/assemblies.txt
cuffdiff运行
/Work1/home/wangdong/lje/cufflinks-2.1.1.Linux_x86_64/cuffdiff -o /Work1/home/wangdong/lss/20141021/Undetermined_indices/sw4/RNA_seq/cuffdiff/SWC2 -b /Work1/home/wangdong/ensemble/genome.fa -L SWC2,SWNC /Work1/home/wangdong/lss/20141021/Undetermined_indices/sw4/RNA_seq/cuffmerge/merged_asm_SWC2/merged.gtf /Work1/home/wangdong/lss/20141021/Undetermined_indices/sw4/RNA_seq/SWC2_1/accepted_hits.bam /Work1/home/wangdong/lss/20141021/Undetermined_indices/sw4/RNA_seq/SWNC_1/accepted_hits.bam
/Work1/home/wangdong/lje/cufflinks-2.1.1.Linux_x86_64/cuffdiff -o /Work1/home/wangdong/lss/20141021/Undetermined_indices/sw4/RNA_seq/cuffdiff/SWC3 -b /Work1/home/wangdong/ensemble/genome.fa -L SWC3,SWNC /Work1/home/wangdong/lss/20141021/Undetermined_indices/sw4/RNA_seq/cuffmerge/merged_asm_SWC3/merged.gtf /Work1/home/wangdong/lss/20141021/Undetermined_indices/sw4/RNA_seq/SWC3_1/accepted_hits.bam /Work1/home/wangdong/lss/20141021/Undetermined_indices/sw4/RNA_seq/SWNC_1/accepted_hits.bam
/Work1/home/wangdong/lje/cufflinks-2.1.1.Linux_x86_64/cuffdiff -o /Work1/home/wangdong/lss/20141021/Undetermined_indices/sw4/RNA_seq/cuffdiff/SWM1 -b /Work1/home/wangdong/ensemble/genome.fa -L SWM1,SWNC /Work1/home/wangdong/lss/20141021/Undetermined_indices/sw4/RNA_seq/cuffmerge/merged_asm_SWM1/merged.gtf /Work1/home/wangdong/lss/20141021/Undetermined_indices/sw4/RNA_seq/SWM1_1/accepted_hits.bam /Work1/home/wangdong/lss/20141021/Undetermined_indices/sw4/RNA_seq/SWNC_1/accepted_hits.bam
/Work1/home/wangdong/lje/cufflinks-2.1.1.Linux_x86_64/cuffdiff -o /Work1/home/wangdong/lss/20141021/Undetermined_indices/sw4/RNA_seq/cuffdiff/SWM2 -b /Work1/home/wangdong/ensemble/genome.fa -L SWM2,SWNC /Work1/home/wangdong/lss/20141021/Undetermined_indices/sw4/RNA_seq/cuffmerge/merged_asm_SWM2/merged.gtf /Work1/home/wangdong/lss/20141021/Undetermined_indices/sw4/RNA_seq/SWM2_1/accepted_hits.bam /Work1/home/wangdong/lss/20141021/Undetermined_indices/sw4/RNA_seq/SWNC_1/accepted_hits.bam
运行过程
Warning: Your version of Cufflinks is not up-to-date. It is recommended that you upgrade to Cufflinks v2.2.1 to benefit from the most recent features and bug fixes (http://cufflinks.cbcb.umd.edu).
[18:12:26] Loading reference annotation and sequence.
This contig will not be bias corrected.
Warning: couldn't find fasta record for 'HSCHR9_3_CTG35'!
This contig will not be bias corrected.
Warning: No conditions are replicated, switching to 'blind' dispersion method
[18:17:18] Inspecting maps and determining fragment length distributions.
[16:29:37] Modeling fragment count overdispersion.
> Map Properties:
>       Normalized Map Mass: 26054451.91
>       Raw Map Mass: 25086373.49
>       Fragment Length Distribution: Truncated Gaussian (default)
>                     Default Mean: 200
>                  Default Std Dev: 80
> Map Properties:
>       Normalized Map Mass: 26054451.91
>       Raw Map Mass: 27073412.27
>       Fragment Length Distribution: Truncated Gaussian (default)
>                     Default Mean: 200
>                  Default Std Dev: 80
[16:31:39] Calculating preliminary abundance estimates
processing locus步奏相当慢
Processed 43715 loci.                        [*************************] 100%
[20:50:23] Learning bias parameters.
 Testing for differential expression and regulation in locus.
> Processing Locus 1:206511376-206511708       [*                        ]  

$ ./configure
安装fastxtoolkit                         
自己安装                                 
libgtextutils-0.1                         $ make
ar -xjf libgtextutils-0.6.tar.bz2         
                                         
                                         $ sudo make install
 $ cd libgtextutils-0.6                  将/Share/home/wangdong/apps/src/libgtextutils-0.1
并且执行下面语句后，再对fastxtoolkit进行./configure即可运行configure了
export PKG_CONFIG_PATH=/Share/home/wangdong/lib/lib/pkgconfig:$PKG_CONFIG_PATH
cd fastx_toolkit-0.0.12
 

$ ./configure
$ make
$  make install


perl state.pl 需要将输入文件格式改为unix系统的
dos2unix
vi替换方法
:%s/原来的/替换成/g
/Share/home/lulab1/users/liyang/apps/bcl2fastq-1.8.4/bin:\

帅帅6个index,双端测序各为51个
configureBclToFastq.pl --input-dir ./Data/Intensities/BaseCalls/ --output ./Unaligned_test --sample-sheet ./SampleSheet.csv --no-eamss --use-bases-mask Y51,I6n,Y51

以前bcl2fastq数据位置和信息
/Share/home/wangdong/.Work1/mv_20140905/140822_C00126_0135_AH9RYYADXX

fastqc安装过程和运行过程
如安装的那个word说明书一样
只需要下载fastqc，解压，将fastqc这个文件 $chmod 755 fastqc
将这个文件夹加入到bashrc,source一下即可
但前提是要有一个1.6版本的java，及通过java -version就可以查看是不是有了，所以到java官网下载一下这个版本，安装java
java --version 
java
无，下载
tar xzvf jdk-8u25-linux-i586.tar.gz 
将Java里面一个bin里面所有文件拷到local/java文件夹中，再将local/java加入bashrc即可
cd /Share/home/wangdong/local/java
cp /Share/home/wangdong/apps/src/jdk1.8.0_25/bin/* ./
ls
java 
java -version
vi ~/.bashrc 
source ~/.bashrc
java -version
即可
fasqc运行
 fastqc -o ./ tss480_1_t.fastq tss480_2_t.fastq tss620_1_t.fastq tss620_2_t.fastq
$mkdir SRR1286228_1_qc
[wangdong@mgt /Work1/home/wangdong/lss/RNA_seq_web]$fastqc -o ./SRR1286228_1_qc SRR1286228_1.fastq
当然在运行fastqc时，后面必须要跟文件，最好给个输出目录，否则就会报下面的错
“No X11 DISPLAY variable was set
意思是没有图形化接入接口

/Share/home/wangdong/.Work1/ensemble(参考序列和GTF所在位置)

bsub提交任务
[wangdong@mgt /Share/home/wangdong/pipeline/RNA_finder/bin]$perl bsub.pl mapping_2.sh 1 TINY ./source.txt
C集群
perl qsub.pl -d(sh的文件夹如mapping_2.sh在的文件夹) -o(输出文件夹) 

李洋
fastq-dump SRR029215.sra
fastx_trimmer -l 25 -i SRR029215.sra -Q 33 -o SRR029215.fastq.process
bowtie -p 6 -S -q -m 1 --best --strata  ../indexes/c_elegans_ws220 SRR029215.fastq.process SRR029215_process.sam
samtools view SRR029215_process.sam >SRR029215_process.bam
 
macs14 -t SRR029217_process.bam -c SRR029215_process.bam -f BAM -g ce -n H3K9me3_N2 -m 10,30 - p 1e-5 --bw=300 -w -S
wignorm -t H3K9me3_N2_treat_afterfiting_all.wig -c H3K9me3_N2_control_afterfiting_all.wig -n H3K9me3_N2_norm --gsize=90000000


ncar上自己摸索安装
bowtie1（bowtie-0.12.7）,bowtie2（bowtie2-2.2.3）,,tophat(tophat-2.0.13.Linux_x86_64)
将包解压到apps/src，然后将里面所有可执行文件拷到local下面
samtools（samtools-0.1.17）需要make一下产生绿色的samtools，然后拷贝samtools,bcftools，mics里面的可执行文件到local下面
最后将export PATH=/home/lijine/local:$PATH
然后运行下面程序即可

 tophat -o test2 -r 20 test_ref reads_1.fq reads_2.fq 

[2014-10-16 18:23:22] Beginning TopHat run (v2.0.13)
-----------------------------------------------
[2014-10-16 18:23:22] Checking for Bowtie
                  Bowtie version:        2.2.3.0
[2014-10-16 18:23:22] Checking for Bowtie index files (genome)..
        Found both Bowtie1 and Bowtie2 indexes.
[2014-10-16 18:23:22] Checking for reference FASTA file
[2014-10-16 18:23:22] Generating SAM header for test_ref
[2014-10-16 18:23:22] Preparing reads
         left reads: min. length=75, max. length=75, 100 kept reads (0 discarded)
        right reads: min. length=75, max. length=75, 100 kept reads (0 discarded)
[2014-10-16 18:23:22] Mapping left_kept_reads to genome test_ref with Bowtie2 
[2014-10-16 18:23:22] Mapping left_kept_reads_seg1 to genome test_ref with Bowtie2 (1/3)
[2014-10-16 18:23:22] Mapping left_kept_reads_seg2 to genome test_ref with Bowtie2 (2/3)
[2014-10-16 18:23:22] Mapping left_kept_reads_seg3 to genome test_ref with Bowtie2 (3/3)
[2014-10-16 18:23:23] Mapping right_kept_reads to genome test_ref with Bowtie2 
[2014-10-16 18:23:23] Mapping right_kept_reads_seg1 to genome test_ref with Bowtie2 (1/3)
[2014-10-16 18:23:23] Mapping right_kept_reads_seg2 to genome test_ref with Bowtie2 (2/3)
[2014-10-16 18:23:23] Mapping right_kept_reads_seg3 to genome test_ref with Bowtie2 (3/3)
[2014-10-16 18:23:23] Searching for junctions via segment mapping
[2014-10-16 18:23:23] Retrieving sequences for splices
[2014-10-16 18:23:23] Indexing splices
Building a SMALL index
[2014-10-16 18:23:24] Mapping left_kept_reads_seg1 to genome segment_juncs with Bowtie2 (1/3)
[2014-10-16 18:23:24] Mapping left_kept_reads_seg2 to genome segment_juncs with Bowtie2 (2/3)
[2014-10-16 18:23:24] Mapping left_kept_reads_seg3 to genome segment_juncs with Bowtie2 (3/3)
[2014-t

illumina-dump: Convert SRA data into Illumina native formats (qseq, etc.)

prefetch: Allows command-line downloading of SRA, dbGaP, and ADSP data

sam-dump: Convert SRA data to sam format

sff-dump: Convert SRA data to sff format

sra-stat: Generate statistics about SRA data (quality distribution, etc.)

sra-pileup: Generate pileup statistics on aligned SRA data

vdb-config: Display and modify VDB configuration information

vdb-dump: Output the native VDB format of SRA data.

vdb-encrypt: Encrypt non-SRA dbGaP data ("phenotype data")

vdb-decrypt: Decrypt non-SRA dbGaP data ("phenotype data")

vdb-validate: Validate the integrity of downloaded SRA data
。主要有samtools（命令示例：samtools view -bS file1.sam -o file.bam)， bedtools （bamToFastq等 命令示例：bamToFastq -i file1.bam -fa file1.fastq)，sratools等，还有些软件自己附带的工具，如igvtools等等.
很多工具如tophat等还会基于samtools的API或者library，所以安装这些工具也是运行后续分析工具的前提
将安装软件加入PATH
vi ~/.bashrc
source  ~/.bashrc
查看PATH
echo $PATH
1)对于.tar结尾的文件
　　tar -xf all.tar
　　2)对于.gz结尾的文件
　　gzip -d all.gz
　　gunzip all.gz
　　3)对于.tgz或.tar.gz结尾的文件
　　tar -xzf all.tar.gz
　　tar -xzf all.tgz
　　4)对于.bz2结尾的文件
　　bzip2 -d all.bz2
　　bunzip2 all.bz2
　　5)对于tar.bz2结尾的文件
　　tar -xjf all.tar.bz2
　　6)对于.Z结尾的文件
　　uncompress all.Z
　　7)对于.tar.Z结尾的文件
　　tar -xZf all.tar.z

　　另外对于Window下的常见压缩文件.zip和.rar，Linux也有相应的方法来解压它
们：
　　1)对于.zip
　　linux下提供了zip和unzip程序，zip是压缩程序，unzip是解压程序。它们的参
数选项很多，这里只做简单介绍，依旧举例说明一下其用法：
　　# zip all.zip *.jpg
　　这条命令是将所有.jpg的文件压缩成一个zip包
# unzip all.zip
　　这条命令是将all.zip中的所有文件解压出来
　　2)对于.rar
　　要在linux下处理.rar文件，需要安装RAR for Linux，可以从网上下载，但要记住，RAR for Linux 不是免费的；可从http://www.rarsoft.com/download.htm下载RARfor Linux 3.2.
0，然后安装：
　　# tar -xzpvf rarlinux-3.2.0.tar.gz
　　# cd rar
　　# make
　　这样就安装好了，安装后就有了rar和unrar这两个程序，rar是压缩程序，unrar 是解压程序。它们的参数选项很多，这里只做简单介绍，依旧举例说明一下其用法：

　　# rar a all *.jpg
　　这条命令是将所有.jpg的文件压缩成一个rar包，名为all.rar，该程序会将.rar
扩展名将自动附加到包名后。
　　# unrar e all.rar
　　这条命令是将all.rar中的所有文件解压出来
　　到此为至，我们已经介绍过linux下的tar、gzip、gunzip、bzip2、bunzip2、compress 、 uncompress、 zip、unzip、rar、unrar等程式，你应该已经能够使用它们对.tar 、.gz、.tar.gz、.tgz、.bz2、.tar.bz2、. Z、.tar.Z、.zip、.rar这10种压缩文
件进行解压了，以后应该不需要为下载了一个软件而不知道如何在Linux下解开而烦恼了。而且以上方法对于Unix也基本有效。
　　本文介绍了linux下的压缩程式tar、gzip、gunzip、bzip2、bunzip2、compress 、uncompress、 zip、 unzip、rar、unrar等程式，以及如何使用它们对.tar、.gz 、.tar.gz、.tgz、.bz2、.tar.bz2、.Z、. tar.Z、.zip、.rar这10种压缩文件进行
操作。

以下补充

tar

-c: 建立压缩档案
-x：解压
-t：查看内容
-r：向压缩归档文件末尾追加文件
-u：更新原压缩包中的文件

这五个是独立的命令，压缩解压都要用到其中一个，可以和别的命令连用但只能用其中一个。下面的参数是根据需要在压缩或解压档案时可选的。

-z：有gzip属性的
-j：有bz2属性的
-Z：有compress属性的
-v：显示所有过程
-O：将文件解开到标准输出

下面的参数-f是必须的
-f: 使用档案名字，切记，这个参数是最后一个参数，后面只能接档案名。
# tar -cf all.tar *.jpg这条命令是将所有.jpg的文件打成一个名为all.tar的包。-c是表示产生新的包，-f指定包的文件名。
# tar -rf all.tar *.gif
这条命令是将所有.gif的文件增加到all.tar的包里面去。-r是表示增加文件的意思。
# tar -uf all.tar logo.gif
这条命令是更新原来tar包all.tar中logo.gif文件，-u是表示更新文件的意思。
# tar -tf all.tar
这条命令是列出all.tar包中所有文件，-t是列出文件的意思
# tar -xf all.tar
这条命令是解出all.tar包中所有文件，-x是解开的意思
压缩
tar Ccvf jpg.tar *.jpg //将目录里所有jpg文件打包成tar.jpg
tar Cczf jpg.tar.gz *.jpg //将目录里所有jpg文件打包成jpg.tar后，并且将其用gzip压缩，生成一个gzip压缩过的包，命名为jpg.tar.gz
tar Ccjf jpg.tar.bz2 *.jpg //将目录里所有jpg文件打包成jpg.tar后，并且将其用bzip2压缩，生成一个bzip2压缩过的包，命名为jpg.tar.bz2
tar CcZf jpg.tar.Z *.jpg //将目录里所有jpg文件打包成jpg.tar后，并且将其用compress压缩，生成一个umcompress压缩过的包，命名为jpg.tar.Z
rar a jpg.rar *.jpg //rar格式的压缩，需要先下载rar for linux
zip jpg.zip *.jpg //zip格式的压缩，需要先下载zip for linux

解压
tar Cxvf file.tar //解压 tar包
tar -xzvf file.tar.gz //解压tar.gz
tar -xjvf file.tar.bz2 //解压 tar.bz2
tar CxZvf file.tar.Z //解压tar.Z
unrar e file.rar //解压rar
unzip file.zip //解压zip

总结
1、*.tar 用 tar Cxvf 解压
2、*.gz 用 gzip -d或者gunzip 解压
3、*.tar.gz和*.tgz 用 tar Cxzf 解压
4、*.bz2 用 bzip2 -d或者用bunzip2 解压
5、*.tar.bz2用tar Cxjf 解压
6、*.Z 用 uncompress 解压
7、*.tar.Z 用tar CxZf 解压
8、*.rar 用 unrar e解压
9、*.zip 用 unzip 解压 
Bcl2fastq
configureBclToFastq.pl --input-dir ./Data/Intensities/BaseCalls/ --output /Share/home/wangdong/whl20140828_3/Unaligned --sample-sheet ./SampleSheet.csv --no-eamss --use-bases-mask Y51,I7n
cd /Share/home/wangdong/whl20140828/Unaligned
nohup make -j 8(或者查询李洋步骤cd $path0/Unaligned
screen
make -j 8 >$path0/Unaligned/log)

nohup和screen命令作用  

2009-08-10 22:42:17|  分类： linux |举报 |字号 订阅

通常linux的进程在父目录被KILL以后，就会接收到sighup命令，然后退出运行。
nohup和screen 都可以对sighup命令起到屏蔽作用。nohup比较小巧，screen命令比较强大。各有用处

nohup 命令对sighup信号有屏蔽作用，让命令可以持续运行。如果需要在终端关闭后还可以运行，
需在后面加 &
格式 nohup <command> [argument...]  &

screen 命令的的用法。
运行screen命令后会自动打开一个新shell，在这个新的shell里可以运行任何命令。
在临时离开，或者电脑关机后
如果你要直接退出这个shell，运行:
exit;
如果这个shell中运行的命令比较重要，可以先按:
crtl+a,然后按d键
临时退出这个shell终端，但任务会继续运行。
想重新连接这个screen  shell ,但是忘记了是那个screen ，用：
screen -ls 来查看screen info

然后重新连接，运行:
screen -r SCREENID

如果screen的进程打开的比较多，为了方便识别不同的screen，可以加上 -S 参数，给每个screen不同的
名称 比如 screen -S test ,进程里就会看到这个screen被标为 test.ttyn.host,而不是进程ID了

 1000  /Share/home/wangdong/pipeline/RNA_finder/bin
 1001  cd /Share/home/wangdong/pipeline/RNA_finder/bin
 1002  ld
 1003  ls
 1004  less qsub.pl 
 1005  ls
 1006  bsub
 1007  perl bsub.pl 
 1008  bqueues 
 1009  ls
 1010  bsub.pl /Share/home/wangdong/140822_C00126_0135_AH9RYYADXX/bcl2fq.sh 1
 1011  perl bsub.pl /Share/home/wangdong/140822_C00126_0135_AH9RYYADXX/bcl2fq.sh 1
 1012  bjobs 
 1013  ll
 1014  less bcl2fq.e405497
 1015  ls
 1016  touch source.txt
 1017  ls
 1018  vi source.txt 
 1019  ls
 1020  vi source.txt 
 1021  ls
 1022  vi source.txt 
 1023  perl bsub.pl
 1024  perl bsub.pl /Share/home/wangdong/140822_C00126_0135_AH9RYYADXX/bcl2fq.sh 1 TINY ./source.txt 
 1025  bjobs 
 1026  ls
 1027  ll
 1028  less bcl2fq.e405498
 1029  perl bsub.pl /Share/home/wangdong/140822_C00126_0135_AH9RYYADXX/bcl2fq.sh 1 TINY ./source.txt 
 1030  bjobs 
 1031  ls
 1032  ll
 1033  less bcl2fq.e405499
 1034  ls
 1035  bsub -h
 1036  cp /Share/home/wangdong/140822_C00126_0135_AH9RYYADXX/bcl2fq.sh ./
 1037  ls
 1038  bsub bcl2fq.sh 
 1039  bjobs 
 1040  bjobs 
 1041  bjobs 
 1042  bjobs 
 1043  history 

linux下如何编译安装bzip2

下载源文件安装包：

http://www.bzip.org/downloads.html

解压：

tar -xzvf bzip2-1.0.6.tar.gz

进入解压后的目录：

cd bzip2-1.0.6

为编译做准备，创建libbz2.so动态链接库(这一步很重要，安装python的时候如果没有这一步，python安装不上bz2模块)：

make -f Makefile-libbz2_so

编译&&安装：

make && make install

至此，大功告成！

ll -al查看隐藏文件
vi ~/.bashrc
source  ~/.bashrc

fq2fa
awk 'NR % 4 < 3 '  K_ATCTCGT_TOTAL.fastq | awk 'NR % 3 > 0 ' | perl -p -e 's/^@/>/g' > K_ATCTCGT_TOTAL.fasta
grep 部分不匹配
grep '[^TAG]TGAAGCCACAGATGTA' L_AAGCACT.fastq > test

 cat /proc/cpuinfo |grep "physical id"| sort| uniq| wc -1 2
每个物理CPU中core的个数(即核数):
 cat /proc/cpuinfo| grep "cpu cores"| uniq cpu cores　: 1 
逻辑CPU的个数:
 cat /proc/cpuinfo| grep "processor"| wc -l  4
gzip -d解压gz文件
UltraEdit如何将相同开头的的文本替换
如：将UltraEdit-1213，UltraEdit-12241，UltraEdit-adf全部替换为UltraEdit-haha!
2012-01-06 09:13 提问者采纳
1.如果后面有空格
将：UltraEdit-* 
替换为：UltraEdit-haha
注意，*后有空格

2.如果后面有回车，则改为
将：UltraEdit-*^p
替换为：UltraEdit-haha^p

 UltraEdit中如何删除含有特定内容的行/删除不含有特定内容的行
分类： 杂类 2010-01-06 21:34 6026人阅读 评论(0) 收藏 举报
正则表达式list
如某文件中有些行含有特定内容PTTAddress

1. UltraEdit中如何删除含有特定内容的行

a. 使用“替换”功能，勾选正则表达式(Regular Expressions)，“替换”内容为“%*PTTAddress*^p”，“替换为”为空；

b. 删除空行，使用“替换”功能，“替换”内容为“^r^n^r^n”，“替换为”为“^r^n”；

2. UltraEdit中如何删除不含有特定内容的行

a. 使用“搜索”功能，勾选“列出所含内容的行”(List Lines Containing Characters)，“搜索”内容为“PTTAddress”；

b. 在搜索结果中选择“拷贝到粘贴板”；

c. 新建一空白文件，然后ctrl+v。
在自己路径拷贝了小蒙的参考序列
cp /home/xuxm/zhihe/Ss046.fasta ./ 
bwa index Ss046.fasta
bwa aln Ss046.fasta SH7_1.fq > SH1_7.sai

bwa sampe Ss046.fasta SH7_1.sai SH7_2.sai SH7_1.fq SH7_2.fq > SH7_bwa.sam

samtools faidx Ss046.fasta 
 samtools import Ss046.fasta.fai SH7_bwa.sam SH7_bwa.bam

lishasha@guest-AltixXE250[xm] samtools                                [ 9:48AM]

Program: samtools (Tools for alignments in the SAM format)
Version: 0.1.19-44428cd

Usage:   samtools <command> [options]

Command: view        SAM<->BAM conversion
         sort        sort alignment file
         mpileup     multi-way pileup
         depth       compute the depth
         faidx       index/extract FASTA
         tview       text alignment viewer
         index       index alignment
         idxstats    BAM index stats (r595 or later)
         fixmate     fix mate information
         flagstat    simple stats
         calmd       recalculate MD/NM tags and '=' bases
         merge       merge sorted alignments
         rmdup       remove PCR duplicates
         reheader    replace BAM header
         cat         concatenate BAMs
         bedcov      read depth per BED region
         targetcut   cut fosmid regions (for fosmid pool only)
         phase       phase heterozygotes
         bamshuf     shuffle and group alignments by name

 samtools sort SH7_bwa.bam SH7_bam_sort
 lishasha@guest-AltixXE250[xm] samtools sort                           [ 9:48AM]

Usage:   samtools sort [options] <in.bam> <out.prefix>

Options: -n        sort by read name
         -f        use <out.prefix> as full file name instead of prefix
         -o        final output to stdout
         -l INT    compression level, from 0 to 9 [-1]
         -@ INT    number of sorting and compression threads [1]
         -m INT    max memory per thread; suffix K/M/G recognized [768M]
         
 samtools index SH7_bam_sort.bam 
samtools mpileup -uD -I -q20 -f Ss046.fasta SH7_bam_sort.bam > SH7.var
lishasha@guest-AltixXE250[xm] samtools mpileup                        [11:41AM]

Usage: samtools mpileup [options] in1.bam [in2.bam [...]]

Input options:

       -6           assume the quality is in the Illumina-1.3+ encoding
       -A           count anomalous read pairs
       -B           disable BAQ computation
       -b FILE      list of input BAM filenames, one per line [null]
       -C INT       parameter for adjusting mapQ; 0 to disable [0]
       -d INT       max per-BAM depth to avoid excessive memory usage [250]
       -E           recalculate extended BAQ on the fly thus ignoring existing BQs
       -f FILE      faidx indexed reference sequence file [null]
       -G FILE      exclude read groups listed in FILE [null]
       -l FILE      list of positions (chr pos) or regions (BED) [null]
       -M INT       cap mapping quality at INT [60]
       -r STR       region in which pileup is generated [null]
       -R           ignore RG tags
       -q INT       skip alignments with mapQ smaller than INT [0]
       -Q INT       skip bases with baseQ/BAQ smaller than INT [13]
       --rf INT     required flags: skip reads with mask bits unset []
       --ff INT     filter flags: skip reads with mask bits set []

Output options:

       -D           output per-sample DP in BCF (require -g/-u)
       -g           generate BCF output (genotype likelihoods)
       -O           output base positions on reads (disabled by -g/-u)
       -s           output mapping quality (disabled by -g/-u)
       -S           output per-sample strand bias P-value in BCF (require -g/-u)
       -u           generate uncompress BCF output

SNP/INDEL genotype likelihoods options (effective with `-g' or `-u'):

       -e INT       Phred-scaled gap extension seq error probability [20]
       -F FLOAT     minimum fraction of gapped reads for candidates [0.002]
       -h INT       coefficient for homopolymer errors [100]
       -I           do not perform indel calling
       -L INT       max per-sample depth for INDEL calling [250]
       -m INT       minimum gapped reads for indel candidates [1]
       -o INT       Phred-scaled gap open sequencing error probability [40]
       -p           apply -m and -F per-sample to increase sensitivity
       -P STR       comma separated list of platforms for indels [all]
 bcftools view SH7_SNP.bcf > SH7.vcf 
 bcftools view -N SH7_SNP.bcf > SH7_SNP_N.bcf
 
lishasha@guest-AltixXE250[xm] bcftools                                [12:11PM]

Program: bcftools (Tools for data in the VCF/BCF formats)
Version: 0.1.17-dev (r973:277)

Usage:   bcftools <command> <arguments>

Command: view      print, extract, convert and call SNPs from BCF
         index     index BCF
         cat       concatenate BCFs
         ld        compute all-pair r^2
         ldpair    compute r^2 between requested pairs 
 lishasha@guest-AltixXE250[xm] bcftools view                           [12:11PM]

Usage: bcftools view [options] <in.bcf> [reg]

Input/output options:

       -A        keep all possible alternate alleles at variant sites
       -b        output BCF instead of VCF
       -D FILE   sequence dictionary for VCF->BCF conversion [null]
       -F        PL generated by r921 or before (which generate old ordering)
       -G        suppress all individual genotype information
       -l FILE   list of sites (chr pos) or regions (BED) to output [all sites]
       -L        calculate LD for adjacent sites
       -N        skip sites where REF is not A/C/G/T
       -Q        output the QCALL likelihood format
       -s FILE   list of samples to use [all samples]
       -S        input is VCF
       -u        uncompressed BCF output (force -b)

Consensus/variant calling options:

       -c        SNP calling (force -e)
       -d FLOAT  skip loci where less than FLOAT fraction of samples covered [0]
       -e        likelihood based analyses
       -g        call genotypes at variant sites (force -c)
       -i FLOAT  indel-to-substitution ratio [-1]
       -I        skip indels
       -p FLOAT  variant if P(ref|D)<FLOAT [0.5]
       -P STR    type of prior: full, cond2, flat [full]
       -t FLOAT  scaled substitution mutation rate [0.001]
       -T STR    constrained calling; STR can be: pair, trioauto, trioxd and trioxs (see manual) [null]
       -v        output potential variant sites only (force -c)

Contrast calling and association test options:

       -1 INT    number of group-1 samples [0]
       -C FLOAT  posterior constrast for LRT<FLOAT and P(ref|D)<0.5 [1]
       -U INT    number of permutations for association testing (effective with -1) [0]
       -X FLOAT  only perform permutations for P(chi^2)<FLOAT [0.01]

lishasha@guest-AltixXE250[xm] vcfutils.pl                             [12:14PM]

Usage:   vcfutils.pl <command> [<arguments>]

Command: subsam       get a subset of samples
         listsam      list the samples
         fillac       fill the allele count field
         qstats       SNP stats stratified by QUAL

         hapmap2vcf   convert the hapmap format to VCF
         ucscsnp2vcf  convert UCSC SNP SQL dump to VCF

         varFilter    filtering short variants (*)
         vcf2fq       VCF->fastq (**)

Notes: Commands with description endting with (*) may need bcftools
       specific annotations.
lishasha@guest-AltixXE250[xm] vcfutils.pl varFilter                   [12:14PM]

Usage:   vcfutils.pl varFilter [options] <in.vcf>

Options: -Q INT    minimum RMS mapping quality for SNPs [10]
         -d INT    minimum read depth [2]
         -D INT    maximum read depth [10000000]
         -a INT    minimum number of alternate bases [2]
         -w INT    SNP within INT bp around a gap to be filtered [3]
         -W INT    window size for filtering adjacent gaps [10]
         -1 FLOAT  min P-value for strand bias (given PV4) [0.0001]
         -2 FLOAT  min P-value for baseQ bias [1e-100]
         -3 FLOAT  min P-value for mapQ bias [0]
         -4 FLOAT  min P-value for end distance bias [0.0001]
                 -e FLOAT  min P-value for HWE (plus F<0) [0.0001]
         -p        print filtered variants

Note: Some of the filters rely on annotations generated by SAMtools/BCFtools.

 vcfutils.pl varFilter -Q20 -d10 -D100 SH7_SNP_N.bcf > SH7_flitered_2.vcf
20140424帮小蒙的SNP
bwa map
bwa index Ss046.fasta 
bwa aln Ss046.fasta SH1_1.fq > SH1_1.sai
bwa aln Ss046.fasta SH1_2.fq > SH1_2.sai  

bwa sampe
bwa sampe Ss046.fasta SH1_1.sai SH1_2.sai SH1_1.fq SH1_2.fq > SH1_bwa.sam 
                                                                        
samtools sort bam                                                                  
samtools faidx Ss046.fasta                                                                         
samtools import Ss046.fasta.fai SH1_bwa.sam SH1_bwa.bam 
samtools sort SH1_bwa.bam SH1_bwa_sorted
samtools index SH1_bwa_sorted.bam


SNP
samtools mpileup -uD -I -q20 -f Ss046.fasta SH1_bwa_sorted.bam | bcftools view -bvcg -> samtools_snps/SH1.bcf
bcftools view -N samtools_snps/SH1.bcf | vcfutils.pl varFilter -D100 -d5 -Q20 > samtools_snps/SH1.vcf




20140122
 /vol3/home/fanhang/bin/ia64-suse-linux/blat
blat - Standalone BLAT v. 35 fast sequence search command line tool
usage:
   blat database query [-ooc=11.ooc] output.psl
where:
   database and query are each either a .fa , .nib or .2bit file,
   or a list these files one file name per line.
   -ooc=11.ooc tells the program to load over-occurring 11-mers from
               and external file.  This will increase the speed
               by a factor of 40 in many cases, but is not required
   output.psl is where to put the output.
   Subranges of nib and .2bit files may specified using the syntax:
      /path/file.nib:seqid:start-end
   or
      /path/file.2bit:seqid:start-end
   or
      /path/file.nib:start-end
   With the second form, a sequence id of file:start-end will be used.
options:
   -t=type     Database type.  Type is one of:
                 dna - DNA sequence
                 prot - protein sequence
                 dnax - DNA sequence translated in six frames to protein
               The default is dna
   -q=type     Query type.  Type is one of:
                 dna - DNA sequence
                 rna - RNA sequence
                 prot - protein sequence
                 dnax - DNA sequence translated in six frames to protein
                 rnax - DNA sequence translated in three frames to protein
               The default is dna
   -prot       Synonymous with -t=prot -q=prot
   -ooc=N.ooc  Use overused tile file N.ooc.  N should correspond to 
               the tileSize
   -tileSize=N sets the size of match that triggers an alignment.  
               Usually between 8 and 12
               Default is 11 for DNA and 5 for protein.
   -stepSize=N spacing between tiles. Default is tileSize.
   -oneOff=N   If set to 1 this allows one mismatch in tile and still
               triggers an alignments.  Default is 0.
   -minMatch=N sets the number of tile matches.  Usually set from 2 to 4
               Default is 2 for nucleotide, 1 for protein.
   -minScore=N sets minimum score.  This is the matches minus the 
               mismatches minus some sort of gap penalty.  Default is 30
   -minIdentity=N Sets minimum sequence identity (in percent).  Default is
               90 for nucleotide searches, 25 for protein or translated
               protein searches.
   -maxGap=N   sets the size of maximum gap between tiles in a clump.  Usually
               set from 0 to 3.  Default is 2. Only relevent for minMatch > 1.
   -noHead     suppress .psl header (so it's just a tab-separated file)
   -makeOoc=N.ooc Make overused tile file. Target needs to be complete genome.
   -repMatch=N sets the number of repetitions of a tile allowed before
               it is marked as overused.  Typically this is 256 for tileSize
               12, 1024 for tile size 11, 4096 for tile size 10.
               Default is 1024.  Typically only comes into play with makeOoc.
               Also affected by stepSize. When stepSize is halved repMatch is
               doubled to compensate.
   -mask=type  Mask out repeats.  Alignments won't be started in masked region
               but may extend through it in nucleotide searches.  Masked areas
               are ignored entirely in protein or translated searches. Types are
                 lower - mask out lower cased sequence
                 upper - mask out upper cased sequence
                 out   - mask according to database.out RepeatMasker .out file
                 file.out - mask database according to RepeatMasker file.out
   -qMask=type Mask out repeats in query sequence.  Similar to -mask above but
               for query rather than target sequence.
   -repeats=type Type is same as mask types above.  Repeat bases will not be
               masked in any way, but matches in repeat areas will be reported
               separately from matches in other areas in the psl output.
   -minRepDivergence=NN - minimum percent divergence of repeats to allow 
               them to be unmasked.  Default is 15.  Only relevant for 
               masking using RepeatMasker .out files.
   -dots=N     Output dot every N sequences to show program's progress
   -trimT      Trim leading poly-T
   -noTrimA    Don't trim trailing poly-A
   -trimHardA  Remove poly-A tail from qSize as well as alignments in 
               psl output
   -fastMap    Run for fast DNA/DNA remapping - not allowing introns, 
               requiring high %ID. Query sizes must not exceed 5000.
   -out=type   Controls output file format.  Type is one of:
                   psl - Default.  Tab separated format, no sequence
                   pslx - Tab separated format with sequence
                   axt - blastz-associated axt format
                   maf - multiz-associated maf format
                   sim4 - similar to sim4 format
                   wublast - similar to wublast format
                   blast - similar to NCBI blast format
                   blast8- NCBI blast tabular format
                   blast9 - NCBI blast tabular format with comments
   -fine       For high quality mRNAs look harder for small initial and
               terminal exons.  Not recommended for ESTs
   -maxIntron=N  Sets maximum intron size. Default is 750000
   -extendThroughN - Allows extension of alignment through large blocks of N's


/vol3/home/fanhang/bin/ia64-suse-linux/blat  /vol2/biodb/human/hg19-fa/hg19.fasta   /vol3/home/liss/2014122/IonXpress_030.R_2012_08_28_17_42_56_user_20140120-2.fa  -fastMap -out=blast  /vol3/home/liss/2014122/119_30.blast  &



python /vol3/home/wangw/tools/blast2tab-n1+e+2cover+nohits-pick.py 119_30.blast 119_30.tab 1 &
提取tab文件中的id号

根据序列号从原始序列中提取reads  并将原始reads分为提取和剩余两部分
python fa-pick-according-list.py  IonXpress_030.R_2012_08_28_17_42_56_user_20140120-2.fa  119_30_id.fa human-reads-2.fa
将nohuman-reads与数据库进行比对
/vol3/home/wangw/blast/blast-2.2.22/bin/blastall -p blastn -d /vol2/nt-fultit/nt-fultit -i /vol3/home/liss/2014122/human-reads-2_non.fa -o 119_30.blastn -v 5 -b 5 -W 35 -a 20 &  

blast2tab
for k in {0..49}; do python /vol3/home/wangw/tools/blast2tab-n1+e+2cover+nohits-pick.py 46_$k.blastn 46_$k.tab 1 & done
python blast2tab-n1+e+2cover+nohits-pick.py 119_30.blastn 119_30_blastn.tab 1 &

tab2sta
python tab2sta-FulTit.py 119_30_blastn.tab 119_30_blastn.sta species &

tax_rank: 1397108
['17065927', 279] ['34740422', 266]
TidLenNum: 4772
['10129445', 121, 1] ['10186549', 155, 1]
TLN: 1659
['Homo sapiens', 121, 1, '9606'] ['Homo sapiens', 155, 1, '9606']
rlt: 1657
notax: 2
['Acetobacter pasteurianus', 42, 1, '438'] ['Achromobacter xylosoxidans', 827, 6, '85698']
result: 699




20131206
 

REAPR version: 1.0.16
Usage:
    reapr <task> [options]

Common tasks:
    facheck    - checks IDs in fasta file
    smaltmap   - map read pairs using SMALT: makes a BAM file to be used as
                 input to the pipeline
    perfectmap - make perfect uniquely mapping plot files
    pipeline   - runs the REAPR pipeline, using an assembly and mapped reads
                 as input, and optionally results of perfectmap.
                 (It runs facheck, preprocess, stats, fcdrate, score,
                 summary and break)
    plots      - makes Artemis plot files for a given contig, using results
                 from stats (and optionally results from score)
    seqrename  - renames all sequences in a BAM file: use this if you already
                 mapped your reads but then found facheck failed - saves
                 remapping the reads so that pipeline can be run

Advanced tasks:
    preprocess - preprocess files: necessary for running stats
    stats      - generates stats from a BAM file
    fcdrate    - estimates FCD cutoff for score, using results from stats
    score      - calculates scores and assembly errors, using results from stats
    summary    - make summary stats file, using results from score
    break      - makes broken assembly, using results from score
    gapresize  - experimental, calculates gap sizes based on read mapping
    perfectfrombam - generate perfect mapping plots from a bam file (alternative
                     to using perfectmap for large genomes)

Reapr用matepair数据检查拼接序列

先检查input文件
fa文件（拼接好的序列）
/home/xuxm/software/Reapr_1.0.16/reapr facheck A16R_ref_gapclose_1.fa #如果报错,运行下一步， 正确
reapr facheck assembly.fa new_assembly #重新命名

fastq文件（matepair reads)
smalt check *.fastq

lishasha@guest-AltixXE250[A16R_cheak] smalt                           [ 4:27AM]

              SMALT - Sequence Mapping and Alignment Tool
                             (version: 0.7.5)
SYNOPSIS:
    smalt <task> [TASK_OPTIONS] [<index_name> <file_name_A> [<file_name_B>]]

Available tasks:
    smalt check   - checks FASTA/FASTQ input
    smalt help    - prints a brief summary of this software
    smalt index   - builds an index of k-mer words for the reference
    smalt map     - maps single or paired reads onto the reference
    smalt sample  - sample insert sizes for paired reads
    smalt version - prints version information

Help on individual tasks:
    smalt <task> -H

lishasha@guest-AltixXE250[A16R_cheak] smalt check *.fastq             [ 4:33AM]
# Command line:  smalt check A16R_split_1.fastq A16R_split_2.fastq
# checked 551022 read pairs: ok.



分两步先把matepair reads map到拼接序列上生成bam文件，然后再做分析
/home/xuxm/software/Reapr_1.0.16/reapr smaltmap A16_complete_new.fa.fa A16R_split_1.fastq A16R_split_2.fastq A16R_complete_mapped.bam

[fai_build_core] different line length in sequence 'A16R_whole_genome_1'.
[REAPR smaltmap] Error in system call:
/home/xuxm/software/Reapr_1.0.16/src/samtools faidx A16R_ref_gapclose_1.fa

This means samtools is unhappy with the assembly
fasta file 'A16R_ref_gapclose_1.fa'.

Common causes are empty lines in the file, or inconsistent line lengths
(all sequence lines must have the same length, except the last line of
any sequence which can be shorter). Cannot continue.

于是再运行下面命令
/home/xuxm/software/Reapr_1.0.16/reapr facheck A16R_ref_gapclose_1.fa A16R_new.fa
再运行
/home/xuxm/software/Reapr_1.0.16/reapr smaltmap A16R_new.fa A16R_split_1.fastq A16R_split_2.fastq A16R_mapped.bam 还是错误 人工检查
/home/xuxm/software/Reapr_1.0.16/reapr smaltmap A16R_split_1.fastq A16R_split_2.fastq A16R_mapped.b
usage:
task_smaltmap.pl [options] <assembly.fa> <reads_1.fastq> <reads_2.fastq> <out.bam>

Maps read pairs to an asembly with SMALT, making a final BAM file that
is sorted, indexed and has duplicates removed.

The n^th read in reads_1.fastq should be the mate of the n^th read in
reads_2.fastq.

It is assumed that reads are 'innies', i.e. the correct orientation
is reads in a pair pointing towards each other (---> <---).

Options:

-k <int>
        The -k option (kmer hash length) when indexing the genome
        with 'smalt index' [13]
-s <int>
        The -s option (step length) when indexing the genome
        with 'smalt index' [2]
-m <int>
        The -m option when mapping reads with 'smalt map' [not used by default]
-y <float>
        The -y option when mapping reads with 'smalt map'.
        The default of 0.5 means that at least 50% of each read must map
        perfectly. Depending on the quality of your reads, you may want to
        increase this to be more stringent (or consider using -m) [0.5]
-x
        Use this to just print the commands that will be run, instead of
        actually running them
-u <int>
        The -u option of 'smalt sample'. This is used to estimate the insert
        size from a sample of the reads, by mapping every n^th read pair [1000]
        
正式跑mapping
 /home/xuxm/software/Reapr_1.0.16/reapr smaltmap A16_complete_new.fa.fa A16R_split_1.fastq A16R_split_2.fastq A16R_complete.bam 正常跑上了

      

/home/xuxm/software/Reapr_1.0.16/reapr pipeline A16_complete_new.fa.fa A16R_complete.bam A16R_complete_reapr
pipeline运行过程
 /home/xuxm/software/Reapr_1.0.16/reapr pip
eline A16R_ref_gapclose_1_circled_checked.fa.fa A16r_mapping.bam A16_checkedby_reapr
Running reapr version 1.0.16 pipeline:
/home/xuxm/software/Reapr_1.0.16/reapr pipeline A16R_ref_gapclose_1_circled_checked.fa.fa A16r_mapping.bam A16_checkedby_reapr
[REAPR pipeline] Running facheck
[REAPR pipeline] Running preprocess
[REAPR preprocess] sampling region A16R_whole_genome_1:3096-1003095.  Already sampled 0 bases
[REAPR preprocess] Sampled 1000000 bases for GC/coverage estimation
[REAPR pipeline] Running stats
[REAPR pipeline] Running fcdrate
[REAPR pipeline] Running score
[REAPR pipeline] Running break
[REAPR pipeline] Running summary



perl /data2/tongyg/software/GapFiller_v1-11_linux-x86_64/GapFiller.pl -l lib.txt -s A16R_with_small_gap.fasta -m 30 -o 2 -r 0.7 -n 10 -d 50 -t 10  -T 3 -i 3 -b A16R_with_small_gap_filled.fa

20131128

Reapr用matepair数据检查拼接序列

先检查input文件
fa文件（拼接好的序列）
reapr facheck assembly.fa #如果报错,运行下一步
reapr facheck assembly.fa new_assembly #重新命名

fastq文件（matepair reads)
smalt check *.fastq

分两步先把matepair reads map到拼接序列上生成bam文件，然后再做分析
reapr smaltmap assembly.fa long_1.fq long_2.fq long_mapped.bam
reapr pipeline assembly.fa long_mapped.bam output_directory

20131127
perl /data2/tongyg/software/GapFiller_v1-11_linux-x86_64/GapFiller.pl -l lib.txt -s pxo1_withgap.fasta -m 30 -o 2 -r 0.7 -n 10 -d 50 -t 10  -T 3 -i 3 -b pxo1_gap_filled
perl /data2/tongyg/software/GapFiller_v1-11_linux-x86_64/GapFiller.pl -l lib.txt -s pxo1_f_e_2.fasta -m 30 -o 2 -r 0.7 -n 10 -d 50 -t 10  -T 3 -i 3 -b pxo1_gap_filled_2

gapcloser
perl gapcloser_newbler

perl gapclose_newbler.pl 1gap_gapcloser_4.fa 108WT_pe.fasta out.fa 5000 0
for k in {1..50}; do perl gapclose_newbler_pack.pl temp/split_$k  temp/ctg_3and5r_ends  & done


gapfiller
perl /data2/tongyg/software/GapFiller_v1-11_linux-x86_64/GapFiller.pl -l libraries.txt -s 108WT_fix_linked.fa -m 30 -o 2 -r 0.7 -n 10 -d 500 -t 10  -T 10 -i 1 -b 108WT_fix_linked_f.fa
perl /data2/tongyg/software/GapFiller_v1-11_linux-x86_64/GapFiller.pl -l lib.txt -s pxo1_withgap.fasta -m 30 -o 2 -r 0.7 -n 10 -d 500 -t 10  -T 3 -i 1 -b pxo1_gap_filled.fa
perl /data2/tongyg/software/GapFiller_v1-11_linux-x86_64/GapFiller.pl -l lib.txt -s pxo1_withgap.fasta -m 30 -o 2 -r 0.7 -n 10 -d 500 -t 10  -T 3 -i 1 -b pxo1_gap_filled.fa



查看版本
linux:/vol3/home/liss # cat /proc/version 
Linux version 2.6.16.60-0.23.PTF.403865.0-default (geeko@buildhost) (gcc version 4.1.2 20070115 (SUSE Linux)) #1 SMP Thu May 15 06:38:31 UTC 2008

yxy COG号
rast 最后一个tab文件
他们服务器上的blast_database进行blastp
结束后去cog库搜索cog号


blast2go
本地建库blastp后要去blast2go做导入file-

做mapping，本地联网打开blast2go, 改blast2go的ip为192.168.1.5，改自己机子ip：192.168.1.18

/vol1/home/tongyg/blast-2.2.22-ia64/bin/blastall -p blastp -d /vol1/home/zhangzhiyi/azeem/blast/human.protein.faa -i /vol1/home/zhangzhiyi/azeem/blast/Agy99_protein.fasta -o Mulcerans.m9v1.blast -m 9 -e 0.005 -v 1 &
/vol1/home/tongyg/blast-2.2.22-ia64/bin/blastall -p blastp -d library_16.fa -o /vol1/home/tongyg/raw-seq/phage-IME10/aa.blast -m 9 -v 10 -b 0 &

/vol1/home/tongyg/blast-2.2.22-ia64/bin/blastall -p blastp -d library_21.fa -i cds_edit.fa -o mm_21cds_blastp.xml -m 7 -v 1 &


runAssembly -o mate -cpu 7 pe_1.fastq pe_2.fastq se.fastq  (可以直接将二者混一起跑）

有细菌库的路径

/vol1/home/zhangzhiyi/NCBI/nt/nt2012/sorted/PICK

20130828 blast建库
/vol1/home/tongyg/blast-2.2.22-ia64/bin/formatdb -p T -i library_16.fa

/vol1/home/tongyg/blast-2.2.22-ia64/bin/formatdb --help

formatdb 2.2.22   arguments:

  -t  Title for database file [String]  Optional
  -i  Input file(s) for formatting [File In]  Optional
  -l  Logfile name: [File Out]  Optional
    default = formatdb.log
  -p  Type of file
         T - protein   
         F - nucleotide [T/F]  Optional
    default = T
  -o  Parse options
         T - True: Parse SeqId and create indexes.
         F - False: Do not parse SeqId. Do not create indexes.
 [T/F]  Optional
    default = F
  -a  Input file is database in ASN.1 format (otherwise FASTA is expected)
         T - True, 
         F - False.
 [T/F]  Optional
    default = F
  -b  ASN.1 database in binary mode
         T - binary, 
         F - text mode.
 [T/F]  Optional
    default = F
  -e  Input is a Seq-entry [T/F]  Optional
    default = F
  -n  Base name for BLAST files [String]  Optional
  -v  Database volume size in millions of letters [Integer]  Optional
    default = 4000
  -s  Create indexes limited only to accessions - sparse [T/F]  Optional
    default = F
  -V  Verbose: check for non-unique string ids in the database [T/F]  Optional
    default = F
  -L  Create an alias file with this name
        use the gifile arg (below) if set to calculate db size
        use the BLAST db specified with -i (above) [File Out]  Optional
  -F  Gifile (file containing list of gi's) [File In]  Optional
  -B  Binary Gifile produced from the Gifile specified above [File Out]  Optional
  -T  Taxid file to set the taxonomy ids in ASN.1 deflines [File In]  Optional
blast成功
/vol1/home/tongyg/blast-2.2.22-ia64/bin/blastall -p blastn -d morganella_mor.fa（即将上一步的-i变为-d，不需要变名字） -i mm1_circle_final.fa -o mm_tax581.blast -K 1 -v 5 -b 5 -e 1E-5 -a 25 &


 fq2fa格式转换(每隔4行取前2行，将行首@变成>)
awk 'NR % 4 < 3 '  IonXpress_015.fastq | awk 'NR % 3 > 0 ' | perl -p -e 's/^@/>/g' > IonXpress_015.fasta
命令行拼接454 newbler
/opt/454/bin/runAssembly IonXpress_015.fasta
显示行数
wc -l IonXpress_015.fastq
文件切割，打印前500k行 (三人之一reads)
awk 'NR < 500001' IonXpress_015.fastq > one-third.fq
/opt/454/bin/runAssembly one-third.fq
 
 命令行直接拼接mate-pair数据
 runAssembly -o mate -cpu 7 pe_1.fastq pe_2.fastq 

20130826
/vol1/home/tongyg/blast-2.2.22-ia64/bin/blastall -p blastn -d morganella_mor.fa -i mm1_circle_final.fa -o mm_tax581.blast -K 1 -v 5 -b 5 -e 1E-5 -a 25 &
>>/vol1/home/tongyg/blast-2.2.22-ia64/bin/blastall -p blastn -d /vol1/home/tongyg/database/nt -i /vol3/home/liss/mm/mm1_circle_final.fa -o mm.blast -e 1e-10 -v 5 -b 5 &
>/vol2/biodb
/vol1/home/tongyg/blast-2.2.22-ia64/bin/blastall -p blastn -d /vol1/home/tongyg/database/nt/nt -i /vol3/home/liss/mm/mm1_circle_final.fa -o mm.blast -e 1e-10 -v 5 -b 5 &

20130725
grep -v 'Unmapped' 454ReadStatus_2.txt > m150_2_mapped.fa                                                              
cut -f 1  m150_2_mapped.fa >  m150_2_mapped_cut.fa                                           
sort m150_2_mapped_cut.fa > m150_2_mapped_cut_sotted.fa                                      
perl getreads_index.pl m150_9_second.fasta m150_2_mapped_cut_sotted.fa > m150_9_indexed.fasta
cut -b -30 m150_shuffle_index.fasta |sort |uniq -c |sort -g -r -o m150_shuffle_se_termini.fa                                                                                             







20130701
末端分析
cut -b -30 TM4.fasta |sort |uniq -c |sort -g -r -o TM4_termini.fa
将绿脓两次数据合并
 perl shuffleSequences_fastq.pl IonXpress_016_R_2012_07_17_21_02_17_user_20130517-R2-P1-P2a6p_20130517-R2-P1-P2a6p_56.fastq IonXpress_016_R_2012_07_20_11_12_00_user_20130530-2-3-pa26p1_20130530-2-3-pa26p1_58.fastq pa26p1.fastq 
 后来手动合并
  perl fq_all2std.pl fq2fa pa26phebing.fastq > pa26phebing.fasta
cut -b -30 R_2012_06_08_21_56_18_user_20120627-2_Auto_20120627-2_4.fasta |sort |uniq -c |sort -g -r -o AB2_termini.fa
20130630
20130626
用上面相同的方法进行muave知道顺序后的3,18间gap确定
取3，18scaf两端3_end,3_start，18_end,18_start
gsMapping3_end,3_start，18_end,18_start +all-pe.fq)

grep '_end' 454ReadStatus.txt | grep '+' | cut -f 1 > all_end_f.list
grep '_start'  454ReadStatus.txt | grep '-' | cut -f 1 > all_start_r.list


grep '\/1' all_end_f.list > all_end_f_1.list
grep '\/2' all_end_f.list > all_end_f_2.list
grep '\/1' all_start_r.list > all_start_r_1.list
grep '\/2' all_start_r.list > all_start_r_2.list

在此/1变/2，/2变/1得到4个文件，加原文件
将以上8个文件1合一起2合一起的两个文件

去掉/1, /2
sed 's/\/1//g' 1_4he.fa > 1_4he.list
sed 's/\/2//g' 2_4he.fa > 2_4he.list
read名字排序
sort 1_4he.list > 1_4he_sort.list
sort 2_4he.list > 2_4he_sort.list
行尾增加/1, /2
sed 's/$/\/1/g' 1_4he_sort.list > 1_4he_same_1.fa
sed 's/$/\/2/g' 2_4he_sort.list > 2_4he_same_2.fa

 提取mate-pair
perl getreads_index.pl bar_shuffle_2.fasta 1_4he_same_1.fa > 1_4he_same_1_reads.fa 
perl getreads_index.pl bar_shuffle_2.fasta 2_4he_same_2.fa > 2_4he_same_2_reads.fa 
fa2fq
perl fq_all2std.pl fa2std 1_4he_same_1_reads.fa > 1_4he_same_1_reads.fq
perl fq_all2std.pl fa2std 2_4he_same_2_reads.fa > 2_4he_same_2_reads.fq

得到gapside变长的片段再走一次程序
将补上gap的scafold1,2变为对应的1_end ,2_start
gsMapping(1_end ,2_start +all-pe.fq)
grep '1_end' 454ReadStatus.txt | grep '+' | cut -f 1 > 1_end_f.list
grep '2_start'  454ReadStatus.txt | grep '-' | cut -f 1 > 2_start_r.list

即合并一下三个步奏
（grep '1_end' 454ReadStatus.txt > 1_end.fa
grep '2_start'  454ReadStatus.txt > 2_start.fa
grep '+' 1_end.fa > 1_end_f.fa
grep '-' 2_start.fa >2_start_r.fa

cut -f 1 1_end_f.fa > 1_end_f.list
cut -f 1 2_start_r.fa > 2_start_r.list）


grep '\/1' 1_end_f.list > 1_end_f_1.list
grep '\/2' 1_end_f.list > 1_end_f_2.list
grep '\/1' 2_start_r.list > 2_start_r_1.list
grep '\/2' 2_start_r.list > 2_start_r_2.list

在此/1变/2，/2变/1得到4个文件，加原文件
将以上8个文件1合一起2合一起的两个文件

去掉/1, /2
sed 's/\/1//g' 1_4he.fa > 1_4he.list
sed 's/\/2//g' 2_4he.fa > 2_4he.list
read名字排序
sort 1_4he.list > 1_4he_sort.list
sort 2_4he.list > 2_4he_sort.list
提取相同的reads(其实就是找mate-pair配对的reads)
comm -1 -2 1_4he_sort.list 2_4he_sort.list > 1_he_2_he_same.fa

行尾增加/1, /2
sed 's/$/\/1/g' 1_he_2_he_same.fa > 1_he_2_he_same_1.fa
sed 's/$/\/2/g' 1_he_2_he_same.fa > 1_he_2_he_same_2.fa

 提取mate-pair
perl getreads_index.pl bar_shuffle_2.fasta 1_he_2_he_same_1.fa > 1_he_2_he_same_1_reads.fa 
perl getreads_index.pl bar_shuffle_2.fasta 1_he_2_he_same_2.fa > 1_he_2_he_same_2_reads.fa 
fa2fq
perl fq_all2std.pl fa2std 1_he_2_he_same_1_reads.fa > 1_he_2_he_same_1_reads.fq
perl fq_all2std.pl fa2std 1_he_2_he_same_2_reads.fa > 1_he_2_he_same_2_reads.fq
20130627SA1,SA2
20号机器上，(不能用)
截取bam文件前1000行。
samtools view -h IonXpress_016_R_2012_06_23_08_10_13_user_20130131-solid-3-4-5-ime-sa2_20130131-solid-3-4-5-ime-sa2_29.basecaller.bam | head -1000 |samtools view -bS - > NewIonXpress.bam
sff2bam [-c] [-o out.bam] in.sff [in2.sff ...]
bam2sff [-o out.sff] in.bam
将
ion上直接打sff2bam即可出如下信息
sff2bam - Converts sff file(s) to unmapped bam file.
Usage: 
  sff2bam [-c] [-o out.bam] in.sff [in2.sff ...]
Options:
  -h,--help              this message
  -c,--combine-sffs      combine all input sff files to a single bam file
                         all sffs must have same flow orders and same keys.
  -o,--out-filename      specify output file name

ion上
sff2bam回车即可有信息
sff2bam -o SA1.bam 454Reads.MID014D.sff 
#'sumtool' from package 'mtd-utils' (universe)
 Command 'samtools' from package 'samtools' (universe
 
samtools view -h SA2_2.bam | head - 1000 |samtools view -bS - > SA2_3.bam
samtools view -h可看view参数
Usage:   samtools view [options] <in.bam>|<in.sam> [region1 [...]]

Options: -b       output BAM
         -h       print header for the SAM output
         -H       print header only (no alignments)
         -S       input is SAM
         -u       uncompressed BAM output (force -b)
         -1       fast compression (force -b)
         -x       output FLAG in HEX (samtools-C specific)
         -X       output FLAG in string (samtools-C specific)
         -c       print only the count of matching records
         -L FILE  output alignments overlapping the input BED FILE [null]
         -t FILE  list of reference names and lengths (force -S) [null]
         -T FILE  reference sequence file (force -S) [null]
         -o FILE  output file name [stdout]
         -R FILE  list of read groups to be outputted [null]
         -f INT   required flag, 0 for unset [0]
         -F INT   filtering flag, 0 for unset [0]
         -q INT   minimum mapping quality [0]
         -l STR   only output reads in library STR [null]
         -r STR   only output reads in read group STR [null]
         -s FLOAT fraction of templates to subsample; integer part as seed [-1]
         -?       longer help
SFFRandom -n 280000 -o SA1_10.sff 454Reads.MID014D.sff 
bam2sff -o SA2_2.sff SA2_2.bam
] no @SQ lines in the header.
[sam_read1] missing header? Abort


20130626
用上面相同的方法进行28全部关联
取28scaf两端28_end,28_start

gsMapping28_end ,28_start +all-pe.fq)
grep '_end' 454ReadStatus.txt | grep '+' | cut -f 1 > all_end_f.list
grep '_start'  454ReadStatus.txt | grep '-' | cut -f 1 > all_start_r.list

即合并一下三个步奏
（grep '_end' 454ReadStatus.txt > all_end.fa
grep '_start'  454ReadStatus.txt > all_start.fa
grep '+' all_end.fa > all_end_f.fa
grep '-' 2_start.fa >2_start_r.fa

cut -f 1 1_end_f.fa > 1_end_f.list
cut -f 1 2_start_r.fa > 2_start_r.list）


grep '\/1' all_end_f.list > all_end_f_1.list
grep '\/2' all_end_f.list > all_end_f_2.list
grep '\/1' all_start_r.list > all_start_r_1.list
grep '\/2' all_start_r.list > all_start_r_2.list

在此/1变/2，/2变/1得到4个文件，加原文件
将以上8个文件1合一起2合一起的两个文件

去掉/1, /2
sed 's/\/1//g' 28_1_4he.fa > 28_1_4he.list
sed 's/\/2//g' 28_2_4he.fa > 28_2_4he.list
read名字排序
sort 28_1_4he.list > 28_1_4he_sort.list
sort 28_2_4he.list > 28_2_4he_sort.list
提取相同的reads(其实就是找mate-pair配对的reads)
comm -1 -2 28_1_4he_sort.list 28_2_4he.list > 28_1_he_2_he_same.fa

行尾增加/1, /2
sed 's/$/\/1/g' 28_1_he_2_he_same.fa > 28_1_he_2_he_same_1.fa
sed 's/$/\/2/g' 28_1_he_2_he_same.fa > 28_1_he_2_he_same_2.fa

 提取mate-pair
perl getreads_index.pl bar_shuffle_2.fasta 28_1_4he.fa > 28_1_4he_same_1_reads.fa 
perl getreads_index.pl bar_shuffle_2.fasta 28_2_4he.fa > 28_2_4he_same_2_reads.fa 
fa2fq
perl fq_all2std.pl fa2std 28_1_4he_same_1_reads.fa > 28_1_4he_same_1_reads.fq
perl fq_all2std.pl fa2std 28_2_4he_same_2_reads.fa > 28_2_4he_same_2_reads.fq

得到gapside变长的片段再走一次程序
将补上gap的scafold1,2变为对应的1_end ,2_start
gsMapping(1_end ,2_start +all-pe.fq)
grep '1_end' 454ReadStatus.txt | grep '+' | cut -f 1 > 1_end_f.list
grep '2_start'  454ReadStatus.txt | grep '-' | cut -f 1 > 2_start_r.list

即合并一下三个步奏
（grep '1_end' 454ReadStatus.txt > 1_end.fa
grep '2_start'  454ReadStatus.txt > 2_start.fa
grep '+' 1_end.fa > 1_end_f.fa
grep '-' 2_start.fa >2_start_r.fa

cut -f 1 1_end_f.fa > 1_end_f.list
cut -f 1 2_start_r.fa > 2_start_r.list）


grep '\/1' 1_end_f.list > 1_end_f_1.list
grep '\/2' 1_end_f.list > 1_end_f_2.list
grep '\/1' 2_start_r.list > 2_start_r_1.list
grep '\/2' 2_start_r.list > 2_start_r_2.list

在此/1变/2，/2变/1得到4个文件，加原文件
将以上8个文件1合一起2合一起的两个文件

去掉/1, /2
sed 's/\/1//g' 1_4he.fa > 1_4he.list
sed 's/\/2//g' 2_4he.fa > 2_4he.list
read名字排序
sort 1_4he.list > 1_4he_sort.list
sort 2_4he.list > 2_4he_sort.list
提取相同的reads(其实就是找mate-pair配对的reads)
comm -1 -2 1_4he_sort.list 2_4he_sort.list > 1_he_2_he_same.fa

行尾增加/1, /2
sed 's/$/\/1/g' 1_he_2_he_same.fa > 1_he_2_he_same_1.fa
sed 's/$/\/2/g' 1_he_2_he_same.fa > 1_he_2_he_same_2.fa

 提取mate-pair
perl getreads_index.pl bar_shuffle_2.fasta 1_he_2_he_same_1.fa > 1_he_2_he_same_1_reads.fa 
perl getreads_index.pl bar_shuffle_2.fasta 1_he_2_he_same_2.fa > 1_he_2_he_same_2_reads.fa 
fa2fq
perl fq_all2std.pl fa2std 1_he_2_he_same_1_reads.fa > 1_he_2_he_same_1_reads.fq
perl fq_all2std.pl fa2std 1_he_2_he_same_2_reads.fa > 1_he_2_he_same_2_reads.fq


20130624
将SE.PE合在一个文件里
cat bar_shuffle.fa lihao_shortgun.fa >> se_pe.fa
python fill_gap_with_mp.py scaffold_name 5000 reads.fa
cat split_pe.pl可以看整个文件
20130624找scaffold间的序列


取scaf1,2间的末端1_end ,2_start
gsMapping(1_end ,2_start +all-pe.fq)
grep '1_end' 454ReadStatus.txt | grep '+' | cut -f 1 > 1_end_f.list
grep '2_start'  454ReadStatus.txt | grep '-' | cut -f 1 > 2_start_r.list

即合并一下三个步奏
（grep '1_end' 454ReadStatus.txt > 1_end.fa
grep '2_start'  454ReadStatus.txt > 2_start.fa
grep '+' 1_end.fa > 1_end_f.fa
grep '-' 2_start.fa >2_start_r.fa

cut -f 1 1_end_f.fa > 1_end_f.list
cut -f 1 2_start_r.fa > 2_start_r.list）


grep '\/1' 1_end_f.list > 1_end_f_1.list
grep '\/2' 1_end_f.list > 1_end_f_2.list
grep '\/1' 2_start_r.list > 2_start_r_1.list
grep '\/2' 2_start_r.list > 2_start_r_2.list

在此/1变/2，/2变/1得到4个文件，加原文件
将以上8个文件1合一起2合一起的两个文件

去掉/1, /2
sed 's/\/1//g' 1_4he.fa > 1_4he.list
sed 's/\/2//g' 2_4he.fa > 2_4he.list
read名字排序
sort 1_4he.list > 1_4he_sort.list
sort 2_4he.list > 2_4he_sort.list
提取相同的reads(其实就是找mate-pair配对的reads)
comm -1 -2 1_4he_sort.list 2_4he_sort.list > 1_he_2_he_same.fa

行尾增加/1, /2
sed 's/$/\/1/g' 1_he_2_he_same.fa > 1_he_2_he_same_1.fa
sed 's/$/\/2/g' 1_he_2_he_same.fa > 1_he_2_he_same_2.fa

 提取mate-pair
perl getreads_index.pl bar_shuffle_2.fasta 1_he_2_he_same_1.fa > 1_he_2_he_same_1_reads.fa 
perl getreads_index.pl bar_shuffle_2.fasta 1_he_2_he_same_2.fa > 1_he_2_he_same_2_reads.fa 
fa2fq
perl fq_all2std.pl fa2std 1_he_2_he_same_1_reads.fa > 1_he_2_he_same_1_reads.fq
perl fq_all2std.pl fa2std 1_he_2_he_same_2_reads.fa > 1_he_2_he_same_2_reads.fq

得到gapside变长的片段再走一次程序
将补上gap的scafold1,2变为对应的1_end ,2_start
gsMapping(1_end ,2_start +all-pe.fq)
grep '1_end' 454ReadStatus.txt | grep '+' | cut -f 1 > 1_end_f.list
grep '2_start'  454ReadStatus.txt | grep '-' | cut -f 1 > 2_start_r.list

即合并一下三个步奏
（grep '1_end' 454ReadStatus.txt > 1_end.fa
grep '2_start'  454ReadStatus.txt > 2_start.fa
grep '+' 1_end.fa > 1_end_f.fa
grep '-' 2_start.fa >2_start_r.fa

cut -f 1 1_end_f.fa > 1_end_f.list
cut -f 1 2_start_r.fa > 2_start_r.list）


grep '\/1' 1_end_f.list > 1_end_f_1.list
grep '\/2' 1_end_f.list > 1_end_f_2.list
grep '\/1' 2_start_r.list > 2_start_r_1.list
grep '\/2' 2_start_r.list > 2_start_r_2.list

在此/1变/2，/2变/1得到4个文件，加原文件
将以上8个文件1合一起2合一起的两个文件

去掉/1, /2
sed 's/\/1//g' 1_4he.fa > 1_4he.list
sed 's/\/2//g' 2_4he.fa > 2_4he.list
read名字排序
sort 1_4he.list > 1_4he_sort.list
sort 2_4he.list > 2_4he_sort.list
提取相同的reads(其实就是找mate-pair配对的reads)
comm -1 -2 1_4he_sort.list 2_4he_sort.list > 1_he_2_he_same.fa

行尾增加/1, /2
sed 's/$/\/1/g' 1_he_2_he_same.fa > 1_he_2_he_same_1.fa
sed 's/$/\/2/g' 1_he_2_he_same.fa > 1_he_2_he_same_2.fa

 提取mate-pair
perl getreads_index.pl bar_shuffle_2.fasta 1_he_2_he_same_1.fa > 1_he_2_he_same_1_reads.fa 
perl getreads_index.pl bar_shuffle_2.fasta 1_he_2_he_same_2.fa > 1_he_2_he_same_2_reads.fa 
fa2fq
perl fq_all2std.pl fa2std 1_he_2_he_same_1_reads.fa > 1_he_2_he_same_1_reads.fq
perl fq_all2std.pl fa2std 1_he_2_he_same_2_reads.fa > 1_he_2_he_same_2_reads.fq


20130608 
从ion上拖到这个目录后解压
R_2012_06_24_03_44_01_user_2013-02-27-9-10-11-ime-sa2_2013-02-27-9-10-11-ime-sa2_30.barcode.sff.zip
unzip R_2012_06_24_03_44_01_user_2013-02-27-9-10-11-ime-sa2_2013-02-27-9-10-11-ime-sa2_30.barcode.sff.zip

用sff_extract#对sff_extract程序进行了一定程度的修改，增加了以下功能：
#1、	能够输出干净的fastq序列，（-c），头尾均去掉了小写字母的序列和对应的质量参数。
#2、	能够对fastq序列进行统计，得出其平均长度，并且具有扩展性，能简单地加载各种模块统计参数
#3、	能够根据参数剪切长度过小的Reads（--remove_short），并能给出被去掉的Reads的数量。
#4、	去掉左侧接头 --min_left_clip

python sff_extract_0_2_13_modified.py -Q IonXpress_010_R_2012_06_24_03_44_01_user_2013-02-27-9-10-11-ime-sa2_2013-02-27-9-10-11-ime-sa2_30.sff -o ime-sa2_18   -c --remove_short 30 --min_left_clip 18
格式转换fq2fa
perl /vol1/home/tongyg/tools/fq_all2std.pl fq2fa IonXpress_010_R_2012_06_24_03_44_01_user_2013-02-27-9-10-11-ime-sa2_2013-02-27-9-10-11-ime-sa2_30.sff.fastq > SA2.fasta
取前30bp进行排序
cut -b -30 SA2_18.fq.fasta |sort |uniq -c |sort -g -r -o SA2-termini-2convert.fa




方案2
直接用分出来的fastq格式，利用awk命令
每隔4行提取第2行得到序列
awk 'NR % 4 == 1'
再cut -b -30 SA2.fasta |sort |uniq -c |sort -g -r -o IME09_DA_30bp.stat
即一条命令
awk 'NR % 4 == 2' sff.fastq|cut -b -30 |sort |uniq -c |sort -g -r -o SA2-termini-awk.fa


5028c提取mate-pair reads
4700上
grep -v 'Unmapped' 454ReadStatus.txt > 5028c_mp-mapped.txt  
cut -f 1 5028c_mp-mapped.txt > 5028c_mp-mapped.list
grep '\/1' 5028c_mp-mapped.list > 5028c_mp-mapped_1.list
grep '\/2' 5028c_mp-mapped.list > 5028c_mp-mapped_2.list
去掉/1, /2
sed 's/\/1//g' 5028c_mp-mapped_1.list > 5028c_mp-mapped_1a.list
sed 's/\/2//g' 5028c_mp-mapped_2.list > 5028c_mp-mapped_2a.list
read名字排序
sort 5028c_mp-mapped_1a.list > 5028c_mp-mapped_1a_sort.list
sort 5028c_mp-mapped_2a.list > 5028c_mp-mapped_2a_sort.list
提取相同的reads
comm -1 -2 5028c_mp-mapped_1a_sort.list 5028c_mp-mapped_2a_sort.list > 5028c_mp-mapped_comm.list
行尾增加/1, /2
sed 's/$/\/1/g' 5028c_mp-mapped_comm.list > 5028c_mp-mapped_comm_1.list
sed 's/$/\/2/g' 5028c_mp-mapped_comm.list > 5028c_mp-mapped_comm_2.list
 perl /vol1/home/tongyg/tools/fq_all2std.pl fq2fa barcode48.fastq >  barcode48.fasta
 提取mate-pair
perl /vol1/home/tongyg/tools/getreads_index.pl barcode48.fasta 5028c_mp-mapped_comm_1.list > mp-mapped_pe_1.fa  (index时就应提取用
后分别提取的，不然没有反向）
perl /vol1/home/tongyg/tools/getreads_index.pl barcode48.fasta 5028c_mp-mapped_comm_2.list > mp-mapped_pe_2.fa 
fa2fq
 perl /vol1/home/tongyg/tools/fq_all2std.pl fa2std mp-mapped_pe_1.fasta > fa2std mp-mapped_pe_1.fq
 perl /vol1/home/tongyg/tools/fq_all2std.pl fa2std mp-mapped_pe_2.fasta > mp-mapped_pe_2.fq

perl /vol1/home/tongyg/tools/shuffleSequences_fasta.pl  mp-mapped_pe_1.fasta mp-mapped_pe_2.fasta file3
 
 
 
Usage:  runAssembly [-o projdir] [-nrm] [-p (sfffile | [regionlist:]analysisDir)]... (sfffile | [regionlist:]analysisDir)...





/vol1/home/tongyg/tools/fq_all2std.pl
python ionMP_split.py  m150-unmapped.fq m150-unmapped123
20130607、
python fill_gap_with_mp.py scaffold_name 5000 reads.fa
python fill_gap_with_mp.py 454Scaffolds.fna 5000 bar-pe.fastq


20130607
python /vol1/home/liss/yy/sff_extract_0_2_13_modified.py -Q IonXpress_010_R_2012_06_24_03_44_01_user_2013-02-27-9-10-11-ime-sa2_2013-02-27-9-10-11-ime-sa2_30.sff -o IonXpress_010_R_2012_06_24_03_44_01_user_2013-02-27-9-10-11-ime-sa2_2013-02-27-9-10-11-ime-sa2_30   -c --remove_short 30 --min_left_clip 19



python /results/analysis/output/Home/tools/sff_extract_0_2_13_modified.py -Q 454Reads.MID004R.sff -o 454Reads.MID004R   -c --remove_short 30 --min_left_clip 23      #-Q: output fq format, -c:cut lower case base and quality value
perl /vol1/home/tongyg/tools/fq_all2std.pl fq2fa R_2011_06_30_13_39_04_user_SN1-8_Auto_SN1-8_8.fastq > R_2011_06_30_13_39_04_user_SN1-8_Auto_SN1-8_8.fasta
去掉长度小于40bp的reads
先变为一行
/vol1/home/tongyg/fastx_toolkit-0.0.13/src/fasta_formatter/fasta_formatter -w 0 -i SA1.fasta -o SA1-formated.fa

/vol1/home/tongyg/fastx_toolkit-0.0.13/src/fastx_clipper/fastx_clipper -l 40 -i SA1-formated.fa -o SA1.fa

cut -b -30 SA2.fasta |sort |uniq -c |sort -g -r -o SA2-termini.fa
cut -b -30 SA1.fa |sort |uniq -c |sort -g -r -o SA1-termini.fa
20上
perl extend_end.pl SA2-Q.fa SA2-2th.fasta 20 50 SA2-ex.fa
blastall参数
1、BLASTP是蛋白序列到蛋白库中的一种查询。库中存在的每条已知序列将逐一地同每条所查序列作一对一的序列比对。
2、BLASTX是核酸序列到蛋白库中的一种查询。先将核酸序列翻译成蛋白序列（一条核酸序列会被翻译成可能的六条蛋白），再对每一条作一对一的蛋白序列比对。
3、BLASTN是核酸序列到核酸库中的一种查询。库中存在的每条已知序列都将同所查序列作一对一地核酸序列比对。
4、TBLASTN是蛋白序列到核酸库中的一种查询。与BLASTX相反，它是将库中的核酸序列翻译成蛋白序列，再同所查序列作蛋白与蛋白的比对。
5、TBLASTX是核酸序列到核酸库中的一种查询。此种查询将库中的核酸序列和所查的核酸序列都翻译成蛋白（每条核酸序列会产生6条可能的蛋白序列），这样每次比对会产生36种比对阵列。
bl2seq参数
  -F  Filter query sequence (DUST with blastn, SEG with others) [String]
    default = T
  -m  Use Mega Blast for search [T/F]  Optional
    default = F (寻找大片段重复：-m T)
4700上
bl2seq -m T -i SA1-circle.fa -j SA1-circle.fa -p blastn -F F -o SA1-repeat.fa
bl2seq -m T -i SA2-circle.fa -j SA2-circle.fa -p blastn -F F -o SA2-repeat.fa


20130605
手工补gap

取末端1_end ,2_start
gsMapping(1_end ,2_start +all-pe.fq)
grep '1_end' 454ReadStatus.txt > 1_end.fa
grep '2_start'  454ReadStatus.txt > 2_start.fa
grep '+' 1_end.fa > 1_end_f.fa
grep '-' 2_start.fa >2_start_r.fa

Cut -f 1 1_end_f.fa > 1_end_f.list
cut -f 1 2_start_r.fa > 2_start_r.list

grep '\/1' 1_end_f.list > 1_end_f_1.list
grep '\/2' 1_end_f.list > 1_end_f_2.list
grep '\/1' 2_start_r.list > 2_start_r_1.list
grep '\/2' 2_start_r.list > 2_start_r_2.list

在此/1变/2，/2变/1得到8个文件
将以上8个文件1合一起2合一起的两个文件

去掉/1, /2
sed 's/\/1//g' 1_4he.fa > 1_4he.list
sed 's/\/2//g' 2_4he.fa > 2_4he.list
read名字排序
sort 1_4he.list > 1_4he_sort.list
sort 2_4he.list > 2_4he_sort.list
提取相同的reads
comm -1 -2 1_4he_sort.list 2_4he_sort.list > 1_he_2_he_same.fa

行尾增加/1, /2
sed 's/$/\/1/g' 1_he_2_he_same.fa > 1_he_2_he_same_1.fa
sed 's/$/\/2/g' 1_he_2_he_same.fa > 1_he_2_he_same_2.fa

 提取mate-pair
perl /home/tongyg/tools/getreads_index.pl bar-pe-all.fa 1_he_2_he_same_1.fa > 1_he_2_he_same_1_reads.fa 
perl /home/tongyg/tools/getreads_index.pl bar-pe-all.fa 1_he_2_he_same_2.fa > 1_he_2_he_same_2_reads.fa 
fa2fq
perl /home/tongyg/tools/fq_all2std.pl fa2std 1_he_2_he_same_1_reads.fa > 1_he_2_he_same_1_reads.fq
perl /home/tongyg/tools/fq_all2std.pl fa2std 1_he_2_he_same_2_reads.fa > 1_he_2_he_same_2_reads.fq


4700上
grep -v 'Unmapped' 454ReadStatus.txt > 28-scaf_mp-mapped.txt  
cut -f 1 28-scaf_mp-mapped.txt > 28-scaf_mp-mapped.list
grep '\/1' 28-scaf_mp-mapped.list > 28-scaf_mp-mapped_1.list
grep '\/2' 28-scaf_mp-mapped.list > 28-scaf_mp-mapped_2.list
去掉/1, /2
sed 's/\/1//g' 28-scaf_mp-mapped_1.list > 28-scaf_mp-mapped_1a.list
sed 's/\/2//g' 28-scaf_mp-mapped_2.list > 28-scaf_mp-mapped_2a.list
read名字排序
sort 28-scaf_mp-mapped_1a.list > 28-scaf_mp-mapped_1a_sort.list
sort 28-scaf_mp-mapped_2a.list > 28-scaf_mp-mapped_2a_sort.list
提取相同的reads
comm -1 -2 28-scaf_mp-mapped_1a_sort.list 28-scaf_mp-mapped_2a_sort.list > 28-scaf_mp-mapped_comm.list
行尾增加/1, /2
sed 's/$/\/1/g' 28-scaf_mp-mapped_comm.list > 28-scaf_mp-mapped_1.list
sed 's/$/\/2/g' 28-scaf_mp-mapped_comm.list > 28-scaf_mp-mapped_2.list
 perl /vol1/home/tongyg/tools/fq_all2std.pl fq2fa bar-pe.fastq >  bar-pe-all.fa
 提取mate-pair（#有时需要用dos2unix变换格式， 需要bioperl（20号机器没有））
perl /vol1/home/tongyg/tools/getreads_index.pl all-mp.fa mp-mapped_pe_1.list > mp-mapped_pe_1.fa  
perl /vol1/home/tongyg/tools/getreads_index.pl all-mp.fa mp-mapped_pe_2.list > mp-mapped_pe_2.fa 
fa2fq
 perl /vol1/home/tongyg/tools/fq_all2std.pl fa2std 28-scaf_mp-mapped_1.fa > 28-scaf_mp-mapped_1.fq
 perl /vol1/home/tongyg/tools/fq_all2std.pl fa2std 28-scaf_mp-mapped_2.fa >  28-scaf_mp-mapped_2.fq
 
 
 
寻找关联步骤：
1、对数据库处理
使用tea.pl程序将shuffle在一起的fa数据文件转换。配对的两条reads置于同一行，分为四列
perl -p -e 's/:/_/' <file-pe.fa >link/file.fa  将其中的：换成_号

perl link/tea.pl file-pe.fa link/file-pe-one.fa 置于同行
格式例子：ZG0N4_00040_00094/1:AGTGGGCTTGATGTAACTTTGAAAATATTTATTTAAAAAAATGTGTGAATACACAACAACAAGT:ZG0N4_00040_00094/2:CTTTTGTTGGTAAAAACGACACAGTTAATTCA

2、构建contig文件
先做reverse-complement； 将reverse-complement的序列名称改成rc；合并文件 cat 454Scaffolds-lh.fna 454rvt.fna >contigs.fa  grep '>' -c contigs.fa

contig1
contig2
contigrc1
contigrc2
```
3、寻找关联
1、初步运行 里面的searchtime设置为0。功能就是将database拆分成100个，放在temp文件夹下
运行： perl makelink.PL contigs.fa filepe-one.fa 56 5000

将里面的searchtime设置为1.否则下次运行还会拆分数据。
2、分配数据
for k in {1..100}; do perl makelink_pack01.pl temp/split_$k  temp/ctg_3_ends  temp/ctg_5r_ends ; done

3、解析数组，统计每个link上的copy数

 perl makelink.PL contigs.fa filepe-one.fa 56 5000

以后的步骤重复2和3


ION
 /home/ionadmin/454/bin/sfffile -s -o 5028c -mcf /home/ionguest/results/analysis/output/Home/20130516-mate-pair-B-h_54_061/basecaller_results/MID_CONF.parse /home/ionguest/results/analysis/output/Home/20130516-mate-pair-B-h_54_061/basecaller_results/R_2012_06_14_13_50_21_user_5028c-5047c-fenchang002-han2-2012-10-16_Auto_5028c-5047c-fenchang002-han2-2012-10-16_16.sff
 20
  cp /home/huangyong/lss/IonXpress_047_R_2012_07_16_21_26_40_user_20130516-mate-pair-B-h_20130516-mate-pair-B-h_54.sff ./
 /home/tongyg/tools> python sff_extract_0_3_0.py -l mp_link.fa -o /data2/lishasha/lh2013526/bar-ex-pe /data2/lishasha/lh2013526/IonXpress_047_R_2012_07_16_21_26_40_user_20130516-mate-pair-B-h_20130516-mate-pair-B-h_54.sff
  python ionMP_split.py  bar-ex-pe.fastq matepair-split123
拼接



20
 -c  /data2/zhangzhiyi/2013_0516_zhangjiusongM150/TEST/M150_d_results/M150_out.contig -m /data2/zhangzhiyi/2013_0516_zhangjiusongM150/TEST/M150_d_results/M150_out.mates -C /data2/zhangzhiyi/2013_0516_zhangjiusongM150/TEST/M150_d_results/M150_out.conf -o bambus
 
根用户
printscaff
/data2/software/IonTorrent/bambus-2.33/bambus/bin/goBambus -c /data2/zhangzhiyi/2013_0516_zhangjiusongM150/TEST/M150_d_results/M150_out.contig -m /data2/zhangzhiyi/2013_0516_zhangjiusongM150/TEST/M150_d_results/M150_out.mates -C /data2/zhangzhiyi/2013_0516_zhangjiusongM150/TEST/M150_d_results/M150_out.conf -o bambus

 /data2/software/IonTorrent/bambus-2.33/bambus/bin/printScaff -e /data2/zhangzhiyi/2013_0516_zhangjiusongM150/bambus/M150_out_bambus.evidence.xml -s /data2/zhangzhiyi/2013_0516_zhangjiusongM150/bambus/bambus.evidence.xml -l /data2/zhangzhiyi/2013_0516_zhangjiusongM150/bambus/bambus.lib -f /data2/zhangzhiyi/2013_0516_zhangjiusongM150/TEST/M150_d_results/M150_out.unpadded.fasta -merge -o /data2/zhangzhiyi/2013_0516_zhangjiusongM150/TEST/bambus2/M150test-bambus_scaffold -page -dot

/data2/software/IonTorrent/bambus-2.33/bambus/bin/printScaff -e bambus.evidence.xml -s bambus.out.xml -l bambus.lib -f /data2/zhangzhiyi/2013_0516_zhangjiusongM150/TEST/M150_d_results/M150_out.unpadded.fasta -merge  -o bambus_scaffold -page -dot


bambus
从第5步开始
convert_project -f caf -t ace M150_out.caf M150_out

/data2/software/IonTorrent/amos-3.1.0/bin/ace2contig -i M150_out.ace -o M150_out.contig

/data2/software/IonTorrent/bambus-2.33/bambus/bin/goBambus -c M150_out.
contig  -m M150_out.mates -C M150_out.conf -o bambus

/data2/zhangzhiyi/2013_0516_zhangjiusongM150/assembly/M150_assembly/M150_d_r
esults # /data2/software/IonTorrent/bambus-2.33/bambus/bin/printScaff -e bambus.evidence.xml -s bambus.out.xml -l bambus.lib -f M150_out.unpadded.fasta -merge -o bambus_scaffold -page -dot

20130529
4700
perl split_n.pl infile outfile
 cp /vol1/home/liss/mate-pair201357/R_2012_06
_11_19_17_00_user_SN1-10-lihao-2012.8.31_Auto_SN1-10-lihao-2012.8.31_11.fastq ./

/mate-pair201357/lhnew> perl fq_all2std.pl fq2fa R_2012_06_11_19_17_
00_user_SN1-10-lihao-2012.8.31_Auto_SN1-10-lihao-2012.8.31_11.fastq > lihao.fasta

perl extend_end.pl lhscaffold-split.fa lihao.fasta 20 50 lhscaffold-extend.fa
perl extend_end.pl lhscaffold-split.fa lh-se-half.fasta 20 50 lh-se-half-ex.fasta


perl extend_end.pl lh-se-half-ex.fasta lihao.fasta 20 50 lhscaffold-extend.fa
perl rev
/home/ionadmin/454/bin
/opt/454/bin/sfffile -s -mcf /home/ionguest/results/analysis/output/Home/20130516-mate-pair-B-h_54_061/basecaller_results/MID_CONF.parse /home/ionguest/results/analysis/output/Home/20130516-mate-pair-B-h_54_061/basecaller_results/R_2012_06_14_13_50_21_user_5028c-5047c-fenchang002-han2-2012-10-16_Auto_5028c-5047c-fenchang002-han2-2012-10-16_16.sff
 /opt/454/bin/sfffile -s -mcf mid_config_file sfffile.sff

20
python /results/analysis/output/Home/tools/sff_extract_0_2_13.py -Q /home/ionguest/results/analysis/output/Home/20130516-mate-pair-B-h_54_061/basecaller_results/IonXpress_047_R_2012_07_16_21_26_40_user_20130516-mate-pair-B-h_20130516-mate-pair-B-h_54.sff -o bar-pe.fastq          #-Q: output fq format

22上
 rm -R /home/lishasha/lh-newextract/forthpe+forth-se/sff

4700

xian
rev_com_rev.pl
perl extend_end.pl lh-se-half-ex.fasta lihao_eight.fa 20 50 lhscaffold-extend-2.fa


4700
perl rev_com_rev.pl lhscaffold-extend-8.fa lhscaffold-extend-8-rc.fa
